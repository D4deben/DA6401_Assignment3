{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP9EiFnI0fNvHFIimzPZBGC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/D4deben/DA6401_Assignment3/blob/main/DL_Assignment4_PartB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lndoaWz5mFCH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 2"
      ],
      "metadata": {
        "id": "bLfVKCU5mFho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import random\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Encoder part of the seq2seq model.\n",
        "    It takes a sequence of input characters and produces a context vector.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_vocab_size, embedding_dim, hidden_dim, n_layers,\n",
        "                 cell_type='LSTM', dropout_p=0.1, bidirectional=False):\n",
        "        \"\"\"\n",
        "        Initializes the Encoder.\n",
        "        \"\"\"\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.input_vocab_size = input_vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.cell_type = cell_type.upper()\n",
        "        self.bidirectional = bidirectional\n",
        "        self.num_directions = 2 if bidirectional else 1\n",
        "\n",
        "        self.embedding = nn.Embedding(input_vocab_size, embedding_dim)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "        rnn_dropout = dropout_p if n_layers > 1 else 0\n",
        "        if self.cell_type == 'RNN':\n",
        "            self.rnn = nn.RNN(embedding_dim, hidden_dim, n_layers,\n",
        "                              dropout=rnn_dropout, batch_first=True, bidirectional=self.bidirectional)\n",
        "        elif self.cell_type == 'GRU':\n",
        "            self.rnn = nn.GRU(embedding_dim, hidden_dim, n_layers,\n",
        "                              dropout=rnn_dropout, batch_first=True, bidirectional=self.bidirectional)\n",
        "        elif self.cell_type == 'LSTM':\n",
        "            self.rnn = nn.LSTM(embedding_dim, hidden_dim, n_layers,\n",
        "                               dropout=rnn_dropout, batch_first=True, bidirectional=self.bidirectional)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported cell type. Choose from 'RNN', 'GRU', 'LSTM'.\")\n",
        "\n",
        "    def forward(self, input_seq):\n",
        "        \"\"\"\n",
        "        Forward pass of the encoder.\n",
        "        \"\"\"\n",
        "        embedded = self.embedding(input_seq)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        outputs, hidden = self.rnn(embedded)\n",
        "\n",
        "        return outputs, hidden\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Decoder part of the seq2seq model.\n",
        "    It takes the encoder's context vector and generates an output sequence.\n",
        "    \"\"\"\n",
        "    def __init__(self, output_vocab_size, embedding_dim, hidden_dim, n_layers,\n",
        "                 cell_type='LSTM', dropout_p=0.1, encoder_bidirectional=False):\n",
        "        \"\"\"\n",
        "        Initializes the Decoder.\n",
        "        \"\"\"\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.output_vocab_size = output_vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.cell_type = cell_type.upper()\n",
        "        self.encoder_bidirectional = encoder_bidirectional\n",
        "\n",
        "\n",
        "        self.embedding = nn.Embedding(output_vocab_size, embedding_dim)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "        rnn_dropout = dropout_p if n_layers > 1 else 0\n",
        "        if self.cell_type == 'RNN':\n",
        "            self.rnn = nn.RNN(embedding_dim, hidden_dim, n_layers,\n",
        "                              dropout=rnn_dropout, batch_first=True)\n",
        "        elif self.cell_type == 'GRU':\n",
        "            self.rnn = nn.GRU(embedding_dim, hidden_dim, n_layers,\n",
        "                              dropout=rnn_dropout, batch_first=True)\n",
        "        elif self.cell_type == 'LSTM':\n",
        "            self.rnn = nn.LSTM(embedding_dim, hidden_dim, n_layers,\n",
        "                               dropout=rnn_dropout, batch_first=True)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported cell type. Choose from 'RNN', 'GRU', 'LSTM'.\")\n",
        "\n",
        "        self.fc_out = nn.Linear(hidden_dim, output_vocab_size)\n",
        "\n",
        "    def forward(self, input_char, hidden_state):\n",
        "        \"\"\"\n",
        "        Forward pass for a single decoding step.\n",
        "        \"\"\"\n",
        "        input_char = input_char.unsqueeze(1)\n",
        "\n",
        "        embedded = self.embedding(input_char)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        rnn_output, new_hidden_state = self.rnn(embedded, hidden_state)\n",
        "\n",
        "        rnn_output_squeezed = rnn_output.squeeze(1)\n",
        "\n",
        "        prediction = self.fc_out(rnn_output_squeezed)\n",
        "\n",
        "        return prediction, new_hidden_state\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    \"\"\"\n",
        "    The main Seq2Seq model that combines Encoder and Decoder.\n",
        "    \"\"\"\n",
        "    def __init__(self, encoder, decoder, device, target_sos_idx):\n",
        "        \"\"\"\n",
        "        Initializes the Seq2Seq model.\n",
        "        \"\"\"\n",
        "        super(Seq2Seq, self).__init__()\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "        self.target_sos_idx = target_sos_idx\n",
        "\n",
        "        if self.encoder.bidirectional:\n",
        "            encoder_hidden_dim_actual = self.encoder.hidden_dim * self.encoder.num_directions\n",
        "            decoder_hidden_dim_expected = self.decoder.hidden_dim\n",
        "\n",
        "            self.fc_hidden = nn.Linear(encoder_hidden_dim_actual, decoder_hidden_dim_expected)\n",
        "            if self.encoder.cell_type == 'LSTM':\n",
        "                self.fc_cell = nn.Linear(encoder_hidden_dim_actual, decoder_hidden_dim_expected)\n",
        "\n",
        "\n",
        "    def _adapt_encoder_hidden(self, encoder_hidden):\n",
        "        \"\"\"\n",
        "        Adapts the encoder's final hidden state to be suitable as the decoder's initial hidden state.\n",
        "        Handles bidirectional encoders by combining forward and backward states.\n",
        "        \"\"\"\n",
        "        if not self.encoder.bidirectional:\n",
        "            return encoder_hidden\n",
        "\n",
        "        if self.encoder.cell_type == 'LSTM':\n",
        "            h_n, c_n = encoder_hidden\n",
        "\n",
        "            h_n = h_n.view(self.encoder.n_layers, self.encoder.num_directions, h_n.size(1), self.encoder.hidden_dim)\n",
        "            c_n = c_n.view(self.encoder.n_layers, self.encoder.num_directions, c_n.size(1), self.encoder.hidden_dim)\n",
        "\n",
        "            h_n_cat = torch.cat((h_n[:, 0, :, :], h_n[:, 1, :, :]), dim=2)\n",
        "            c_n_cat = torch.cat((c_n[:, 0, :, :], c_n[:, 1, :, :]), dim=2)\n",
        "\n",
        "            adapted_h = self.fc_hidden(h_n_cat)\n",
        "            adapted_c = self.fc_cell(c_n_cat)\n",
        "            return (adapted_h, adapted_c)\n",
        "\n",
        "        else:\n",
        "            h_n = encoder_hidden\n",
        "            h_n = h_n.view(self.encoder.n_layers, self.encoder.num_directions, h_n.size(1), self.encoder.hidden_dim)\n",
        "            h_n_cat = torch.cat((h_n[:, 0, :, :], h_n[:, 1, :, :]), dim=2)\n",
        "            adapted_h = self.fc_hidden(h_n_cat)\n",
        "            return adapted_h\n",
        "\n",
        "\n",
        "    def forward(self, source_seq, target_seq, teacher_forcing_ratio=0.5):\n",
        "        \"\"\"\n",
        "        Forward pass of the Seq2Seq model.\n",
        "        \"\"\"\n",
        "        batch_size = source_seq.shape[0]\n",
        "        target_len = target_seq.shape[1]\n",
        "        target_vocab_size = self.decoder.output_vocab_size\n",
        "\n",
        "        outputs = torch.zeros(batch_size, target_len, target_vocab_size).to(self.device)\n",
        "\n",
        "        _, encoder_final_hidden = self.encoder(source_seq)\n",
        "\n",
        "        decoder_hidden = self._adapt_encoder_hidden(encoder_final_hidden)\n",
        "\n",
        "        decoder_input = target_seq[:, 0]\n",
        "\n",
        "        for t in range(target_len -1):\n",
        "            decoder_output_logits, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
        "\n",
        "            outputs[:, t+1] = decoder_output_logits\n",
        "\n",
        "            teacher_force_this_step = random.random() < teacher_forcing_ratio\n",
        "\n",
        "            top1_predicted_token = decoder_output_logits.argmax(1)\n",
        "\n",
        "            decoder_input = target_seq[:, t+1] if teacher_force_this_step else top1_predicted_token\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def predict(self, source_seq, max_output_len=50):\n",
        "        \"\"\"\n",
        "        Generate a sequence of characters given a source sequence during inference.\n",
        "        No teacher forcing is used.\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        batch_size = source_seq.shape[0]\n",
        "        if batch_size != 1:\n",
        "            raise ValueError(\"Predict function currently supports batch_size=1 for simplicity.\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            _, encoder_final_hidden = self.encoder(source_seq)\n",
        "            decoder_hidden = self._adapt_encoder_hidden(encoder_final_hidden)\n",
        "\n",
        "            decoder_input = torch.tensor([self.target_sos_idx], device=self.device)\n",
        "\n",
        "            predicted_indices = []\n",
        "\n",
        "            for _ in range(max_output_len):\n",
        "                decoder_output_logits, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
        "\n",
        "                top1_predicted_token = decoder_output_logits.argmax(1)\n",
        "                predicted_idx = top1_predicted_token.item()\n",
        "                predicted_indices.append(predicted_idx)\n",
        "\n",
        "                decoder_input = top1_predicted_token\n",
        "\n",
        "        self.train()\n",
        "        return predicted_indices\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    INPUT_VOCAB_SIZE = 50\n",
        "    OUTPUT_VOCAB_SIZE = 60\n",
        "    TARGET_SOS_IDX = 0\n",
        "    TARGET_EOS_IDX = 1\n",
        "\n",
        "    EMBEDDING_DIM = 128\n",
        "    HIDDEN_DIM_ENCODER = 256\n",
        "    HIDDEN_DIM_DECODER = 256\n",
        "    N_LAYERS_ENCODER = 2\n",
        "    N_LAYERS_DECODER = 2\n",
        "    CELL_TYPE = 'LSTM'\n",
        "    DROPOUT = 0.3\n",
        "    ENCODER_BIDIRECTIONAL = True\n",
        "\n",
        "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    encoder = Encoder(INPUT_VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM_ENCODER, N_LAYERS_ENCODER,\n",
        "                      CELL_TYPE, DROPOUT, bidirectional=ENCODER_BIDIRECTIONAL).to(DEVICE)\n",
        "\n",
        "    decoder = Decoder(OUTPUT_VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM_DECODER, N_LAYERS_DECODER,\n",
        "                      CELL_TYPE, DROPOUT, encoder_bidirectional=ENCODER_BIDIRECTIONAL).to(DEVICE)\n",
        "\n",
        "    model = Seq2Seq(encoder, decoder, DEVICE, TARGET_SOS_IDX).to(DEVICE)\n",
        "\n",
        "    print(f\"Model initialized on {DEVICE}\")\n",
        "    print(f\"Encoder cell type: {encoder.cell_type}, Layers: {encoder.n_layers}, Hidden: {encoder.hidden_dim}, Bidirectional: {encoder.bidirectional}\")\n",
        "    print(f\"Decoder cell type: {decoder.cell_type}, Layers: {decoder.n_layers}, Hidden: {decoder.hidden_dim}\")\n",
        "\n",
        "    BATCH_SIZE = 4\n",
        "    SOURCE_SEQ_LEN = 10\n",
        "    TARGET_SEQ_LEN = 12\n",
        "\n",
        "    dummy_source_seq = torch.randint(0, INPUT_VOCAB_SIZE, (BATCH_SIZE, SOURCE_SEQ_LEN)).to(DEVICE)\n",
        "\n",
        "    dummy_target_seq = torch.randint(1, OUTPUT_VOCAB_SIZE, (BATCH_SIZE, TARGET_SEQ_LEN)).to(DEVICE)\n",
        "    dummy_target_seq[:, 0] = TARGET_SOS_IDX\n",
        "\n",
        "    print(f\"\\nDummy source shape: {dummy_source_seq.shape}\")\n",
        "    print(f\"Dummy target shape: {dummy_target_seq.shape}\")\n",
        "\n",
        "    model.train()\n",
        "    output_logits = model(dummy_source_seq, dummy_target_seq, teacher_forcing_ratio=0.5)\n",
        "    print(f\"\\nOutput logits shape from forward pass: {output_logits.shape}\")\n",
        "\n",
        "    model.eval()\n",
        "    dummy_single_source_seq = torch.randint(0, INPUT_VOCAB_SIZE, (1, SOURCE_SEQ_LEN)).to(DEVICE)\n",
        "    predicted_sequence_indices = model.predict(dummy_single_source_seq, max_output_len=TARGET_SEQ_LEN)\n",
        "    print(f\"\\nPredicted sequence indices for a single source: {predicted_sequence_indices}\")\n",
        "    print(f\"Length of predicted sequence: {len(predicted_sequence_indices)}\")\n",
        "\n",
        "    def count_parameters(m):\n",
        "        return sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
        "\n",
        "    print(f'\\nThe model has {count_parameters(model):,} trainable parameters')\n",
        "\n",
        "    print(\"\\n--- Example of changing parameters ---\")\n",
        "    encoder_gru_unidir = Encoder(INPUT_VOCAB_SIZE, embedding_dim=64, hidden_dim=128, n_layers=1,\n",
        "                                 cell_type='GRU', dropout_p=0.1, bidirectional=False).to(DEVICE)\n",
        "    decoder_gru_unidir = Decoder(OUTPUT_VOCAB_SIZE, embedding_dim=64, hidden_dim=128, n_layers=1,\n",
        "                                 cell_type='GRU', dropout_p=0.1, encoder_bidirectional=False).to(DEVICE)\n",
        "    model_gru_unidir = Seq2Seq(encoder_gru_unidir, decoder_gru_unidir, DEVICE, TARGET_SOS_IDX).to(DEVICE)\n",
        "    print(f\"GRU Unidirectional Model initialized on {DEVICE}\")\n",
        "    print(f\"Encoder cell type: {encoder_gru_unidir.cell_type}, Layers: {encoder_gru_unidir.n_layers}, Hidden: {encoder_gru_unidir.hidden_dim}, Bidirectional: {encoder_gru_unidir.bidirectional}\")\n",
        "    print(f\"Decoder cell type: {decoder_gru_unidir.cell_type}, Layers: {decoder_gru_unidir.n_layers}, Hidden: {decoder_gru_unidir.hidden_dim}\")\n",
        "    print(f'The GRU Unidir model has {count_parameters(model_gru_unidir):,} trainable parameters')\n"
      ],
      "metadata": {
        "id": "RUnMFY-ZmM4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q3 - Vanilla Model"
      ],
      "metadata": {
        "id": "CEGuP3HemkC7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import numpy as np\n",
        "import wandb\n",
        "import os\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "import heapq\n",
        "\n",
        "# --- Constants ---\n",
        "SOS_TOKEN = \"<sos>\"\n",
        "EOS_TOKEN = \"<eos>\"\n",
        "PAD_TOKEN = \"<pad>\"\n",
        "UNK_TOKEN = \"<unk>\"\n",
        "\n",
        "# --- Model Definition (Encoder, Decoder, Seq2Seq) ---\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_vocab_size, embedding_dim, hidden_dim, n_layers,\n",
        "                 cell_type='LSTM', dropout_p=0.1, bidirectional=False, pad_idx=0):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.input_vocab_size = input_vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.cell_type = cell_type.upper()\n",
        "        self.bidirectional = bidirectional\n",
        "        self.num_directions = 2 if bidirectional else 1\n",
        "\n",
        "        self.embedding = nn.Embedding(input_vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "        rnn_dropout = dropout_p if n_layers > 1 else 0\n",
        "        if self.cell_type == 'RNN':\n",
        "            self.rnn = nn.RNN(embedding_dim, hidden_dim, n_layers,\n",
        "                              dropout=rnn_dropout, batch_first=True, bidirectional=self.bidirectional)\n",
        "        elif self.cell_type == 'GRU':\n",
        "            self.rnn = nn.GRU(embedding_dim, hidden_dim, n_layers,\n",
        "                              dropout=rnn_dropout, batch_first=True, bidirectional=self.bidirectional)\n",
        "        elif self.cell_type == 'LSTM':\n",
        "            self.rnn = nn.LSTM(embedding_dim, hidden_dim, n_layers,\n",
        "                               dropout=rnn_dropout, batch_first=True, bidirectional=self.bidirectional)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported cell type. Choose from 'RNN', 'GRU', 'LSTM'.\")\n",
        "\n",
        "    def forward(self, input_seq, input_lengths):\n",
        "        embedded = self.embedding(input_seq)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "        packed_outputs, hidden = self.rnn(packed_embedded)\n",
        "        return None, hidden\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_vocab_size, embedding_dim,\n",
        "                 decoder_hidden_dim, n_layers, cell_type='LSTM', dropout_p=0.1, pad_idx=0):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.output_vocab_size = output_vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.decoder_hidden_dim = decoder_hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.cell_type = cell_type.upper()\n",
        "\n",
        "        self.embedding = nn.Embedding(output_vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "        rnn_input_dim = embedding_dim\n",
        "\n",
        "        rnn_dropout = dropout_p if n_layers > 1 else 0\n",
        "        if self.cell_type == 'RNN':\n",
        "            self.rnn = nn.RNN(rnn_input_dim, decoder_hidden_dim, n_layers,\n",
        "                              dropout=rnn_dropout, batch_first=True)\n",
        "        elif self.cell_type == 'GRU':\n",
        "            self.rnn = nn.GRU(rnn_input_dim, decoder_hidden_dim, n_layers,\n",
        "                              dropout=rnn_dropout, batch_first=True)\n",
        "        elif self.cell_type == 'LSTM':\n",
        "            self.rnn = nn.LSTM(rnn_input_dim, decoder_hidden_dim, n_layers,\n",
        "                               dropout=rnn_dropout, batch_first=True)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported cell type. Choose from 'RNN', 'GRU', 'LSTM'.\")\n",
        "\n",
        "        self.fc_out = nn.Linear(decoder_hidden_dim, output_vocab_size)\n",
        "\n",
        "    def forward(self, input_char, prev_decoder_hidden):\n",
        "        input_char = input_char.unsqueeze(1)\n",
        "        embedded = self.embedding(input_char)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        rnn_output, current_decoder_hidden = self.rnn(embedded, prev_decoder_hidden)\n",
        "\n",
        "        rnn_output_squeezed = rnn_output.squeeze(1)\n",
        "        prediction_logits = self.fc_out(rnn_output_squeezed)\n",
        "        return prediction_logits, current_decoder_hidden\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device, target_sos_idx):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "        self.target_sos_idx = target_sos_idx\n",
        "\n",
        "        encoder_effective_output_dim_per_layer = self.encoder.hidden_dim * self.encoder.num_directions\n",
        "        decoder_rnn_expected_hidden_dim = self.decoder.decoder_hidden_dim\n",
        "\n",
        "        self.needs_dim_adaptation = encoder_effective_output_dim_per_layer != decoder_rnn_expected_hidden_dim\n",
        "        self.fc_adapt_hidden = None\n",
        "        self.fc_adapt_cell = None\n",
        "\n",
        "        if self.needs_dim_adaptation:\n",
        "            self.fc_adapt_hidden = nn.Linear(encoder_effective_output_dim_per_layer, decoder_rnn_expected_hidden_dim)\n",
        "            if self.encoder.cell_type == 'LSTM':\n",
        "                self.fc_adapt_cell = nn.Linear(encoder_effective_output_dim_per_layer, decoder_rnn_expected_hidden_dim)\n",
        "\n",
        "    def _adapt_encoder_hidden_for_decoder(self, encoder_final_hidden_state):\n",
        "        is_lstm = self.encoder.cell_type == 'LSTM'\n",
        "        if is_lstm:\n",
        "            h_from_enc, c_from_enc = encoder_final_hidden_state\n",
        "        else:\n",
        "            h_from_enc = encoder_final_hidden_state\n",
        "            c_from_enc = None\n",
        "\n",
        "        batch_size = h_from_enc.size(1)\n",
        "\n",
        "        h_processed = h_from_enc.view(self.encoder.n_layers, self.encoder.num_directions,\n",
        "                                     batch_size, self.encoder.hidden_dim)\n",
        "        if self.encoder.bidirectional:\n",
        "            h_processed = torch.cat([h_processed[:, 0, :, :], h_processed[:, 1, :, :]], dim=2)\n",
        "        else:\n",
        "            h_processed = h_processed.squeeze(1)\n",
        "\n",
        "        c_processed = None\n",
        "        if is_lstm and c_from_enc is not None:\n",
        "            c_processed = c_from_enc.view(self.encoder.n_layers, self.encoder.num_directions,\n",
        "                                         batch_size, self.encoder.hidden_dim)\n",
        "            if self.encoder.bidirectional:\n",
        "                c_processed = torch.cat([c_processed[:, 0, :, :], c_processed[:, 1, :, :]], dim=2)\n",
        "            else:\n",
        "                c_processed = c_processed.squeeze(1)\n",
        "\n",
        "        if self.needs_dim_adaptation:\n",
        "            h_processed = self.fc_adapt_hidden(h_processed)\n",
        "            if is_lstm and c_processed is not None and self.fc_adapt_cell:\n",
        "                c_processed = self.fc_adapt_cell(c_processed)\n",
        "\n",
        "        final_h = torch.zeros(self.decoder.n_layers, batch_size, self.decoder.decoder_hidden_dim, device=self.device)\n",
        "        final_c = torch.zeros(self.decoder.n_layers, batch_size, self.decoder.decoder_hidden_dim, device=self.device) if is_lstm else None\n",
        "\n",
        "        if self.encoder.n_layers == self.decoder.n_layers:\n",
        "            final_h = h_processed\n",
        "            if is_lstm: final_c = c_processed\n",
        "        elif self.encoder.n_layers > self.decoder.n_layers:\n",
        "            final_h = h_processed[-self.decoder.n_layers:, :, :]\n",
        "            if is_lstm and c_processed is not None:\n",
        "                final_c = c_processed[-self.decoder.n_layers:, :, :]\n",
        "        else:\n",
        "            final_h[:self.encoder.n_layers, :, :] = h_processed\n",
        "            if is_lstm and c_processed is not None:\n",
        "                final_c[:self.encoder.n_layers, :, :] = c_processed\n",
        "\n",
        "            if self.encoder.n_layers > 0:\n",
        "                last_h_layer_to_repeat = h_processed[self.encoder.n_layers-1, :, :]\n",
        "                for i in range(self.encoder.n_layers, self.decoder.n_layers):\n",
        "                    final_h[i, :, :] = last_h_layer_to_repeat\n",
        "                    if is_lstm and c_processed is not None:\n",
        "                        last_c_layer_to_repeat = c_processed[self.encoder.n_layers-1, :, :]\n",
        "                        final_c[i, :, :] = last_c_layer_to_repeat\n",
        "\n",
        "        return (final_h, final_c) if is_lstm else final_h\n",
        "\n",
        "    def forward(self, source_seq, source_lengths, target_seq, teacher_forcing_ratio=0.5):\n",
        "        batch_size = source_seq.shape[0]\n",
        "        target_len = target_seq.shape[1]\n",
        "        target_vocab_size = self.decoder.output_vocab_size\n",
        "\n",
        "        outputs_logits = torch.zeros(batch_size, target_len, target_vocab_size).to(self.device)\n",
        "\n",
        "        _, encoder_final_hidden = self.encoder(source_seq, source_lengths)\n",
        "        decoder_hidden = self._adapt_encoder_hidden_for_decoder(encoder_final_hidden)\n",
        "\n",
        "        decoder_input = target_seq[:, 0]\n",
        "\n",
        "        for t in range(target_len - 1):\n",
        "            decoder_output_logits, decoder_hidden = \\\n",
        "                self.decoder(decoder_input, decoder_hidden)\n",
        "\n",
        "            outputs_logits[:, t+1] = decoder_output_logits\n",
        "\n",
        "            teacher_force_this_step = random.random() < teacher_forcing_ratio\n",
        "            top1_predicted_token = decoder_output_logits.argmax(1)\n",
        "\n",
        "            decoder_input = target_seq[:, t+1] if teacher_force_this_step else top1_predicted_token\n",
        "\n",
        "        return outputs_logits\n",
        "\n",
        "    def predict_greedy(self, source_seq, source_lengths, max_output_len=50, target_eos_idx=None):\n",
        "        if source_seq.dim() == 1:\n",
        "            source_seq = source_seq.unsqueeze(0)\n",
        "            source_lengths = torch.tensor([source_lengths if isinstance(source_lengths, int) else len(source_lengths)], device=self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            _, encoder_final_hidden = self.encoder(source_seq, source_lengths)\n",
        "            decoder_hidden = self._adapt_encoder_hidden_for_decoder(encoder_final_hidden)\n",
        "\n",
        "            decoder_input = torch.tensor([self.target_sos_idx], device=self.device)\n",
        "            predicted_indices = []\n",
        "\n",
        "            for _ in range(max_output_len):\n",
        "                decoder_output_logits, decoder_hidden = \\\n",
        "                    self.decoder(decoder_input, decoder_hidden)\n",
        "\n",
        "                top1_predicted_token = decoder_output_logits.argmax(1)\n",
        "                predicted_idx = top1_predicted_token.item()\n",
        "\n",
        "                if target_eos_idx is not None and predicted_idx == target_eos_idx:\n",
        "                    break\n",
        "                predicted_indices.append(predicted_idx)\n",
        "                decoder_input = top1_predicted_token\n",
        "        return predicted_indices\n",
        "\n",
        "    def predict_beam_search(self, source_seq, source_lengths, max_output_len=50, target_eos_idx=None, beam_width=3):\n",
        "        if source_seq.dim() == 1:\n",
        "            source_seq = source_seq.unsqueeze(0)\n",
        "            source_lengths = torch.tensor([source_lengths if isinstance(source_lengths, int) else len(source_lengths)], device=self.device)\n",
        "\n",
        "        batch_size = source_seq.shape[0]\n",
        "        if batch_size != 1:\n",
        "            raise ValueError(\"Beam search predict function currently supports batch_size=1.\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            _, encoder_final_hidden = self.encoder(source_seq, source_lengths)\n",
        "            decoder_hidden_init = self._adapt_encoder_hidden_for_decoder(encoder_final_hidden)\n",
        "\n",
        "            beams = [(0.0, [self.target_sos_idx], decoder_hidden_init)]\n",
        "            completed_sequences = []\n",
        "\n",
        "            for _ in range(max_output_len):\n",
        "                new_beams = []\n",
        "                if len(completed_sequences) >= beam_width and all(b[1][-1] == target_eos_idx for b in beams if b[1]):\n",
        "                    break\n",
        "\n",
        "                for log_prob_beam, seq_beam, hidden_beam in beams:\n",
        "                    if not seq_beam or seq_beam[-1] == target_eos_idx:\n",
        "                        completed_sequences.append((log_prob_beam / len(seq_beam) if len(seq_beam) > 0 else -float('inf'), seq_beam))\n",
        "                        continue\n",
        "\n",
        "                    decoder_input = torch.tensor([seq_beam[-1]], device=self.device)\n",
        "\n",
        "                    decoder_output_logits, next_hidden_beam = \\\n",
        "                        self.decoder(decoder_input, hidden_beam)\n",
        "\n",
        "                    log_probs_next_token = F.log_softmax(decoder_output_logits, dim=1)\n",
        "                    topk_log_probs, topk_indices = torch.topk(log_probs_next_token, beam_width, dim=1)\n",
        "\n",
        "                    for k in range(beam_width):\n",
        "                        next_token_idx = topk_indices[0, k].item()\n",
        "                        token_log_prob = topk_log_probs[0, k].item()\n",
        "\n",
        "                        new_seq = seq_beam + [next_token_idx]\n",
        "                        new_log_prob = log_prob_beam + token_log_prob\n",
        "                        new_beams.append((new_log_prob, new_seq, next_hidden_beam))\n",
        "\n",
        "                if not new_beams: break\n",
        "\n",
        "                new_beams.sort(key=lambda x: x[0], reverse=True)\n",
        "                beams = new_beams[:beam_width]\n",
        "\n",
        "            for log_prob_beam, seq_beam, _ in beams:\n",
        "                if not seq_beam or seq_beam[-1] != target_eos_idx :\n",
        "                    completed_sequences.append((log_prob_beam / len(seq_beam) if len(seq_beam) > 0 else -float('inf'), seq_beam))\n",
        "\n",
        "            if not completed_sequences:\n",
        "                return [target_eos_idx] if target_eos_idx is not None else []\n",
        "\n",
        "            completed_sequences.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "            best_sequence_indices = completed_sequences[0][1]\n",
        "            return best_sequence_indices[1:] if best_sequence_indices and best_sequence_indices[0] == self.target_sos_idx else best_sequence_indices\n",
        "\n",
        "# --- Data Loading and Preprocessing ---\n",
        "class Vocabulary:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.char2index = {PAD_TOKEN: 0, SOS_TOKEN: 1, EOS_TOKEN: 2, UNK_TOKEN: 3}\n",
        "        self.index2char = {0: PAD_TOKEN, 1: SOS_TOKEN, 2: EOS_TOKEN, 3: UNK_TOKEN}\n",
        "        self.char_counts = Counter()\n",
        "        self.n_chars = 4\n",
        "        self.pad_idx = self.char2index[PAD_TOKEN]\n",
        "        self.sos_idx = self.char2index[SOS_TOKEN]\n",
        "        self.eos_idx = self.char2index[EOS_TOKEN]\n",
        "\n",
        "    def add_sequence(self, sequence):\n",
        "        for char in list(sequence):\n",
        "            self.char_counts[char] += 1\n",
        "\n",
        "    def build_vocab(self, min_freq=1):\n",
        "        sorted_chars = sorted(self.char_counts.keys(), key=lambda char: (-self.char_counts[char], char))\n",
        "        for char in sorted_chars:\n",
        "            if self.char_counts[char] >= min_freq:\n",
        "                if char not in self.char2index:\n",
        "                    self.char2index[char] = self.n_chars\n",
        "                    self.index2char[self.n_chars] = char\n",
        "                    self.n_chars += 1\n",
        "\n",
        "    def sequence_to_indices(self, sequence, add_eos=False, add_sos=False):\n",
        "        indices = []\n",
        "        if add_sos:\n",
        "            indices.append(self.sos_idx)\n",
        "        for char in list(sequence):\n",
        "            indices.append(self.char2index.get(char, self.char2index[UNK_TOKEN]))\n",
        "        if add_eos:\n",
        "            indices.append(self.eos_idx)\n",
        "        return indices\n",
        "\n",
        "    def indices_to_sequence(self, indices):\n",
        "        chars = []\n",
        "        for index_val in indices:\n",
        "            char = self.index2char.get(index_val, UNK_TOKEN)\n",
        "            if index_val == self.eos_idx:\n",
        "                break\n",
        "            if index_val != self.sos_idx and index_val != self.pad_idx:\n",
        "                chars.append(char)\n",
        "        return \"\".join(chars)\n",
        "\n",
        "class TransliterationDataset(Dataset):\n",
        "    def __init__(self, file_path, source_vocab, target_vocab, max_len=None):\n",
        "        self.pairs = []\n",
        "        self.source_vocab = source_vocab\n",
        "        self.target_vocab = target_vocab\n",
        "\n",
        "        if not os.path.exists(file_path):\n",
        "            print(f\"ERROR: Data file not found during Dataset init: {file_path}\")\n",
        "            return\n",
        "\n",
        "        print(f\"Loading data from: {file_path}\")\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                for line_num, line in enumerate(f):\n",
        "                    parts = line.strip().split('\\t')\n",
        "                    if len(parts) >= 2:\n",
        "                        target_sequence, source_sequence = parts[0], parts[1]\n",
        "\n",
        "                        if max_len and (len(source_sequence) > max_len or len(target_sequence) > max_len):\n",
        "                            continue\n",
        "                        if not source_sequence or not target_sequence:\n",
        "                            continue\n",
        "                        self.pairs.append((source_sequence, target_sequence))\n",
        "            print(f\"Loaded {len(self.pairs)} pairs from {file_path}.\")\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR: Could not read or process file {file_path}. Error: {e}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if idx >= len(self.pairs):\n",
        "            raise IndexError(\"Index out of bounds for dataset\")\n",
        "        source_str, target_str = self.pairs[idx]\n",
        "        source_indices = self.source_vocab.sequence_to_indices(source_str, add_eos=True)\n",
        "        target_indices = self.target_vocab.sequence_to_indices(target_str, add_sos=True, add_eos=True)\n",
        "        return torch.tensor(source_indices, dtype=torch.long), \\\n",
        "               torch.tensor(target_indices, dtype=torch.long)\n",
        "\n",
        "def collate_fn(batch, pad_idx_source, pad_idx_target):\n",
        "    batch = [item for item in batch if item is not None and item[0] is not None and item[1] is not None]\n",
        "    if not batch:\n",
        "        return None, None, None\n",
        "\n",
        "    source_seqs, target_seqs = zip(*batch)\n",
        "\n",
        "    valid_indices = [i for i, s in enumerate(source_seqs) if len(s) > 0]\n",
        "    if not valid_indices: return None, None, None\n",
        "\n",
        "    source_seqs = [source_seqs[i] for i in valid_indices]\n",
        "    target_seqs = [target_seqs[i] for i in valid_indices]\n",
        "    source_lengths = torch.tensor([len(s) for s in source_seqs], dtype=torch.long)\n",
        "\n",
        "    padded_sources = pad_sequence(source_seqs, batch_first=True, padding_value=pad_idx_source)\n",
        "    padded_targets = pad_sequence(target_seqs, batch_first=True, padding_value=pad_idx_target)\n",
        "    return padded_sources, source_lengths, padded_targets\n",
        "\n",
        "# --- Training and Evaluation Functions ---\n",
        "def train_epoch(model, dataloader, optimizer, criterion, device, clip_value, teacher_forcing_ratio, target_vocab):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    total_correct_train = 0\n",
        "    total_samples_train = 0\n",
        "\n",
        "    if len(dataloader) == 0:\n",
        "        print(\"Warning: Training dataloader is empty.\")\n",
        "        return 0.0, 0.0\n",
        "\n",
        "    for batch_data in tqdm(dataloader, desc=\"Training\", leave=False):\n",
        "        if batch_data[0] is None: continue\n",
        "        sources, source_lengths, targets = batch_data\n",
        "\n",
        "        if sources is None or source_lengths is None or targets is None or source_lengths.numel() == 0 or sources.shape[0] == 0:\n",
        "            print(\"Warning: Empty or invalid batch data after collate_fn in training. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        sources, targets, source_lengths = sources.to(device), targets.to(device), source_lengths.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs_logits = model(sources, source_lengths, targets, teacher_forcing_ratio)\n",
        "\n",
        "        output_dim = outputs_logits.shape[-1]\n",
        "        flat_outputs = outputs_logits[:, 1:].reshape(-1, output_dim)\n",
        "        flat_targets = targets[:, 1:].reshape(-1)\n",
        "\n",
        "        loss = criterion(flat_outputs, flat_targets)\n",
        "        if torch.isnan(loss) or torch.isinf(loss):\n",
        "            print(\"Warning: NaN or Inf loss detected in training. Skipping batch.\")\n",
        "            continue\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        predictions_indices_train = outputs_logits.argmax(dim=2)\n",
        "        for i in range(targets.shape[0]):\n",
        "            pred_str_train = target_vocab.indices_to_sequence(predictions_indices_train[i, 1:].tolist())\n",
        "            true_str_train = target_vocab.indices_to_sequence(targets[i, 1:].tolist())\n",
        "            if pred_str_train == true_str_train:\n",
        "                total_correct_train += 1\n",
        "            total_samples_train += 1\n",
        "\n",
        "    avg_epoch_loss = epoch_loss / len(dataloader) if len(dataloader) > 0 else 0.0\n",
        "    train_accuracy = total_correct_train / total_samples_train if total_samples_train > 0 else 0.0\n",
        "    return avg_epoch_loss, train_accuracy\n",
        "\n",
        "def evaluate(model, dataloader, criterion, device, target_vocab, beam_width=1, target_eos_idx=None):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    if len(dataloader) == 0:\n",
        "        print(\"WARNING: Validation dataloader is empty. Returning 0 loss and 0 accuracy.\")\n",
        "        return 0.0, 0.0\n",
        "\n",
        "    print_debug_once = True\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch_data in enumerate(tqdm(dataloader, desc=\"Evaluating\", leave=False)):\n",
        "            if batch_data[0] is None: continue\n",
        "            sources, source_lengths, targets = batch_data\n",
        "\n",
        "            if sources is None or source_lengths is None or targets is None or source_lengths.numel() == 0 or sources.shape[0] == 0:\n",
        "                print(\"Warning: Empty or invalid batch data in eval after collate_fn. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            sources, targets, source_lengths = sources.to(device), targets.to(device), source_lengths.to(device)\n",
        "\n",
        "            outputs_for_loss = model(sources, source_lengths, targets, teacher_forcing_ratio=0.0)\n",
        "            output_dim = outputs_for_loss.shape[-1]\n",
        "            flat_outputs_for_loss = outputs_for_loss[:, 1:].reshape(-1, output_dim)\n",
        "            flat_targets_for_loss = targets[:, 1:].reshape(-1)\n",
        "\n",
        "            loss = criterion(flat_outputs_for_loss, flat_targets_for_loss)\n",
        "            if torch.isnan(loss) or torch.isinf(loss):\n",
        "                print(\"Warning: NaN or Inf loss detected in evaluation. Skipping batch for loss accumulation.\")\n",
        "            else:\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "            for i in range(sources.shape[0]):\n",
        "                src_single = sources[i:i+1]\n",
        "                src_len_single = source_lengths[i:i+1]\n",
        "\n",
        "                if beam_width > 1 and hasattr(model, 'predict_beam_search'):\n",
        "                    predicted_indices = model.predict_beam_search(src_single, src_len_single,\n",
        "                                                                  max_output_len=targets.size(1),\n",
        "                                                                  target_eos_idx=target_eos_idx,\n",
        "                                                                  beam_width=beam_width)\n",
        "                elif hasattr(model, 'predict_greedy'):\n",
        "                    predicted_indices = model.predict_greedy(src_single, src_len_single,\n",
        "                                                             max_output_len=targets.size(1),\n",
        "                                                             target_eos_idx=target_eos_idx)\n",
        "                else:\n",
        "                    predicted_indices = outputs_for_loss[i:i+1].argmax(dim=2)[0, 1:].tolist()\n",
        "\n",
        "                pred_str = target_vocab.indices_to_sequence(predicted_indices)\n",
        "                true_seq_indices = targets[i, 1:].tolist()\n",
        "                true_str = target_vocab.indices_to_sequence(true_seq_indices)\n",
        "\n",
        "                if pred_str == true_str:\n",
        "                    total_correct += 1\n",
        "                total_samples += 1\n",
        "\n",
        "                if print_debug_once and batch_idx == 0 and i < 1 :\n",
        "                    print(f\"\\n--- Evaluation Debug Sample {i} (Batch {batch_idx}) ---\")\n",
        "                    print(f\"    Source (indices): {src_single[0, :15].tolist()}\")\n",
        "                    print(f\"    Predicted Indices: {predicted_indices[:15]}\")\n",
        "                    print(f\"    Predicted String: '{pred_str}'\")\n",
        "                    print(f\"    True Indices: {true_seq_indices[:15]}\")\n",
        "                    print(f\"    True String: '{true_str}'\")\n",
        "                    print(f\"    Match: {pred_str == true_str}\")\n",
        "\n",
        "            if print_debug_once and batch_idx == 0:\n",
        "                print_debug_once = False\n",
        "\n",
        "    avg_epoch_loss = epoch_loss / len(dataloader) if len(dataloader) > 0 else 0.0\n",
        "    accuracy = total_correct / total_samples if total_samples > 0 else 0.0\n",
        "\n",
        "    print(f\"Evaluation - Total Correct: {total_correct}, Total Samples: {total_samples}, Calculated Accuracy: {accuracy:.4f}, Avg Loss: {avg_epoch_loss:.4f}\")\n",
        "    return avg_epoch_loss, accuracy\n",
        "\n",
        "# --- Main Training Function for W&B ---\n",
        "def train_model():\n",
        "    with wandb.init() as run:\n",
        "        config = wandb.config\n",
        "\n",
        "        run_name = f\"{config.cell_type}_emb{config.embedding_dim}_hid{config.hidden_dim}\" \\\n",
        "                   f\"_encL{config.encoder_layers}_decL{config.decoder_layers}\" \\\n",
        "                   f\"_do{config.dropout_p:.2f}_lr{config.learning_rate:.2e}\" \\\n",
        "                   f\"_encBi{str(config.encoder_bidirectional)[0]}\" \\\n",
        "                   f\"_beam{config.get('beam_width_eval', 1)}\"\n",
        "\n",
        "        if hasattr(run, 'name'): run.name = run_name\n",
        "\n",
        "        DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        current_run_name_for_log = run.name if run.name else run.id\n",
        "        print(f\"Using device: {DEVICE}. Run: {current_run_name_for_log} (ID: {run.id})\")\n",
        "        print(f\"Config: {config}\")\n",
        "\n",
        "        BASE_DATA_DIR = \"/kaggle/input/dakshina-dl-a3/dakshina_dataset_v1.0/hi/\"\n",
        "        DATA_DIR = os.path.join(BASE_DATA_DIR, \"lexicons/\")\n",
        "\n",
        "        if not os.path.exists(DATA_DIR):\n",
        "            print(f\"ERROR: Lexicons directory not found: {DATA_DIR}\")\n",
        "            return 1\n",
        "\n",
        "        train_file = os.path.join(DATA_DIR, \"hi.translit.sampled.train.tsv\")\n",
        "        dev_file = os.path.join(DATA_DIR, \"hi.translit.sampled.dev.tsv\")\n",
        "\n",
        "        if not os.path.exists(train_file) or not os.path.exists(dev_file):\n",
        "            print(f\"ERROR: Train or Dev file not found. Train: {train_file}, Dev: {dev_file}\")\n",
        "            return 1\n",
        "\n",
        "        source_vocab = Vocabulary(\"latin\")\n",
        "        target_vocab = Vocabulary(\"devanagari\")\n",
        "\n",
        "        temp_train_dataset_for_vocab = TransliterationDataset(train_file, source_vocab, target_vocab, max_len=config.max_seq_len)\n",
        "        if not temp_train_dataset_for_vocab.pairs:\n",
        "            print(f\"ERROR: No data loaded for vocab building from {train_file}.\")\n",
        "            return 1\n",
        "        for src_str, tgt_str in temp_train_dataset_for_vocab.pairs:\n",
        "            source_vocab.add_sequence(src_str)\n",
        "            target_vocab.add_sequence(tgt_str)\n",
        "        source_vocab.build_vocab(min_freq=config.vocab_min_freq)\n",
        "        target_vocab.build_vocab(min_freq=config.vocab_min_freq)\n",
        "\n",
        "        print(f\"Source Vocab: {source_vocab.n_chars} chars. Target Vocab: {target_vocab.n_chars} chars.\")\n",
        "\n",
        "        train_dataset = TransliterationDataset(train_file, source_vocab, target_vocab, max_len=config.max_seq_len)\n",
        "        dev_dataset = TransliterationDataset(dev_file, source_vocab, target_vocab, max_len=config.max_seq_len)\n",
        "\n",
        "        if len(train_dataset) == 0 or len(dev_dataset) == 0:\n",
        "            print(f\"ERROR: Train/Dev dataset empty. Train: {len(train_dataset)}, Dev: {len(dev_dataset)}\")\n",
        "            return 1\n",
        "\n",
        "        num_loader_workers = 0\n",
        "        if DEVICE.type == 'cuda' and os.cpu_count() and os.cpu_count() > 1:\n",
        "            num_loader_workers = min(4, os.cpu_count() // 2)\n",
        "\n",
        "        train_dataloader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True,\n",
        "                                      collate_fn=lambda b: collate_fn(b, source_vocab.pad_idx, target_vocab.pad_idx),\n",
        "                                      num_workers=num_loader_workers, pin_memory=True if DEVICE.type == 'cuda' else False, drop_last=True)\n",
        "        dev_dataloader = DataLoader(dev_dataset, batch_size=config.batch_size, shuffle=False,\n",
        "                                     collate_fn=lambda b: collate_fn(b, source_vocab.pad_idx, target_vocab.pad_idx),\n",
        "                                     num_workers=num_loader_workers, pin_memory=True if DEVICE.type == 'cuda' else False, drop_last=False)\n",
        "\n",
        "        if len(train_dataloader) == 0 or len(dev_dataloader) == 0:\n",
        "            print(f\"ERROR: Train/Dev Dataloader empty. Train: {len(train_dataloader)}, Dev: {len(dev_dataloader)}\")\n",
        "            return 1\n",
        "\n",
        "        encoder = Encoder(source_vocab.n_chars, config.embedding_dim, config.hidden_dim,\n",
        "                          config.encoder_layers, config.cell_type, config.dropout_p,\n",
        "                          config.encoder_bidirectional, pad_idx=source_vocab.pad_idx).to(DEVICE)\n",
        "        decoder = Decoder(target_vocab.n_chars, config.embedding_dim,\n",
        "                          config.hidden_dim,\n",
        "                          config.decoder_layers, config.cell_type, config.dropout_p,\n",
        "                          pad_idx=target_vocab.pad_idx).to(DEVICE)\n",
        "        model = Seq2Seq(encoder, decoder, DEVICE, target_vocab.sos_idx).to(DEVICE)\n",
        "\n",
        "        print(f\"Model parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
        "        wandb.watch(model, log=\"all\", log_freq=100)\n",
        "\n",
        "        optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=config.get('lr_scheduler_patience', 3), factor=0.3, verbose=True)\n",
        "        criterion = nn.CrossEntropyLoss(ignore_index=target_vocab.pad_idx)\n",
        "\n",
        "        best_val_accuracy = -1.0\n",
        "        epochs_no_improve = 0\n",
        "        max_epochs_no_improve = config.get('max_epochs_no_improve', 7)\n",
        "\n",
        "        for epoch in range(config.epochs):\n",
        "            train_loss, train_accuracy = train_epoch(model, train_dataloader, optimizer, criterion, DEVICE,\n",
        "                                                     config.clip_value, config.teacher_forcing_ratio, target_vocab)\n",
        "\n",
        "            val_loss, val_accuracy = evaluate(model, dev_dataloader, criterion, DEVICE, target_vocab,\n",
        "                                              beam_width=config.get('beam_width_eval', 1),\n",
        "                                              target_eos_idx=target_vocab.eos_idx)\n",
        "\n",
        "            scheduler.step(val_accuracy)\n",
        "\n",
        "            print(f\"Epoch {epoch+1}/{config.epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_accuracy:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_accuracy:.4f}\")\n",
        "            log_dict = {\n",
        "                \"epoch\": epoch + 1,\n",
        "                \"train_loss\": train_loss if not np.isnan(train_loss) else 0.0,\n",
        "                \"train_accuracy\": train_accuracy if not np.isnan(train_accuracy) else 0.0,\n",
        "                \"val_loss\": val_loss if not np.isnan(val_loss) else 0.0,\n",
        "                \"val_accuracy\": val_accuracy if not np.isnan(val_accuracy) else 0.0,\n",
        "                \"learning_rate\": optimizer.param_groups[0]['lr']\n",
        "            }\n",
        "            wandb.log(log_dict)\n",
        "\n",
        "            current_val_acc = val_accuracy if not np.isnan(val_accuracy) else -1.0\n",
        "            if current_val_acc > best_val_accuracy:\n",
        "                best_val_accuracy = current_val_acc\n",
        "                epochs_no_improve = 0\n",
        "            else:\n",
        "                epochs_no_improve += 1\n",
        "\n",
        "            if config.early_stopping and epochs_no_improve >= max_epochs_no_improve:\n",
        "                print(f\"Early stopping triggered at epoch {epoch+1} after {epochs_no_improve} epochs with no improvement.\")\n",
        "                break\n",
        "\n",
        "        wandb.summary[\"best_val_accuracy\"] = best_val_accuracy if not np.isnan(best_val_accuracy) else 0.0\n",
        "        print(f\"Finished run. Best Validation Accuracy: {best_val_accuracy:.4f}\")\n",
        "        return 0\n",
        "\n",
        "# --- W&B Sweep Configuration ---\n",
        "sweep_config = {\n",
        "    'method': 'bayes',\n",
        "    'metric': {'name': 'val_accuracy', 'goal': 'maximize'},\n",
        "    'parameters': {\n",
        "        'embedding_dim': {'values': [128, 256, 300]},\n",
        "        'hidden_dim': {'values': [256, 512]},\n",
        "        'encoder_layers': {'values': [1, 2]},\n",
        "        'decoder_layers': {'values': [1, 2]},\n",
        "        'cell_type': {'values': ['GRU', 'LSTM']},\n",
        "        'dropout_p': {'values': [0.2, 0.3, 0.4, 0.5]},\n",
        "        'encoder_bidirectional': {'values': [True, False]},\n",
        "        'learning_rate': {'distribution': 'log_uniform_values', 'min': 1e-4, 'max': 3e-3},\n",
        "        'batch_size': {'values': [32, 64, 128]},\n",
        "        'epochs': {'value': 15},\n",
        "        'clip_value': {'value': 1.0},\n",
        "        'teacher_forcing_ratio': {'distribution': 'uniform', 'min': 0.4, 'max': 0.6},\n",
        "        'vocab_min_freq': {'value': 1},\n",
        "        'max_seq_len': {'value': 50},\n",
        "        'early_stopping': {'value': True},\n",
        "        'max_epochs_no_improve': {'values': [5, 7]},\n",
        "        'lr_scheduler_patience': {'values': [2, 3]},\n",
        "        'beam_width_eval': {'values': [1, 3]}\n",
        "    }\n",
        "}\n",
        "\n",
        "# --- Main Execution Block ---\n",
        "if __name__ == '__main__':\n",
        "    try:\n",
        "        if 'KAGGLE_KERNEL_RUN_TYPE' in os.environ:\n",
        "            print(\"Detected Kaggle environment. Ensure WANDB_API_KEY is set as a secret or environment variable.\")\n",
        "            try:\n",
        "                from kaggle_secrets import UserSecretsClient\n",
        "                user_secrets = UserSecretsClient()\n",
        "                wandb_api_key = user_secrets.get_secret(\"WANDB_API_KEY\")\n",
        "                wandb.login(key=wandb_api_key)\n",
        "                print(\"W&B login with Kaggle secret successful.\")\n",
        "            except Exception as e_secret:\n",
        "                print(f\"Could not login with Kaggle secret ({e_secret}). Attempting default login.\")\n",
        "                if \"WANDB_API_KEY\" in os.environ:\n",
        "                    wandb.login()\n",
        "                    print(\"W&B login using environment variable.\")\n",
        "                else:\n",
        "                    print(\"WANDB_API_KEY not found. Please set it up or login manually if prompted.\")\n",
        "                    wandb.login()\n",
        "        else:\n",
        "            wandb.login()\n",
        "        print(\"W&B login process attempted/completed.\")\n",
        "    except Exception as e:\n",
        "        print(f\"W&B login failed: {e}. Please ensure your W&B API key is correctly configured.\")\n",
        "        exit()\n",
        "\n",
        "    SWEEP_PROJECT_NAME = \"DL_A3\"\n",
        "\n",
        "    print(\"Initializing sweep...\")\n",
        "    try:\n",
        "        sweep_id = wandb.sweep(sweep_config, project=SWEEP_PROJECT_NAME)\n",
        "        print(f\"Sweep ID: {sweep_id}\")\n",
        "        print(f\"To run agents, execute: wandb agent YOUR_WANDB_USERNAME/{SWEEP_PROJECT_NAME}/{sweep_id}\")\n",
        "\n",
        "        print(\"Starting a W&B agent (adjust count as needed)...\")\n",
        "        wandb.agent(sweep_id, function=train_model, count=10)\n",
        "    except Exception as e:\n",
        "        print(f\"Could not initialize sweep or run agent. Error: {e}\")\n",
        "\n",
        "    print(\"Sweep agent finished or stopped. Check W&B dashboard for results.\")"
      ],
      "metadata": {
        "id": "8ccMJRPFmpqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q4 train and testing"
      ],
      "metadata": {
        "id": "r-8KVwOcnvkz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import numpy as np\n",
        "import wandb\n",
        "import os\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "import heapq\n",
        "\n",
        "# --- Constants for Special Tokens ---\n",
        "SOS_TOKEN = \"<sos>\"  # Start-of-sequence token\n",
        "EOS_TOKEN = \"<eos>\"  # End-of-sequence token\n",
        "PAD_TOKEN = \"<pad>\"  # Padding token\n",
        "UNK_TOKEN = \"<unk>\"  # Unknown token\n",
        "\n",
        "# --- Model Definitions (Non-Attention Version) ---\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    The Encoder processes the input sequence and produces a context vector (final hidden state).\n",
        "    It uses a recurrent neural network (RNN, GRU, or LSTM).\n",
        "    \"\"\"\n",
        "    def __init__(self, input_vocab_size, embedding_dim, hidden_dim, n_layers,\n",
        "                 cell_type='LSTM', dropout_p=0.1, bidirectional=False, pad_idx=0):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.input_vocab_size = input_vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.cell_type = cell_type.upper()\n",
        "        self.bidirectional = bidirectional\n",
        "        self.num_directions = 2 if bidirectional else 1\n",
        "\n",
        "        # Embedding layer converts input tokens (indices) into dense vectors\n",
        "        self.embedding = nn.Embedding(input_vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "        # Dropout layer for regularization\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "        # Apply dropout to RNN layers if n_layers > 1\n",
        "        rnn_dropout = dropout_p if n_layers > 1 else 0\n",
        "        if self.cell_type == 'RNN':\n",
        "            self.rnn = nn.RNN(embedding_dim, hidden_dim, n_layers,\n",
        "                              dropout=rnn_dropout, batch_first=True, bidirectional=self.bidirectional)\n",
        "        elif self.cell_type == 'GRU':\n",
        "            self.rnn = nn.GRU(embedding_dim, hidden_dim, n_layers,\n",
        "                              dropout=rnn_dropout, batch_first=True, bidirectional=self.bidirectional)\n",
        "        elif self.cell_type == 'LSTM':\n",
        "            self.rnn = nn.LSTM(embedding_dim, hidden_dim, n_layers,\n",
        "                               dropout=rnn_dropout, batch_first=True, bidirectional=self.bidirectional)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported cell type. Choose from 'RNN', 'GRU', 'LSTM'.\")\n",
        "\n",
        "    def forward(self, input_seq, input_lengths):\n",
        "        \"\"\"\n",
        "        Forward pass for the encoder.\n",
        "\n",
        "        Args:\n",
        "            input_seq (torch.Tensor): Padded input sequences of shape (batch_size, seq_len).\n",
        "            input_lengths (torch.Tensor): Lengths of the original sequences in the batch of shape (batch_size,).\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple (None, hidden_state).\n",
        "                   The first element is None because this encoder does not output full sequences for attention.\n",
        "                   The second element is the final hidden state of the encoder, which serves as the context.\n",
        "                   For LSTM, `hidden_state` is a tuple (h, c).\n",
        "        \"\"\"\n",
        "        embedded = self.embedding(input_seq)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        # Pack the padded sequences to handle variable-length inputs efficiently\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(\n",
        "            embedded, input_lengths.cpu(), batch_first=True, enforce_sorted=False\n",
        "        )\n",
        "        # Pass packed sequences through the RNN\n",
        "        # For a non-attention decoder, we primarily need the final hidden state.\n",
        "        _, hidden = self.rnn(packed_embedded)\n",
        "        return None, hidden # Return None for encoder_outputs as they are not used by the simple decoder\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    \"\"\"\n",
        "    The Decoder generates the output sequence one token at a time, conditioned on the\n",
        "    encoder's final hidden state and previously generated tokens.\n",
        "    This version does NOT use attention.\n",
        "    \"\"\"\n",
        "    def __init__(self, output_vocab_size, embedding_dim,\n",
        "                 decoder_hidden_dim, n_layers, cell_type='LSTM', dropout_p=0.1, pad_idx=0):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.output_vocab_size = output_vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.decoder_hidden_dim = decoder_hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.cell_type = cell_type.upper()\n",
        "\n",
        "        self.embedding = nn.Embedding(output_vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "        # Input to decoder RNN is just the embedding of the previous token\n",
        "        rnn_input_dim = embedding_dim\n",
        "\n",
        "        # Apply dropout to RNN layers if n_layers > 1\n",
        "        rnn_dropout = dropout_p if n_layers > 1 else 0\n",
        "        if self.cell_type == 'RNN':\n",
        "            self.rnn = nn.RNN(rnn_input_dim, decoder_hidden_dim, n_layers,\n",
        "                              dropout=rnn_dropout, batch_first=True)\n",
        "        elif self.cell_type == 'GRU':\n",
        "            self.rnn = nn.GRU(rnn_input_dim, decoder_hidden_dim, n_layers,\n",
        "                              dropout=rnn_dropout, batch_first=True)\n",
        "        elif self.cell_type == 'LSTM':\n",
        "            self.rnn = nn.LSTM(rnn_input_dim, decoder_hidden_dim, n_layers,\n",
        "                               dropout=rnn_dropout, batch_first=True)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported cell type. Choose from 'RNN', 'GRU', 'LSTM'.\")\n",
        "\n",
        "        # Output linear layer to project decoder's hidden state to vocabulary size\n",
        "        self.fc_out = nn.Linear(decoder_hidden_dim, output_vocab_size)\n",
        "\n",
        "    def forward(self, input_char, prev_decoder_hidden):\n",
        "        \"\"\"\n",
        "        Forward pass for the decoder.\n",
        "\n",
        "        Args:\n",
        "            input_char (torch.Tensor): A single token (or batch of single tokens) to be embedded,\n",
        "                                       shape (batch_size,).\n",
        "            prev_decoder_hidden (torch.Tensor or tuple): The previous hidden state (and cell state for LSTM)\n",
        "                                                         of the decoder RNN.\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple (prediction_logits, current_decoder_hidden).\n",
        "                   `prediction_logits` are the raw scores for each token in the vocabulary.\n",
        "                   `current_decoder_hidden` is the updated hidden state after processing the input.\n",
        "        \"\"\"\n",
        "        # Add a sequence dimension for RNN input (batch_size, 1, embedding_dim)\n",
        "        input_char = input_char.unsqueeze(1)\n",
        "        embedded = self.embedding(input_char)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        # Pass through the RNN layer\n",
        "        rnn_output, current_decoder_hidden = self.rnn(embedded, prev_decoder_hidden)\n",
        "\n",
        "        # Squeeze the sequence dimension for the linear layer\n",
        "        rnn_output_squeezed = rnn_output.squeeze(1)\n",
        "        # Project to vocabulary size to get logits\n",
        "        prediction_logits = self.fc_out(rnn_output_squeezed)\n",
        "        return prediction_logits, current_decoder_hidden\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    \"\"\"\n",
        "    The main Sequence-to-Sequence model that connects the Encoder and Decoder.\n",
        "    This architecture does NOT use an attention mechanism.\n",
        "    \"\"\"\n",
        "    def __init__(self, encoder, decoder, device, target_sos_idx):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "        self.target_sos_idx = target_sos_idx\n",
        "\n",
        "        # Calculate effective dimensions for hidden state adaptation\n",
        "        encoder_effective_output_dim_per_layer = self.encoder.hidden_dim * self.encoder.num_directions\n",
        "        decoder_rnn_expected_hidden_dim = self.decoder.decoder_hidden_dim\n",
        "\n",
        "        # Check if hidden state dimensions need to be adapted from encoder to decoder\n",
        "        self.needs_dim_adaptation = encoder_effective_output_dim_per_layer != decoder_rnn_expected_hidden_dim\n",
        "        self.fc_adapt_hidden = None\n",
        "        self.fc_adapt_cell = None\n",
        "\n",
        "        # Create linear layers for hidden state adaptation if necessary\n",
        "        if self.needs_dim_adaptation:\n",
        "            self.fc_adapt_hidden = nn.Linear(encoder_effective_output_dim_per_layer, decoder_rnn_expected_hidden_dim)\n",
        "            if self.encoder.cell_type == 'LSTM':\n",
        "                self.fc_adapt_cell = nn.Linear(encoder_effective_output_dim_per_layer, decoder_rnn_expected_hidden_dim)\n",
        "\n",
        "    def _adapt_encoder_hidden_for_decoder(self, encoder_final_hidden_state):\n",
        "        \"\"\"\n",
        "        Adapts the encoder's final hidden state(s) to match the decoder's expected hidden state dimensions and layer count.\n",
        "        Handles bidirectionality by concatenating forward and backward hidden states.\n",
        "        If decoder has more layers than encoder, the last encoder layer's state is repeated.\n",
        "        \"\"\"\n",
        "        is_lstm = self.encoder.cell_type == 'LSTM'\n",
        "        if is_lstm:\n",
        "            h_from_enc, c_from_enc = encoder_final_hidden_state\n",
        "        else:\n",
        "            h_from_enc = encoder_final_hidden_state\n",
        "            c_from_enc = None\n",
        "\n",
        "        batch_size = h_from_enc.size(1)\n",
        "\n",
        "        # Reshape encoder hidden state to (n_layers, num_directions, batch_size, hidden_dim)\n",
        "        h_processed = h_from_enc.view(self.encoder.n_layers, self.encoder.num_directions,\n",
        "                                     batch_size, self.encoder.hidden_dim)\n",
        "        if self.encoder.bidirectional:\n",
        "            # Concatenate forward and backward hidden states across the hidden dimension\n",
        "            h_processed = torch.cat([h_processed[:, 0, :, :], h_processed[:, 1, :, :]], dim=2)\n",
        "        else:\n",
        "            # Remove the single direction dimension\n",
        "            h_processed = h_processed.squeeze(1)\n",
        "\n",
        "        c_processed = None\n",
        "        if is_lstm and c_from_enc is not None:\n",
        "            c_processed = c_from_enc.view(self.encoder.n_layers, self.encoder.num_directions,\n",
        "                                         batch_size, self.encoder.hidden_dim)\n",
        "            if self.encoder.bidirectional:\n",
        "                c_processed = torch.cat([c_processed[:, 0, :, :], c_processed[:, 1, :, :]], dim=2)\n",
        "            else:\n",
        "                c_processed = c_processed.squeeze(1)\n",
        "\n",
        "        # Apply linear transformation if hidden dimensions mismatch\n",
        "        if self.needs_dim_adaptation:\n",
        "            h_processed = self.fc_adapt_hidden(h_processed)\n",
        "            if is_lstm and c_processed is not None and self.fc_adapt_cell:\n",
        "                c_processed = self.fc_adapt_cell(c_processed)\n",
        "\n",
        "        # Initialize decoder's hidden state(s)\n",
        "        final_h = torch.zeros(self.decoder.n_layers, batch_size, self.decoder.decoder_hidden_dim, device=self.device)\n",
        "        final_c = torch.zeros(self.decoder.n_layers, batch_size, self.decoder.decoder_hidden_dim, device=self.device) if is_lstm else None\n",
        "\n",
        "        # Copy or repeat encoder hidden states to match decoder's layer count\n",
        "        if self.encoder.n_layers == self.decoder.n_layers:\n",
        "            final_h = h_processed\n",
        "            if is_lstm: final_c = c_processed\n",
        "        elif self.encoder.n_layers > self.decoder.n_layers:\n",
        "            # Use the last N encoder layers where N is decoder_layers\n",
        "            final_h = h_processed[-self.decoder.n_layers:, :, :]\n",
        "            if is_lstm and c_processed is not None:\n",
        "                final_c = c_processed[-self.decoder.n_layers:, :, :]\n",
        "        else:\n",
        "            # Copy encoder states to the first M decoder layers, and repeat the last encoder layer state for the rest\n",
        "            final_h[:self.encoder.n_layers, :, :] = h_processed\n",
        "            if is_lstm and c_processed is not None:\n",
        "                final_c[:self.encoder.n_layers, :, :] = c_processed\n",
        "\n",
        "            if self.encoder.n_layers > 0:\n",
        "                last_h_layer_to_repeat = h_processed[self.encoder.n_layers-1, :, :]\n",
        "                for i in range(self.encoder.n_layers, self.decoder.n_layers):\n",
        "                    final_h[i, :, :] = last_h_layer_to_repeat\n",
        "                    if is_lstm and c_processed is not None:\n",
        "                        last_c_layer_to_repeat = c_processed[self.encoder.n_layers-1, :, :]\n",
        "                        final_c[i, :, :] = last_c_layer_to_repeat\n",
        "\n",
        "        return (final_h, final_c) if is_lstm else final_h\n",
        "\n",
        "    def forward(self, source_seq, source_lengths, target_seq, teacher_forcing_ratio=0.5):\n",
        "        \"\"\"\n",
        "        Forward pass for the Seq2Seq model during training.\n",
        "\n",
        "        Args:\n",
        "            source_seq (torch.Tensor): Padded source sequences.\n",
        "            source_lengths (torch.Tensor): Lengths of source sequences.\n",
        "            target_seq (torch.Tensor): Padded target sequences (including SOS token).\n",
        "            teacher_forcing_ratio (float): Probability of using actual target token as next input.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Logits for the predicted target sequence.\n",
        "        \"\"\"\n",
        "        batch_size = source_seq.shape[0]\n",
        "        target_len = target_seq.shape[1]\n",
        "        target_vocab_size = self.decoder.output_vocab_size\n",
        "\n",
        "        # Tensor to store decoder outputs\n",
        "        outputs_logits = torch.zeros(batch_size, target_len, target_vocab_size).to(self.device)\n",
        "\n",
        "        # Encode the source sequence to get the initial hidden state for the decoder\n",
        "        # Encoder returns None for encoder_outputs as they are not used by the simple decoder\n",
        "        _, encoder_final_hidden = self.encoder(source_seq, source_lengths)\n",
        "        # Adapt encoder hidden state to decoder's expected shape\n",
        "        decoder_hidden = self._adapt_encoder_hidden_for_decoder(encoder_final_hidden)\n",
        "\n",
        "        # First input to the decoder is the <sos> token\n",
        "        decoder_input = target_seq[:, 0]\n",
        "\n",
        "        # Iterate through the target sequence, predicting one token at a time\n",
        "        for t in range(target_len - 1): # Exclude the last token as we predict up to target_len-1\n",
        "            # Pass input and previous hidden state to the decoder\n",
        "            decoder_output_logits, decoder_hidden = \\\n",
        "                self.decoder(decoder_input, decoder_hidden)\n",
        "\n",
        "            # Store the current step's predictions\n",
        "            outputs_logits[:, t+1] = decoder_output_logits\n",
        "\n",
        "            # Decide whether to use teacher forcing\n",
        "            teacher_force_this_step = random.random() < teacher_forcing_ratio\n",
        "            # Get the top predicted token for the next input if not teacher forcing\n",
        "            top1_predicted_token = decoder_output_logits.argmax(1)\n",
        "\n",
        "            # Use actual target token (teacher forcing) or predicted token for the next step\n",
        "            decoder_input = target_seq[:, t+1] if teacher_force_this_step else top1_predicted_token\n",
        "\n",
        "        return outputs_logits\n",
        "\n",
        "    def predict_greedy(self, source_seq, source_lengths, max_output_len=50, target_eos_idx=None):\n",
        "        \"\"\"\n",
        "        Generates a sequence using greedy decoding.\n",
        "\n",
        "        Args:\n",
        "            source_seq (torch.Tensor): Input source sequence (can be a single example or a batch of 1).\n",
        "            source_lengths (torch.Tensor): Length of the source sequence.\n",
        "            max_output_len (int): Maximum length of the sequence to generate.\n",
        "            target_eos_idx (int, optional): Index of the EOS token in the target vocabulary.\n",
        "\n",
        "        Returns:\n",
        "            list: List of predicted token indices.\n",
        "        \"\"\"\n",
        "        self.eval() # Set model to evaluation mode\n",
        "        # Ensure input sequence is batched (even if batch size is 1)\n",
        "        if source_seq.dim() == 1:\n",
        "            source_seq = source_seq.unsqueeze(0)\n",
        "            source_lengths = torch.tensor([source_lengths if isinstance(source_lengths, int) else len(source_lengths)], device=self.device)\n",
        "\n",
        "        with torch.no_grad(): # Disable gradient calculations\n",
        "            # Encode the source sequence\n",
        "            _, encoder_final_hidden = self.encoder(source_seq, source_lengths)\n",
        "            # Adapt encoder hidden state for decoder initialization\n",
        "            decoder_hidden = self._adapt_encoder_hidden_for_decoder(encoder_final_hidden)\n",
        "\n",
        "            # First input to the decoder is the <sos> token\n",
        "            decoder_input = torch.tensor([self.target_sos_idx], device=self.device)\n",
        "            predicted_indices = []\n",
        "\n",
        "            for _ in range(max_output_len):\n",
        "                # Get decoder output and updated hidden state\n",
        "                decoder_output_logits, decoder_hidden = \\\n",
        "                    self.decoder(decoder_input, decoder_hidden)\n",
        "\n",
        "                # Greedily select the token with the highest probability\n",
        "                top1_predicted_token = decoder_output_logits.argmax(1)\n",
        "                predicted_idx = top1_predicted_token.item()\n",
        "\n",
        "                # Stop if EOS token is predicted\n",
        "                if target_eos_idx is not None and predicted_idx == target_eos_idx:\n",
        "                    break\n",
        "\n",
        "                predicted_indices.append(predicted_idx)\n",
        "                # Use the predicted token as the input for the next step\n",
        "                decoder_input = top1_predicted_token\n",
        "        return predicted_indices\n",
        "\n",
        "    def predict_beam_search(self, source_seq, source_lengths, max_output_len=50, target_eos_idx=None, beam_width=3):\n",
        "        \"\"\"\n",
        "        Generates a sequence using beam search decoding.\n",
        "\n",
        "        Args:\n",
        "            source_seq (torch.Tensor): Input source sequence (must be a single example).\n",
        "            source_lengths (torch.Tensor): Length of the source sequence.\n",
        "            max_output_len (int): Maximum length of the sequence to generate.\n",
        "            target_eos_idx (int, optional): Index of the EOS token in the target vocabulary.\n",
        "            beam_width (int): The number of top sequences to keep at each step.\n",
        "\n",
        "        Returns:\n",
        "            list: List of predicted token indices for the best sequence found.\n",
        "        \"\"\"\n",
        "        self.eval() # Set model to evaluation mode\n",
        "        # Ensure input sequence is batched (even if batch size is 1)\n",
        "        if source_seq.dim() == 1:\n",
        "            source_seq = source_seq.unsqueeze(0)\n",
        "            source_lengths = torch.tensor([source_lengths if isinstance(source_lengths, int) else len(source_lengths)], device=self.device)\n",
        "\n",
        "        if source_seq.shape[0] != 1:\n",
        "            raise ValueError(\"Beam search predict function currently supports batch_size=1.\")\n",
        "\n",
        "        with torch.no_grad(): # Disable gradient calculations\n",
        "            # Encode the source sequence\n",
        "            _, encoder_final_hidden = self.encoder(source_seq, source_lengths)\n",
        "            # Adapt encoder hidden state for decoder initialization\n",
        "            decoder_hidden_init = self._adapt_encoder_hidden_for_decoder(encoder_final_hidden)\n",
        "\n",
        "            # Beams are stored as (log_probability, sequence_of_indices, decoder_hidden_state)\n",
        "            # Use a min-heap to keep track of the top `beam_width` sequences (smallest log_prob at top)\n",
        "            beams = [(0.0, [self.target_sos_idx], decoder_hidden_init)]\n",
        "            completed_sequences = [] # Stores sequences that have predicted EOS\n",
        "\n",
        "            for _ in range(max_output_len):\n",
        "                new_beams = []\n",
        "                all_current_beams_ended = True # Flag to check if all beams have terminated\n",
        "\n",
        "                # Process each beam in the current set of beams\n",
        "                for log_prob_beam, seq_beam, hidden_beam in beams:\n",
        "                    # If this beam has already ended, move it to completed sequences and skip\n",
        "                    if not seq_beam or seq_beam[-1] == target_eos_idx:\n",
        "                        # Normalize log probability by length to counteract bias towards shorter sequences\n",
        "                        completed_sequences.append((log_prob_beam / len(seq_beam) if len(seq_beam) > 0 else -float('inf'), seq_beam))\n",
        "                        continue # Skip to the next beam\n",
        "\n",
        "                    all_current_beams_ended = False # At least one beam is still active\n",
        "\n",
        "                    # Get the last token from the current beam sequence as decoder input\n",
        "                    decoder_input = torch.tensor([seq_beam[-1]], device=self.device)\n",
        "                    # Pass through the decoder to get next token logits and hidden state\n",
        "                    decoder_output_logits, next_hidden_beam = \\\n",
        "                        self.decoder(decoder_input, hidden_beam)\n",
        "\n",
        "                    # Convert logits to log probabilities and get top K candidates\n",
        "                    log_probs_next_token = F.log_softmax(decoder_output_logits, dim=1)\n",
        "                    topk_log_probs, topk_indices = torch.topk(log_probs_next_token, beam_width, dim=1)\n",
        "\n",
        "                    # Expand each current beam into `beam_width` new beams\n",
        "                    for k in range(beam_width):\n",
        "                        next_token_idx = topk_indices[0, k].item()\n",
        "                        token_log_prob = topk_log_probs[0, k].item()\n",
        "\n",
        "                        new_seq = seq_beam + [next_token_idx]\n",
        "                        new_log_prob = log_prob_beam + token_log_prob # Accumulate log probability\n",
        "                        new_beams.append((new_log_prob, new_seq, next_hidden_beam))\n",
        "\n",
        "                # If no new beams were generated (e.g., all beams ended) or all current beams ended, stop\n",
        "                if not new_beams or all_current_beams_ended: break\n",
        "\n",
        "                # Sort all new candidate beams by log probability and select the top `beam_width`\n",
        "                new_beams.sort(key=lambda x: x[0], reverse=True)\n",
        "                beams = new_beams[:beam_width]\n",
        "\n",
        "            # After generation loop, add any remaining active beams to completed sequences\n",
        "            for log_prob_beam, seq_beam, _ in beams:\n",
        "                 if not seq_beam or seq_beam[-1] != target_eos_idx:\n",
        "                    completed_sequences.append((log_prob_beam / len(seq_beam) if len(seq_beam) > 0 else -float('inf'), seq_beam))\n",
        "\n",
        "            # If no sequences were completed (should not happen with good parameters), return default\n",
        "            if not completed_sequences:\n",
        "                return [target_eos_idx] if target_eos_idx is not None else []\n",
        "\n",
        "            # Sort all completed sequences by normalized log probability and return the best one\n",
        "            completed_sequences.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "            best_sequence_indices = completed_sequences[0][1]\n",
        "            # Remove the SOS token if it's at the beginning of the best sequence\n",
        "            return best_sequence_indices[1:] if best_sequence_indices and best_sequence_indices[0] == self.target_sos_idx else best_sequence_indices\n",
        "\n",
        "# --- Data Loading and Preprocessing ---\n",
        "\n",
        "class Vocabulary:\n",
        "    \"\"\"\n",
        "    Manages the mapping between characters and their numerical indices.\n",
        "    Includes special tokens for padding, start-of-sequence, end-of-sequence, and unknown characters.\n",
        "    \"\"\"\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.char2index = {PAD_TOKEN: 0, SOS_TOKEN: 1, EOS_TOKEN: 2, UNK_TOKEN: 3}\n",
        "        self.index2char = {0: PAD_TOKEN, 1: SOS_TOKEN, 2: EOS_TOKEN, 3: UNK_TOKEN}\n",
        "        self.char_counts = Counter()\n",
        "        self.n_chars = 4 # Initialize count with special tokens\n",
        "        self.pad_idx = self.char2index[PAD_TOKEN]\n",
        "        self.sos_idx = self.char2index[SOS_TOKEN]\n",
        "        self.eos_idx = self.char2index[EOS_TOKEN]\n",
        "\n",
        "    def add_sequence(self, sequence):\n",
        "        \"\"\"Adds characters from a sequence to the character counts.\"\"\"\n",
        "        for char in list(sequence): self.char_counts[char] += 1\n",
        "\n",
        "    def build_vocab(self, min_freq=1):\n",
        "        \"\"\"\n",
        "        Builds the vocabulary mapping based on character counts and a minimum frequency.\n",
        "        Characters appearing less than `min_freq` will be treated as UNK_TOKEN.\n",
        "        \"\"\"\n",
        "        sorted_chars = sorted(self.char_counts.keys(), key=lambda char: (-self.char_counts[char], char))\n",
        "        for char in sorted_chars:\n",
        "            if self.char_counts[char] >= min_freq and char not in self.char2index:\n",
        "                self.char2index[char] = self.n_chars\n",
        "                self.index2char[self.n_chars] = char\n",
        "                self.n_chars += 1\n",
        "\n",
        "    def sequence_to_indices(self, sequence, add_eos=False, add_sos=False):\n",
        "        \"\"\"Converts a character sequence into a list of numerical indices.\"\"\"\n",
        "        indices = [self.sos_idx] if add_sos else []\n",
        "        for char in list(sequence): indices.append(self.char2index.get(char, self.char2index[UNK_TOKEN]))\n",
        "        if add_eos: indices.append(self.eos_idx)\n",
        "        return indices\n",
        "\n",
        "    def indices_to_sequence(self, indices):\n",
        "        \"\"\"Converts a list of numerical indices back into a character sequence.\"\"\"\n",
        "        chars = []\n",
        "        for index_val in indices:\n",
        "            if index_val == self.eos_idx: break # Stop at EOS token\n",
        "            if index_val not in [self.sos_idx, self.pad_idx]: # Ignore SOS and PAD tokens in output\n",
        "                chars.append(self.index2char.get(index_val, UNK_TOKEN))\n",
        "        return \"\".join(chars)\n",
        "\n",
        "class TransliterationDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A PyTorch Dataset for loading and preparing transliteration pairs.\n",
        "    Reads data from a TSV file and converts text sequences to token indices.\n",
        "    \"\"\"\n",
        "    def __init__(self, file_path, source_vocab, target_vocab, max_len=50):\n",
        "        self.pairs = []\n",
        "        self.source_vocab = source_vocab\n",
        "        self.target_vocab = target_vocab\n",
        "\n",
        "        if not os.path.exists(file_path):\n",
        "            print(f\"ERROR: Data file not found: {file_path}\")\n",
        "            return\n",
        "\n",
        "        print(f\"Loading data from: {file_path}\")\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                parts = line.strip().split('\\t')\n",
        "                if len(parts) >= 2:\n",
        "                    target, source = parts[0], parts[1]\n",
        "                    # Skip empty sequences or sequences exceeding max_len\n",
        "                    if not source or not target or \\\n",
        "                       (max_len and (len(source) > max_len or len(target) > max_len)):\n",
        "                        continue\n",
        "                    self.pairs.append((source, target))\n",
        "        print(f\"Loaded {len(self.pairs)} pairs from {file_path}.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Returns a source-target pair as Tensors of indices.\"\"\"\n",
        "        source_str, target_str = self.pairs[idx]\n",
        "        source_indices = self.source_vocab.sequence_to_indices(source_str, add_eos=True)\n",
        "        target_indices = self.target_vocab.sequence_to_indices(target_str, add_sos=True, add_eos=True)\n",
        "        return torch.tensor(source_indices, dtype=torch.long), torch.tensor(target_indices, dtype=torch.long)\n",
        "\n",
        "def collate_fn(batch, pad_idx_source, pad_idx_target):\n",
        "    \"\"\"\n",
        "    Custom collate function for DataLoader to handle variable-length sequences.\n",
        "    Pads sequences within a batch to the maximum length of that batch.\n",
        "    \"\"\"\n",
        "    # Filter out any None items if __getitem__ could return them\n",
        "    batch = [item for item in batch if item is not None and len(item[0]) > 0 and len(item[1]) > 0]\n",
        "    if not batch: return None, None, None # Return None if batch is empty after filtering\n",
        "\n",
        "    source_seqs, target_seqs = zip(*batch)\n",
        "    source_lengths = torch.tensor([len(s) for s in source_seqs], dtype=torch.long)\n",
        "\n",
        "    # Pad sequences to the length of the longest sequence in the batch\n",
        "    padded_sources = pad_sequence(source_seqs, batch_first=True, padding_value=pad_idx_source)\n",
        "    padded_targets = pad_sequence(target_seqs, batch_first=True, padding_value=pad_idx_target)\n",
        "\n",
        "    return padded_sources, source_lengths, padded_targets\n",
        "\n",
        "# --- Training and Evaluation Functions ---\n",
        "\n",
        "def _train_one_epoch(model, dataloader, optimizer, criterion, device, clip_value, teacher_forcing_ratio, target_vocab):\n",
        "    \"\"\"\n",
        "    Trains the model for a single epoch.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The Seq2Seq model.\n",
        "        dataloader (DataLoader): DataLoader for the training set.\n",
        "        optimizer (optim.Optimizer): Optimizer for model parameters.\n",
        "        criterion (nn.Module): Loss function (e.g., CrossEntropyLoss).\n",
        "        device (torch.device): Device to run the model on (CPU or GPU).\n",
        "        clip_value (float): Gradient clipping value.\n",
        "        teacher_forcing_ratio (float): Probability of using teacher forcing.\n",
        "        target_vocab (Vocabulary): Vocabulary for the target language.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Average epoch loss and training accuracy.\n",
        "    \"\"\"\n",
        "    model.train() # Set model to training mode\n",
        "    epoch_loss = 0\n",
        "    total_correct_train = 0\n",
        "    total_samples_train = 0\n",
        "\n",
        "    if len(dataloader) == 0:\n",
        "        print(\"Warning: Training dataloader is empty.\")\n",
        "        return 0.0, 0.0\n",
        "\n",
        "    for batch_data in tqdm(dataloader, desc=\"Training\", leave=False):\n",
        "        # Skip invalid batches (e.g., from drop_last=True or filtering)\n",
        "        if batch_data[0] is None: continue\n",
        "        sources, source_lengths, targets = batch_data\n",
        "        if sources is None or sources.shape[0] == 0: continue # Skip if sources tensor is empty\n",
        "\n",
        "        sources, targets, source_lengths = sources.to(device), targets.to(device), source_lengths.to(device)\n",
        "        optimizer.zero_grad() # Clear gradients\n",
        "\n",
        "        # Forward pass: model predicts logits for the target sequence\n",
        "        outputs_logits = model(sources, source_lengths, targets, teacher_forcing_ratio)\n",
        "\n",
        "        # Reshape outputs and targets for CrossEntropyLoss\n",
        "        # We ignore the first token (SOS) in the target for loss calculation\n",
        "        output_dim = outputs_logits.shape[-1]\n",
        "        flat_outputs = outputs_logits[:, 1:].reshape(-1, output_dim)\n",
        "        flat_targets = targets[:, 1:].reshape(-1) # Also remove SOS from targets\n",
        "\n",
        "        loss = criterion(flat_outputs, flat_targets)\n",
        "\n",
        "        # Handle potential NaN/Inf loss (e.g., due to bad gradients or initial parameters)\n",
        "        if not (torch.isnan(loss) or torch.isinf(loss)):\n",
        "            loss.backward() # Backpropagation\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value) # Clip gradients to prevent exploding gradients\n",
        "            optimizer.step() # Update model parameters\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        # Calculate training accuracy (exact sequence match)\n",
        "        predictions_indices_train = outputs_logits.argmax(dim=2) # Get predicted token indices\n",
        "        for i in range(targets.shape[0]):\n",
        "            # Convert predicted and true indices to strings for comparison\n",
        "            pred_str_train = target_vocab.indices_to_sequence(predictions_indices_train[i, 1:].tolist())\n",
        "            true_str_train = target_vocab.indices_to_sequence(targets[i, 1:].tolist())\n",
        "            if pred_str_train == true_str_train:\n",
        "                total_correct_train += 1\n",
        "            total_samples_train += 1\n",
        "\n",
        "    avg_epoch_loss = epoch_loss / len(dataloader) if len(dataloader) > 0 else 0.0\n",
        "    train_accuracy = total_correct_train / total_samples_train if total_samples_train > 0 else 0.0\n",
        "    return avg_epoch_loss, train_accuracy\n",
        "\n",
        "def _evaluate_one_epoch(model, dataloader, criterion, device, target_vocab, beam_width=1, is_test_set=False):\n",
        "    \"\"\"\n",
        "    Evaluates the model's performance on a given dataset (validation or test).\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The Seq2Seq model.\n",
        "        dataloader (DataLoader): DataLoader for the evaluation set.\n",
        "        criterion (nn.Module): Loss function.\n",
        "        device (torch.device): Device to run the model on.\n",
        "        target_vocab (Vocabulary): Vocabulary for the target language.\n",
        "        beam_width (int): Beam width for decoding (1 for greedy, >1 for beam search).\n",
        "        is_test_set (bool): Flag to indicate if this is the final test evaluation (for logging and samples).\n",
        "\n",
        "    Returns:\n",
        "        tuple: Average epoch loss and evaluation accuracy.\n",
        "    \"\"\"\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    epoch_loss = 0\n",
        "    total_correct, total_samples = 0, 0\n",
        "\n",
        "    if len(dataloader) == 0:\n",
        "        print(\"WARNING: Evaluation dataloader is empty.\")\n",
        "        return 0.0, 0.0\n",
        "\n",
        "    desc_prefix = \"Testing\" if is_test_set else \"Validating\"\n",
        "\n",
        "    with torch.no_grad(): # Disable gradient calculations during evaluation\n",
        "        for batch_idx, batch_data in enumerate(tqdm(dataloader, desc=desc_prefix, leave=False)):\n",
        "            if batch_data[0] is None: continue\n",
        "            sources, source_lengths, targets = batch_data\n",
        "            if sources is None or sources.shape[0] == 0: continue\n",
        "\n",
        "            sources, targets, source_lengths = sources.to(device), targets.to(device), source_lengths.to(device)\n",
        "\n",
        "            # Forward pass for loss calculation (using teacher forcing=0.0)\n",
        "            outputs_for_loss = model(sources, source_lengths, targets, teacher_forcing_ratio=0.0)\n",
        "            output_dim = outputs_for_loss.shape[-1]\n",
        "            flat_outputs_for_loss = outputs_for_loss[:, 1:].reshape(-1, output_dim)\n",
        "            flat_targets_for_loss = targets[:, 1:].reshape(-1)\n",
        "\n",
        "            loss = criterion(flat_outputs_for_loss, flat_targets_for_loss)\n",
        "            epoch_loss += loss.item() if not (torch.isnan(loss) or torch.isinf(loss)) else 0\n",
        "\n",
        "            # Generate predictions for accuracy calculation for each item in batch\n",
        "            for i in range(sources.shape[0]):\n",
        "                src_single, src_len_single = sources[i:i+1], source_lengths[i:i+1]\n",
        "\n",
        "                # Choose decoding strategy (beam search or greedy)\n",
        "                if beam_width > 1 and hasattr(model, 'predict_beam_search'):\n",
        "                    predicted_indices = model.predict_beam_search(src_single, src_len_single,\n",
        "                                                                  max_output_len=targets.size(1) + 5, # Allow slightly longer output\n",
        "                                                                  target_eos_idx=target_vocab.eos_idx,\n",
        "                                                                  beam_width=beam_width)\n",
        "                else: # Default to greedy if beam_width is 1 or method not found\n",
        "                     predicted_indices = model.predict_greedy(src_single, src_len_single,\n",
        "                                                             max_output_len=targets.size(1) + 5,\n",
        "                                                             target_eos_idx=target_vocab.eos_idx)\n",
        "\n",
        "                # Convert predicted and true indices to strings for comparison\n",
        "                pred_str = target_vocab.indices_to_sequence(predicted_indices)\n",
        "                true_str = target_vocab.indices_to_sequence(targets[i, 1:].tolist()) # Exclude SOS from true target\n",
        "\n",
        "                # Check for exact match\n",
        "                if pred_str == true_str: total_correct += 1\n",
        "                total_samples += 1\n",
        "\n",
        "                # Print debug samples for the test set\n",
        "                if is_test_set and batch_idx == 0 and i < 3: # Print first 3 samples of the first batch\n",
        "                    print(f\"  Test Example {i} - Source: '{source_vocab.indices_to_sequence(src_single[0].tolist())}'\")\n",
        "                    print(f\"    Pred: '{pred_str}', True: '{true_str}', Match: {pred_str == true_str}\")\n",
        "\n",
        "    avg_epoch_loss = epoch_loss / len(dataloader) if len(dataloader) > 0 else 0.0\n",
        "    accuracy = total_correct / total_samples if total_samples > 0 else 0.0\n",
        "\n",
        "    print(f\"{desc_prefix} - Total Correct: {total_correct}, Total Samples: {total_samples}, Accuracy: {accuracy:.4f}, Avg Loss: {avg_epoch_loss:.4f}\")\n",
        "    return avg_epoch_loss, accuracy\n",
        "\n",
        "# --- Function to Train and Save the Best Model ---\n",
        "\n",
        "def train_and_save_best_model(config, model_save_path, device):\n",
        "    \"\"\"\n",
        "    Performs a dedicated training run using the best hyperparameters found from a sweep.\n",
        "    Saves the model checkpoint with the best validation accuracy.\n",
        "\n",
        "    Args:\n",
        "        config (dict): Dictionary of hyperparameters for this specific training run.\n",
        "        model_save_path (str): Path to save the best model's state_dict.\n",
        "        device (torch.device): Device to run the training on.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if training was successful and a model was saved, False otherwise.\n",
        "    \"\"\"\n",
        "    # Create a unique run name for this dedicated training\n",
        "    run_name_train_best = f\"TRAIN_BEST_{config['cell_type']}_emb{config['embedding_dim']}_hid{config['hidden_dim']}\"\n",
        "\n",
        "    # Initialize W&B run for this dedicated training\n",
        "    with wandb.init(project=\"DL_A3\", name=run_name_train_best, config=config, job_type=\"training_best_model\", reinit=True) as run:\n",
        "        cfg = wandb.config # Access hyperparameters via wandb.config\n",
        "        print(f\"Starting dedicated training for best model with config: {cfg}\")\n",
        "\n",
        "        # --- Data Loading and Vocabulary Building ---\n",
        "        BASE_DATA_DIR = \"/kaggle/input/dakshina-dl-a3/dakshina_dataset_v1.0/hi/\"\n",
        "        DATA_DIR = os.path.join(BASE_DATA_DIR, \"lexicons/\")\n",
        "        train_file = os.path.join(DATA_DIR, \"hi.translit.sampled.train.tsv\")\n",
        "        dev_file = os.path.join(DATA_DIR, \"hi.translit.sampled.dev.tsv\")\n",
        "\n",
        "        source_vocab = Vocabulary(\"latin\")\n",
        "        target_vocab = Vocabulary(\"devanagari\")\n",
        "\n",
        "        # Build vocabulary from the training data\n",
        "        temp_train_ds_vocab = TransliterationDataset(train_file, source_vocab, target_vocab, max_len=cfg.max_seq_len)\n",
        "        if not temp_train_ds_vocab.pairs:\n",
        "            print(f\"ERROR: No training data loaded for vocabulary building from {train_file}.\")\n",
        "            return False # Indicate failure\n",
        "        for src, tgt in temp_train_ds_vocab.pairs:\n",
        "            source_vocab.add_sequence(src)\n",
        "            target_vocab.add_sequence(tgt)\n",
        "        source_vocab.build_vocab(min_freq=cfg.vocab_min_freq)\n",
        "        target_vocab.build_vocab(min_freq=cfg.vocab_min_freq)\n",
        "        print(f\"Vocabs built. Source: {source_vocab.n_chars} unique chars, Target: {target_vocab.n_chars} unique chars.\")\n",
        "\n",
        "        # Create actual datasets for training and validation\n",
        "        train_dataset = TransliterationDataset(train_file, source_vocab, target_vocab, max_len=cfg.max_seq_len)\n",
        "        dev_dataset = TransliterationDataset(dev_file, source_vocab, target_vocab, max_len=cfg.max_seq_len)\n",
        "        if not train_dataset.pairs or not dev_dataset.pairs:\n",
        "            print(f\"ERROR: Train or Dev dataset is empty after filtering. Train: {len(train_dataset.pairs)}, Dev: {len(dev_dataset.pairs)}\")\n",
        "            return False\n",
        "\n",
        "        # --- DataLoader Setup ---\n",
        "        # Adjust number of workers based on CPU availability and device\n",
        "        num_w = 0 if device.type == 'cpu' else min(4, os.cpu_count() // 2 if os.cpu_count() else 0)\n",
        "        train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size_train, shuffle=True,\n",
        "                                  collate_fn=lambda b: collate_fn(b, source_vocab.pad_idx, target_vocab.pad_idx),\n",
        "                                  num_workers=num_w, pin_memory=True if device.type == 'cuda' else False, drop_last=True)\n",
        "        dev_loader = DataLoader(dev_dataset, batch_size=cfg.batch_size_train, shuffle=False,\n",
        "                                collate_fn=lambda b: collate_fn(b, source_vocab.pad_idx, target_vocab.pad_idx),\n",
        "                                num_workers=num_w, pin_memory=True if device.type == 'cuda' else False)\n",
        "        if not train_loader or not dev_loader:\n",
        "            print(f\"ERROR: Train or Dev DataLoader is empty. Train: {len(train_loader)}, Dev: {len(dev_loader)}\")\n",
        "            return False\n",
        "\n",
        "        # --- Model, Optimizer, Loss Function Setup ---\n",
        "        encoder = Encoder(source_vocab.n_chars, cfg.embedding_dim, cfg.hidden_dim,\n",
        "                          cfg.encoder_layers, cfg.cell_type, cfg.dropout_p,\n",
        "                          cfg.encoder_bidirectional, pad_idx=source_vocab.pad_idx).to(device)\n",
        "        decoder = Decoder(target_vocab.n_chars, cfg.embedding_dim, cfg.hidden_dim,\n",
        "                          cfg.decoder_layers, cfg.cell_type, cfg.dropout_p,\n",
        "                          pad_idx=target_vocab.pad_idx).to(device)\n",
        "        model = Seq2Seq(encoder, decoder, device, target_vocab.sos_idx).to(device)\n",
        "        print(f\"Best model parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
        "        wandb.watch(model, log=\"all\", log_freq=100) # Log model weights and gradients to W&B\n",
        "\n",
        "        optimizer = optim.Adam(model.parameters(), lr=cfg.learning_rate_train)\n",
        "        # Scheduler to reduce learning rate if validation accuracy plateaus\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=cfg.lr_scheduler_patience_train, factor=0.3, verbose=True)\n",
        "        criterion = nn.CrossEntropyLoss(ignore_index=target_vocab.pad_idx) # Ignore padding tokens in loss calculation\n",
        "\n",
        "        # --- Training Loop ---\n",
        "        best_val_acc_this_training = -1.0\n",
        "        epochs_no_improve = 0 # Counter for early stopping\n",
        "\n",
        "        for epoch in range(cfg.epochs_train):\n",
        "            train_loss, train_acc = _train_one_epoch(model, train_loader, optimizer, criterion, device,\n",
        "                                                 cfg.clip_value_train, cfg.teacher_forcing_train, target_vocab)\n",
        "            # Evaluate on validation set using greedy decoding during training for simplicity\n",
        "            val_loss, val_acc = _evaluate_one_epoch(model, dev_loader, criterion, device, target_vocab,\n",
        "                                                beam_width=1)\n",
        "\n",
        "            scheduler.step(val_acc) # Update learning rate based on validation accuracy\n",
        "\n",
        "            print(f\"Epoch {epoch+1}/{cfg.epochs_train} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "            # Log metrics to W&B\n",
        "            wandb.log({\"epoch_train_best\": epoch + 1, \"train_loss_best\": train_loss, \"train_acc_best\": train_acc,\n",
        "                       \"val_loss_best\": val_loss, \"val_acc_best\": val_acc, \"lr_best\": optimizer.param_groups[0]['lr']})\n",
        "\n",
        "            # Early stopping logic\n",
        "            if val_acc > best_val_acc_this_training:\n",
        "                best_val_acc_this_training = val_acc\n",
        "                epochs_no_improve = 0\n",
        "                torch.save(model.state_dict(), model_save_path) # Save best model\n",
        "                print(f\"Saved new best model to {model_save_path} (Val Acc: {best_val_acc_this_training:.4f})\")\n",
        "            else:\n",
        "                epochs_no_improve += 1\n",
        "\n",
        "            if epochs_no_improve >= cfg.max_epochs_no_improve_train:\n",
        "                print(f\"Early stopping for best model training at epoch {epoch+1}.\")\n",
        "                break\n",
        "\n",
        "        # Save the final model state if no improvement happened over the initial checkpoint (though this shouldn't be the best)\n",
        "        if not os.path.exists(model_save_path):\n",
        "            torch.save(model.state_dict(), model_save_path)\n",
        "            print(f\"Saved final model state (no improvement over initial) to {model_save_path}\")\n",
        "\n",
        "        wandb.summary[\"final_best_val_accuracy_during_training\"] = best_val_acc_this_training\n",
        "        print(f\"Finished training best model. Best Val Acc: {best_val_acc_this_training:.4f}\")\n",
        "    return True # Indicate successful training\n",
        "\n",
        "# --- Main Execution Block ---\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Determine the device (GPU if available, else CPU)\n",
        "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "    # --- W&B Login ---\n",
        "    try:\n",
        "        # Handle W&B login for Kaggle environments\n",
        "        if 'KAGGLE_KERNEL_RUN_TYPE' in os.environ:\n",
        "            print(\"Detected Kaggle environment. Ensuring WANDB_API_KEY is set.\")\n",
        "            if \"WANDB_API_KEY\" in os.environ:\n",
        "                wandb.login()\n",
        "            else:\n",
        "                # Attempt to get API key from Kaggle secrets\n",
        "                from kaggle_secrets import UserSecretsClient\n",
        "                user_secrets = UserSecretsClient()\n",
        "                wandb_api_key = user_secrets.get_secret(\"WANDB_API_KEY\")\n",
        "                wandb.login(key=wandb_api_key)\n",
        "            print(\"W&B login for Kaggle successful.\")\n",
        "        else:\n",
        "            wandb.login() # Regular W&B login for local environments\n",
        "        print(\"W&B login process attempted/completed.\")\n",
        "    except Exception as e:\n",
        "        print(f\"W&B login failed: {e}. Please ensure your W&B API key is correctly configured.\")\n",
        "        exit() # Exit if W&B login fails\n",
        "\n",
        "    # --- Best Hyperparameters (obtained from a previous sweep) ---\n",
        "    # These hyperparameters are chosen based on the provided sweep results (e.g., from the best run shown in comments).\n",
        "    BEST_HYPERPARAMETERS = {\n",
        "        'embedding_dim': 300,\n",
        "        'hidden_dim': 512,\n",
        "        'encoder_layers': 2,\n",
        "        'decoder_layers': 1,\n",
        "        'cell_type': 'LSTM',\n",
        "        'dropout_p': 0.5,\n",
        "        'encoder_bidirectional': True,\n",
        "        # Training specific parameters for this dedicated run:\n",
        "        'learning_rate_train': 0.000376, # Learning rate from the best sweep run\n",
        "        'batch_size_train': 128,         # Batch size from the best sweep run\n",
        "        'epochs_train': 20,              # Max epochs for this dedicated training\n",
        "        'clip_value_train': 1.0,         # Gradient clipping value\n",
        "        'teacher_forcing_train': 0.5,    # Teacher forcing ratio for this training\n",
        "        'max_epochs_no_improve_train': 7, # Early stopping patience for this training run\n",
        "        'lr_scheduler_patience_train': 3, # Patience for LR scheduler\n",
        "        # Parameters needed for dataset/vocabulary consistency:\n",
        "        'vocab_min_freq': 1,\n",
        "        'max_seq_len': 50,\n",
        "        # Parameters for final test evaluation:\n",
        "        'eval_batch_size': 128,          # Batch size for evaluation on test set\n",
        "        'beam_width_eval': 3             # Beam width for final test evaluation\n",
        "    }\n",
        "    MODEL_SAVE_PATH = \"/kaggle/working/best_model_for_testing.pt\" # Path to save the best model checkpoint\n",
        "\n",
        "    print(f\"Best hyperparameters selected for dedicated training and testing: {BEST_HYPERPARAMETERS}\")\n",
        "\n",
        "    # --- Phase 1: Train and Save the Best Model ---\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"--- Phase 1: Training and Saving Best Model Configuration ---\".center(80))\n",
        "    print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "    training_successful = train_and_save_best_model(BEST_HYPERPARAMETERS, MODEL_SAVE_PATH, DEVICE)\n",
        "\n",
        "    if not training_successful or not os.path.exists(MODEL_SAVE_PATH):\n",
        "        print(\"ERROR: Failed to train and save the best model. Exiting before test evaluation.\")\n",
        "        exit()\n",
        "    print(f\"\\nBest model trained and saved to {MODEL_SAVE_PATH}\")\n",
        "\n",
        "    # --- Phase 2: Load and Evaluate on Test Set ---\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"--- Phase 2: Loading and Evaluating Best Model on Test Set ---\".center(80))\n",
        "    print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "    # --- Data Setup for Test Evaluation ---\n",
        "    # Need to rebuild vocab using training data to ensure consistency before processing test data\n",
        "    BASE_DATA_DIR = \"/kaggle/input/dakshina-dl-a3/dakshina_dataset_v1.0/hi/\"\n",
        "    DATA_DIR = os.path.join(BASE_DATA_DIR, \"lexicons/\")\n",
        "    train_file_for_vocab = os.path.join(DATA_DIR, \"hi.translit.sampled.train.tsv\")\n",
        "    test_file = os.path.join(DATA_DIR, \"hi.translit.sampled.test.tsv\")\n",
        "\n",
        "    source_vocab_test = Vocabulary(\"latin_test\")\n",
        "    target_vocab_test = Vocabulary(\"devanagari_test\")\n",
        "\n",
        "    # Build vocabulary using the training dataset (crucial for consistent tokenization)\n",
        "    temp_train_ds_test_vocab = TransliterationDataset(train_file_for_vocab, source_vocab_test, target_vocab_test,\n",
        "                                                max_len=BEST_HYPERPARAMETERS['max_seq_len'])\n",
        "    if not temp_train_ds_test_vocab.pairs:\n",
        "        print(f\"ERROR: No data loaded for vocabulary building from {train_file_for_vocab}.\")\n",
        "        exit()\n",
        "    for src, tgt in temp_train_ds_test_vocab.pairs:\n",
        "        source_vocab_test.add_sequence(src)\n",
        "        target_vocab_test.add_sequence(tgt)\n",
        "    source_vocab_test.build_vocab(min_freq=BEST_HYPERPARAMETERS['vocab_min_freq'])\n",
        "    target_vocab_test.build_vocab(min_freq=BEST_HYPERPARAMETERS['vocab_min_freq'])\n",
        "    print(f\"Test Vocab built from training data. Source: {source_vocab_test.n_chars} chars. Target: {target_vocab_test.n_chars} chars.\")\n",
        "\n",
        "    # Create the test dataset\n",
        "    test_dataset = TransliterationDataset(test_file, source_vocab_test, target_vocab_test,\n",
        "                                          max_len=BEST_HYPERPARAMETERS['max_seq_len'])\n",
        "    if not test_dataset.pairs:\n",
        "        print(f\"ERROR: Test dataset is empty from {test_file}.\")\n",
        "        exit()\n",
        "\n",
        "    # Setup test DataLoader\n",
        "    num_w_test = 0 if DEVICE.type == 'cpu' else min(4, os.cpu_count() // 2 if os.cpu_count() else 0)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=BEST_HYPERPARAMETERS['eval_batch_size'], shuffle=False,\n",
        "                                 collate_fn=lambda b: collate_fn(b, source_vocab_test.pad_idx, target_vocab_test.pad_idx),\n",
        "                                 num_workers=num_w_test, pin_memory=True if DEVICE.type == 'cuda' else False)\n",
        "    if not test_dataloader:\n",
        "        print(f\"ERROR: Test DataLoader is empty.\")\n",
        "        exit()\n",
        "\n",
        "    # --- Initialize Model for Testing and Load Weights ---\n",
        "    # Instantiate the model architecture with the same hyperparameters as the trained model\n",
        "    encoder_test = Encoder(source_vocab_test.n_chars, BEST_HYPERPARAMETERS['embedding_dim'], BEST_HYPERPARAMETERS['hidden_dim'],\n",
        "                           BEST_HYPERPARAMETERS['encoder_layers'], BEST_HYPERPARAMETERS['cell_type'], BEST_HYPERPARAMETERS['dropout_p'],\n",
        "                           BEST_HYPERPARAMETERS['encoder_bidirectional'], pad_idx=source_vocab_test.pad_idx).to(DEVICE)\n",
        "    decoder_test = Decoder(target_vocab_test.n_chars, BEST_HYPERPARAMETERS['embedding_dim'], BEST_HYPERPARAMETERS['hidden_dim'],\n",
        "                           BEST_HYPERPARAMETERS['decoder_layers'], BEST_HYPERPARAMETERS['cell_type'], BEST_HYPERPARAMETERS['dropout_p'],\n",
        "                           pad_idx=target_vocab_test.pad_idx).to(DEVICE)\n",
        "    model_test = Seq2Seq(encoder_test, decoder_test, DEVICE, target_vocab_test.sos_idx).to(DEVICE)\n",
        "\n",
        "    print(f\"Loading weights into test model from: {MODEL_SAVE_PATH}\")\n",
        "    model_test.load_state_dict(torch.load(MODEL_SAVE_PATH, map_location=DEVICE)) # Load saved weights\n",
        "    print(\"Weights loaded successfully for test evaluation.\")\n",
        "\n",
        "    # --- Evaluate on Test Set ---\n",
        "    criterion_test = nn.CrossEntropyLoss(ignore_index=target_vocab_test.pad_idx) # Use the same loss criterion\n",
        "    test_loss, test_accuracy = _evaluate_one_epoch(model_test, test_dataloader, criterion_test, DEVICE,\n",
        "                                                   target_vocab_test,\n",
        "                                                   beam_width=BEST_HYPERPARAMETERS.get('beam_width_eval', 1), # Use the eval beam width\n",
        "                                                   is_test_set=True) # Flag for printing test samples\n",
        "\n",
        "    print(f\"\\n\" + \"=\"*80)\n",
        "    print(f\"--- FINAL TEST SET PERFORMANCE ---\".center(80))\n",
        "    print(f\"=\"*80)\n",
        "    print(f\"  Model Checkpoint: '{MODEL_SAVE_PATH}'\")\n",
        "    print(f\"  Hyperparameters used for training and testing: {BEST_HYPERPARAMETERS}\")\n",
        "    print(f\"  Test Loss (avg per batch): {test_loss:.4f}\")\n",
        "    print(f\"  Test Accuracy (Exact Match): {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
        "    print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "    # --- Log Final Test Results to W&B ---\n",
        "    try:\n",
        "        run_name_final_test = f\"FINAL_TEST_EVAL_{BEST_HYPERPARAMETERS['cell_type']}_beam{BEST_HYPERPARAMETERS['beam_width_eval']}\"\n",
        "        # Start a new W&B run to log only the final test results\n",
        "        with wandb.init(project=\"DL_A3\", name=run_name_final_test, config=BEST_HYPERPARAMETERS, job_type=\"final_evaluation\", reinit=True) as test_run:\n",
        "            test_run.summary[\"final_test_accuracy\"] = test_accuracy\n",
        "            test_run.summary[\"final_test_loss\"] = test_loss\n",
        "            test_run.summary[\"model_checkpoint_used\"] = MODEL_SAVE_PATH\n",
        "            print(\"Final test results logged to W&B.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not log final test results to W&B: {e}\")"
      ],
      "metadata": {
        "id": "_iM8i33xnuyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_ykkmlrcqitF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q5 Attention based model"
      ],
      "metadata": {
        "id": "iaoKVxUbqmlo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import numpy as np\n",
        "import wandb\n",
        "import os\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "import heapq\n",
        "\n",
        "# --- Constants for Special Tokens ---\n",
        "SOS_TOKEN = \"<sos>\"  # Start-of-sequence token\n",
        "EOS_TOKEN = \"<eos>\"  # End-of-sequence token\n",
        "PAD_TOKEN = \"<pad>\"  # Padding token\n",
        "UNK_TOKEN = \"<unk>\"  # Unknown token\n",
        "\n",
        "# --- Model Definitions (Non-Attention Version) ---\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    The Encoder processes the input sequence and produces a context vector (final hidden state).\n",
        "    It uses a recurrent neural network (RNN, GRU, or LSTM).\n",
        "    \"\"\"\n",
        "    def __init__(self, input_vocab_size, embedding_dim, hidden_dim, n_layers,\n",
        "                 cell_type='LSTM', dropout_p=0.1, bidirectional=False, pad_idx=0):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.input_vocab_size = input_vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.cell_type = cell_type.upper()\n",
        "        self.bidirectional = bidirectional\n",
        "        self.num_directions = 2 if bidirectional else 1\n",
        "\n",
        "        self.embedding = nn.Embedding(input_vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "        rnn_dropout = dropout_p if n_layers > 1 else 0\n",
        "        if self.cell_type == 'RNN':\n",
        "            self.rnn = nn.RNN(embedding_dim, hidden_dim, n_layers,\n",
        "                              dropout=rnn_dropout, batch_first=True, bidirectional=self.bidirectional)\n",
        "        elif self.cell_type == 'GRU':\n",
        "            self.rnn = nn.GRU(embedding_dim, hidden_dim, n_layers,\n",
        "                              dropout=rnn_dropout, batch_first=True, bidirectional=self.bidirectional)\n",
        "        elif self.cell_type == 'LSTM':\n",
        "            self.rnn = nn.LSTM(embedding_dim, hidden_dim, n_layers,\n",
        "                               dropout=rnn_dropout, batch_first=True, bidirectional=self.bidirectional)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported cell type. Choose from 'RNN', 'GRU', 'LSTM'.\")\n",
        "\n",
        "    def forward(self, input_seq, input_lengths):\n",
        "        \"\"\"\n",
        "        Forward pass for the encoder.\n",
        "\n",
        "        Args:\n",
        "            input_seq (torch.Tensor): Padded input sequences of shape (batch_size, seq_len).\n",
        "            input_lengths (torch.Tensor): Lengths of the original sequences in the batch of shape (batch_size,).\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple (None, hidden_state).\n",
        "                   The first element is None because this encoder does not output full sequences for attention.\n",
        "                   The second element is the final hidden state of the encoder, which serves as the context.\n",
        "                   For LSTM, `hidden_state` is a tuple (h, c).\n",
        "        \"\"\"\n",
        "        embedded = self.embedding(input_seq)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        # Pack the padded sequences to handle variable-length inputs efficiently\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(\n",
        "            embedded, input_lengths.cpu(), batch_first=True, enforce_sorted=False\n",
        "        )\n",
        "        # Pass packed sequences through the RNN\n",
        "        _, hidden = self.rnn(packed_embedded)\n",
        "        return None, hidden\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    \"\"\"\n",
        "    The Decoder generates the output sequence one token at a time, conditioned on the\n",
        "    encoder's final hidden state and previously generated tokens.\n",
        "    This version does NOT use attention.\n",
        "    \"\"\"\n",
        "    def __init__(self, output_vocab_size, embedding_dim,\n",
        "                 decoder_hidden_dim, n_layers, cell_type='LSTM', dropout_p=0.1, pad_idx=0):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.output_vocab_size = output_vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.decoder_hidden_dim = decoder_hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.cell_type = cell_type.upper()\n",
        "\n",
        "        self.embedding = nn.Embedding(output_vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "        rnn_input_dim = embedding_dim\n",
        "\n",
        "        rnn_dropout = dropout_p if n_layers > 1 else 0\n",
        "        if self.cell_type == 'RNN':\n",
        "            self.rnn = nn.RNN(rnn_input_dim, decoder_hidden_dim, n_layers,\n",
        "                              dropout=rnn_dropout, batch_first=True)\n",
        "        elif self.cell_type == 'GRU':\n",
        "            self.rnn = nn.GRU(rnn_input_dim, decoder_hidden_dim, n_layers,\n",
        "                              dropout=rnn_dropout, batch_first=True)\n",
        "        elif self.cell_type == 'LSTM':\n",
        "            self.rnn = nn.LSTM(rnn_input_dim, decoder_hidden_dim, n_layers,\n",
        "                               dropout=rnn_dropout, batch_first=True)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported cell type. Choose from 'RNN', 'GRU', 'LSTM'.\")\n",
        "\n",
        "        # Output linear layer to project decoder's hidden state to vocabulary size\n",
        "        self.fc_out = nn.Linear(decoder_hidden_dim, output_vocab_size)\n",
        "\n",
        "    def forward(self, input_char, prev_decoder_hidden):\n",
        "        \"\"\"\n",
        "        Forward pass for the decoder.\n",
        "\n",
        "        Args:\n",
        "            input_char (torch.Tensor): A single token (or batch of single tokens) to be embedded,\n",
        "                                       shape (batch_size,).\n",
        "            prev_decoder_hidden (torch.Tensor or tuple): The previous hidden state (and cell state for LSTM)\n",
        "                                                         of the decoder RNN.\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple (prediction_logits, current_decoder_hidden).\n",
        "                   `prediction_logits` are the raw scores for each token in the vocabulary.\n",
        "                   `current_decoder_hidden` is the updated hidden state after processing the input.\n",
        "        \"\"\"\n",
        "        # Add a sequence dimension for RNN input (batch_size, 1, embedding_dim)\n",
        "        input_char = input_char.unsqueeze(1)\n",
        "        embedded = self.embedding(input_char)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        # Pass through the RNN layer\n",
        "        rnn_output, current_decoder_hidden = self.rnn(embedded, prev_decoder_hidden)\n",
        "\n",
        "        # Squeeze the sequence dimension for the linear layer\n",
        "        rnn_output_squeezed = rnn_output.squeeze(1)\n",
        "        # Project to vocabulary size to get logits\n",
        "        prediction_logits = self.fc_out(rnn_output_squeezed)\n",
        "        return prediction_logits, current_decoder_hidden\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    \"\"\"\n",
        "    The main Sequence-to-Sequence model that connects the Encoder and Decoder.\n",
        "    This architecture does NOT use an attention mechanism.\n",
        "    \"\"\"\n",
        "    def __init__(self, encoder, decoder, device, target_sos_idx):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "        self.target_sos_idx = target_sos_idx\n",
        "\n",
        "        # Calculate effective dimensions for hidden state adaptation\n",
        "        encoder_effective_output_dim_per_layer = self.encoder.hidden_dim * self.encoder.num_directions\n",
        "        decoder_rnn_expected_hidden_dim = self.decoder.decoder_hidden_dim\n",
        "\n",
        "        # Check if hidden state dimensions need to be adapted from encoder to decoder\n",
        "        self.needs_dim_adaptation = encoder_effective_output_dim_per_layer != decoder_rnn_expected_hidden_dim\n",
        "        self.fc_adapt_hidden = None\n",
        "        self.fc_adapt_cell = None\n",
        "\n",
        "        # Create linear layers for hidden state adaptation if necessary\n",
        "        if self.needs_dim_adaptation:\n",
        "            self.fc_adapt_hidden = nn.Linear(encoder_effective_output_dim_per_layer, decoder_rnn_expected_hidden_dim)\n",
        "            if self.encoder.cell_type == 'LSTM':\n",
        "                self.fc_adapt_cell = nn.Linear(encoder_effective_output_dim_per_layer, decoder_rnn_expected_hidden_dim)\n",
        "\n",
        "    def _adapt_encoder_hidden_for_decoder(self, encoder_final_hidden_state):\n",
        "        \"\"\"\n",
        "        Adapts the encoder's final hidden state(s) to match the decoder's expected hidden state dimensions and layer count.\n",
        "        Handles bidirectionality by concatenating forward and backward hidden states.\n",
        "        If decoder has more layers than encoder, the last encoder layer's state is repeated.\n",
        "        \"\"\"\n",
        "        is_lstm = self.encoder.cell_type == 'LSTM'\n",
        "        if is_lstm:\n",
        "            h_from_enc, c_from_enc = encoder_final_hidden_state\n",
        "        else:\n",
        "            h_from_enc = encoder_final_hidden_state\n",
        "            c_from_enc = None\n",
        "\n",
        "        batch_size = h_from_enc.size(1)\n",
        "\n",
        "        # Reshape encoder hidden state\n",
        "        h_processed = h_from_enc.view(self.encoder.n_layers, self.encoder.num_directions,\n",
        "                                     batch_size, self.encoder.hidden_dim)\n",
        "        if self.encoder.bidirectional:\n",
        "            h_processed = torch.cat([h_processed[:, 0, :, :], h_processed[:, 1, :, :]], dim=2)\n",
        "        else:\n",
        "            h_processed = h_processed.squeeze(1)\n",
        "\n",
        "        c_processed = None\n",
        "        if is_lstm and c_from_enc is not None:\n",
        "            c_processed = c_from_enc.view(self.encoder.n_layers, self.encoder.num_directions,\n",
        "                                         batch_size, self.encoder.hidden_dim)\n",
        "            if self.encoder.bidirectional:\n",
        "                c_processed = torch.cat([c_processed[:, 0, :, :], c_processed[:, 1, :, :]], dim=2)\n",
        "            else:\n",
        "                c_processed = c_processed.squeeze(1)\n",
        "\n",
        "        # Apply linear transformation if hidden dimensions mismatch\n",
        "        if self.needs_dim_adaptation:\n",
        "            h_processed = self.fc_adapt_hidden(h_processed)\n",
        "            if is_lstm and c_processed is not None and self.fc_adapt_cell:\n",
        "                c_processed = self.fc_adapt_cell(c_processed)\n",
        "\n",
        "        # Initialize decoder's hidden state(s)\n",
        "        final_h = torch.zeros(self.decoder.n_layers, batch_size, self.decoder.decoder_hidden_dim, device=self.device)\n",
        "        final_c = torch.zeros(self.decoder.n_layers, batch_size, self.decoder.decoder_hidden_dim, device=self.device) if is_lstm else None\n",
        "\n",
        "        # Copy or repeat encoder hidden states to match decoder's layer count\n",
        "        if self.encoder.n_layers == self.decoder.n_layers:\n",
        "            final_h = h_processed\n",
        "            if is_lstm: final_c = c_processed\n",
        "        elif self.encoder.n_layers > self.decoder.n_layers:\n",
        "            final_h = h_processed[-self.decoder.n_layers:, :, :]\n",
        "            if is_lstm and c_processed is not None:\n",
        "                final_c = c_processed[-self.decoder.n_layers:, :, :]\n",
        "        else:\n",
        "            final_h[:self.encoder.n_layers, :, :] = h_processed\n",
        "            if is_lstm and c_processed is not None:\n",
        "                final_c[:self.encoder.n_layers, :, :] = c_processed\n",
        "\n",
        "            if self.encoder.n_layers > 0:\n",
        "                last_h_layer_to_repeat = h_processed[self.encoder.n_layers-1, :, :]\n",
        "                for i in range(self.encoder.n_layers, self.decoder.n_layers):\n",
        "                    final_h[i, :, :] = last_h_layer_to_repeat\n",
        "                    if is_lstm and c_processed is not None:\n",
        "                        last_c_layer_to_repeat = c_processed[self.encoder.n_layers-1, :, :]\n",
        "                        final_c[i, :, :] = last_c_layer_to_repeat\n",
        "\n",
        "        return (final_h, final_c) if is_lstm else final_h\n",
        "\n",
        "    def forward(self, source_seq, source_lengths, target_seq, teacher_forcing_ratio=0.5):\n",
        "        \"\"\"\n",
        "        Forward pass for the Seq2Seq model during training.\n",
        "\n",
        "        Args:\n",
        "            source_seq (torch.Tensor): Padded source sequences.\n",
        "            source_lengths (torch.Tensor): Lengths of source sequences.\n",
        "            target_seq (torch.Tensor): Padded target sequences (including SOS token).\n",
        "            teacher_forcing_ratio (float): Probability of using actual target token as next input.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Logits for the predicted target sequence.\n",
        "        \"\"\"\n",
        "        batch_size = source_seq.shape[0]\n",
        "        target_len = target_seq.shape[1]\n",
        "        target_vocab_size = self.decoder.output_vocab_size\n",
        "        outputs_logits = torch.zeros(batch_size, target_len, target_vocab_size).to(self.device)\n",
        "\n",
        "        # Encode the source sequence to get the initial hidden state for the decoder\n",
        "        _, encoder_final_hidden = self.encoder(source_seq, source_lengths)\n",
        "        # Adapt encoder hidden state to decoder's expected shape\n",
        "        decoder_hidden = self._adapt_encoder_hidden_for_decoder(encoder_final_hidden)\n",
        "\n",
        "        # First input to the decoder is the <sos> token\n",
        "        decoder_input = target_seq[:, 0]\n",
        "\n",
        "        # Iterate through the target sequence, predicting one token at a time\n",
        "        for t in range(target_len - 1):\n",
        "            decoder_output_logits, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
        "            outputs_logits[:, t+1] = decoder_output_logits\n",
        "\n",
        "            # Decide whether to use teacher forcing\n",
        "            teacher_force_this_step = random.random() < teacher_forcing_ratio\n",
        "            top1_predicted_token = decoder_output_logits.argmax(1)\n",
        "\n",
        "            # Use actual target token (teacher forcing) or predicted token for the next step\n",
        "            decoder_input = target_seq[:, t+1] if teacher_force_this_step else top1_predicted_token\n",
        "        return outputs_logits\n",
        "\n",
        "    def predict_greedy(self, source_seq, source_lengths, max_output_len=50, target_eos_idx=None):\n",
        "        \"\"\"\n",
        "        Generates a sequence using greedy decoding.\n",
        "\n",
        "        Args:\n",
        "            source_seq (torch.Tensor): Input source sequence (can be a single example or a batch of 1).\n",
        "            source_lengths (torch.Tensor): Length of the source sequence.\n",
        "            max_output_len (int): Maximum length of the sequence to generate.\n",
        "            target_eos_idx (int, optional): Index of the EOS token in the target vocabulary.\n",
        "\n",
        "        Returns:\n",
        "            list: List of predicted token indices.\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        if source_seq.dim() == 1:\n",
        "            source_seq = source_seq.unsqueeze(0)\n",
        "            source_lengths = torch.tensor([source_lengths if isinstance(source_lengths, int) else len(source_lengths)], device=self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            _, encoder_final_hidden = self.encoder(source_seq, source_lengths)\n",
        "            decoder_hidden = self._adapt_encoder_hidden_for_decoder(encoder_final_hidden)\n",
        "            decoder_input = torch.tensor([self.target_sos_idx], device=self.device)\n",
        "            predicted_indices = []\n",
        "            for _ in range(max_output_len):\n",
        "                decoder_output_logits, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
        "                top1_predicted_token = decoder_output_logits.argmax(1)\n",
        "                predicted_idx = top1_predicted_token.item()\n",
        "                if target_eos_idx is not None and predicted_idx == target_eos_idx:\n",
        "                    break\n",
        "                predicted_indices.append(predicted_idx)\n",
        "                decoder_input = top1_predicted_token\n",
        "        return predicted_indices\n",
        "\n",
        "    def predict_beam_search(self, source_seq, source_lengths, max_output_len=50, target_eos_idx=None, beam_width=3):\n",
        "        \"\"\"\n",
        "        Generates a sequence using beam search decoding.\n",
        "\n",
        "        Args:\n",
        "            source_seq (torch.Tensor): Input source sequence (must be a single example).\n",
        "            source_lengths (torch.Tensor): Length of the source sequence.\n",
        "            max_output_len (int): Maximum length of the sequence to generate.\n",
        "            target_eos_idx (int, optional): Index of the EOS token.\n",
        "            beam_width (int): The number of top sequences to keep at each step.\n",
        "\n",
        "        Returns:\n",
        "            list: List of predicted token indices for the best sequence found.\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        if source_seq.dim() == 1:\n",
        "            source_seq = source_seq.unsqueeze(0)\n",
        "            source_lengths = torch.tensor([source_lengths if isinstance(source_lengths, int) else len(source_lengths)], device=self.device)\n",
        "\n",
        "        if source_seq.shape[0] != 1:\n",
        "            raise ValueError(\"Beam search predict function currently supports batch_size=1.\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            _, encoder_final_hidden = self.encoder(source_seq, source_lengths)\n",
        "            decoder_hidden_init = self._adapt_encoder_hidden_for_decoder(encoder_final_hidden)\n",
        "\n",
        "            # Beams are stored as (cumulative_log_probability, sequence_of_indices, decoder_hidden_state)\n",
        "            beams = [(0.0, [self.target_sos_idx], decoder_hidden_init)]\n",
        "            completed_sequences = []\n",
        "\n",
        "            for _ in range(max_output_len):\n",
        "                new_beams = []\n",
        "                all_current_beams_ended = True\n",
        "                for log_prob_beam, seq_beam, hidden_beam in beams:\n",
        "                    # If this beam has already ended, move it to completed sequences and skip expansion\n",
        "                    if not seq_beam or seq_beam[-1] == target_eos_idx:\n",
        "                        # Normalize log probability by length to counteract bias towards shorter sequences\n",
        "                        completed_sequences.append((log_prob_beam / len(seq_beam) if len(seq_beam) > 0 else -float('inf'), seq_beam))\n",
        "                        continue\n",
        "                    all_current_beams_ended = False\n",
        "\n",
        "                    decoder_input = torch.tensor([seq_beam[-1]], device=self.device)\n",
        "                    decoder_output_logits, next_hidden_beam = self.decoder(decoder_input, hidden_beam)\n",
        "\n",
        "                    # Get top K next tokens (log probabilities)\n",
        "                    log_probs_next_token = F.log_softmax(decoder_output_logits, dim=1)\n",
        "                    topk_log_probs, topk_indices = torch.topk(log_probs_next_token, beam_width, dim=1)\n",
        "\n",
        "                    # Expand each current beam into `beam_width` new beams\n",
        "                    for k in range(beam_width):\n",
        "                        next_token_idx = topk_indices[0, k].item()\n",
        "                        token_log_prob = topk_log_probs[0, k].item()\n",
        "\n",
        "                        new_seq = seq_beam + [next_token_idx]\n",
        "                        new_log_prob = log_prob_beam + token_log_prob\n",
        "                        new_beams.append((new_log_prob, new_seq, next_hidden_beam))\n",
        "\n",
        "                if not new_beams or all_current_beams_ended:\n",
        "                    break # No new beams to explore or all current beams ended\n",
        "\n",
        "                # Sort all new candidate beams by their log probability and keep top `beam_width`\n",
        "                new_beams.sort(key=lambda x: x[0], reverse=True)\n",
        "                beams = new_beams[:beam_width]\n",
        "\n",
        "            # Add any remaining active beams to completed sequences\n",
        "            for log_prob_beam, seq_beam, _ in beams:\n",
        "                completed_sequences.append((log_prob_beam / len(seq_beam) if len(seq_beam) > 0 else -float('inf'), seq_beam))\n",
        "\n",
        "            if not completed_sequences:\n",
        "                # Fallback if somehow no sequences were completed or started\n",
        "                return [target_eos_idx] if target_eos_idx is not None else []\n",
        "            completed_sequences.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "            best_sequence_indices = completed_sequences[0][1]\n",
        "            # Remove SOS from the beginning if it was added\n",
        "            return best_sequence_indices[1:] if best_sequence_indices and best_sequence_indices[0] == self.target_sos_idx else best_sequence_indices\n",
        "\n",
        "# --- Data Loading and Preprocessing ---\n",
        "\n",
        "class Vocabulary:\n",
        "    \"\"\"\n",
        "    Manages the mapping between characters and their numerical indices.\n",
        "    Includes special tokens for padding, start-of-sequence, end-of-sequence, and unknown characters.\n",
        "    \"\"\"\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.char2index = {PAD_TOKEN: 0, SOS_TOKEN: 1, EOS_TOKEN: 2, UNK_TOKEN: 3}\n",
        "        self.index2char = {0: PAD_TOKEN, 1: SOS_TOKEN, 2: EOS_TOKEN, 3: UNK_TOKEN}\n",
        "        self.char_counts = Counter()\n",
        "        self.n_chars = 4\n",
        "        self.pad_idx = self.char2index[PAD_TOKEN]\n",
        "        self.sos_idx = self.char2index[SOS_TOKEN]\n",
        "        self.eos_idx = self.char2index[EOS_TOKEN]\n",
        "\n",
        "    def add_sequence(self, sequence):\n",
        "        \"\"\"Adds characters from a sequence to the character counts.\"\"\"\n",
        "        for char in list(sequence):\n",
        "            self.char_counts[char] += 1\n",
        "\n",
        "    def build_vocab(self, min_freq=1):\n",
        "        \"\"\"\n",
        "        Builds the vocabulary mapping based on character counts and a minimum frequency.\n",
        "        Characters appearing less than `min_freq` will be treated as UNK_TOKEN.\n",
        "        \"\"\"\n",
        "        sorted_chars = sorted(self.char_counts.keys(), key=lambda char: (-self.char_counts[char], char))\n",
        "        for char in sorted_chars:\n",
        "            if self.char_counts[char] >= min_freq and char not in self.char2index:\n",
        "                self.char2index[char] = self.n_chars\n",
        "                self.index2char[self.n_chars] = char\n",
        "                self.n_chars += 1\n",
        "\n",
        "    def sequence_to_indices(self, sequence, add_eos=False, add_sos=False):\n",
        "        \"\"\"Converts a character sequence into a list of numerical indices.\"\"\"\n",
        "        indices = []\n",
        "        if add_sos:\n",
        "            indices.append(self.sos_idx)\n",
        "        for char in list(sequence):\n",
        "            indices.append(self.char2index.get(char, self.char2index[UNK_TOKEN]))\n",
        "        if add_eos:\n",
        "            indices.append(self.eos_idx)\n",
        "        return indices\n",
        "\n",
        "    def indices_to_sequence(self, indices):\n",
        "        \"\"\"Converts a list of numerical indices back into a character sequence.\"\"\"\n",
        "        chars = []\n",
        "        for index_val in indices:\n",
        "            if index_val == self.eos_idx:\n",
        "                break # Stop at EOS token\n",
        "            if index_val != self.sos_idx and index_val != self.pad_idx:\n",
        "                 chars.append(self.index2char.get(index_val, UNK_TOKEN))\n",
        "        return \"\".join(chars)\n",
        "\n",
        "\n",
        "class TransliterationDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A PyTorch Dataset for loading and preparing transliteration pairs.\n",
        "    Reads data from a TSV file and converts text sequences to token indices.\n",
        "    \"\"\"\n",
        "    def __init__(self, file_path, source_vocab, target_vocab, max_len=50):\n",
        "        self.pairs = []\n",
        "        self.source_vocab = source_vocab\n",
        "        self.target_vocab = target_vocab\n",
        "\n",
        "        if not os.path.exists(file_path):\n",
        "            print(f\"ERROR: Data file not found during Dataset init: {file_path}\")\n",
        "            return\n",
        "\n",
        "        print(f\"Loading data from: {file_path}\")\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                for line_num, line in enumerate(f):\n",
        "                    parts = line.strip().split('\\t')\n",
        "                    if len(parts) >= 2:\n",
        "                        target_sequence, source_sequence = parts[0], parts[1]\n",
        "\n",
        "                        if max_len and (len(source_sequence) > max_len or len(target_sequence) > max_len):\n",
        "                            continue\n",
        "                        if not source_sequence or not target_sequence:\n",
        "                            continue\n",
        "                        self.pairs.append((source_sequence, target_sequence))\n",
        "            print(f\"Loaded {len(self.pairs)} pairs from {file_path}.\")\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR: Could not read or process file {file_path}. Error: {e}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Returns a source-target pair as Tensors of indices.\"\"\"\n",
        "        if idx >= len(self.pairs):\n",
        "            raise IndexError(\"Index out of bounds for dataset\")\n",
        "        source_str, target_str = self.pairs[idx]\n",
        "        source_indices = self.source_vocab.sequence_to_indices(source_str, add_eos=True)\n",
        "        target_indices = self.target_vocab.sequence_to_indices(target_str, add_sos=True, add_eos=True)\n",
        "        return torch.tensor(source_indices, dtype=torch.long), \\\n",
        "               torch.tensor(target_indices, dtype=torch.long)\n",
        "\n",
        "def collate_fn(batch, pad_idx_source, pad_idx_target):\n",
        "    \"\"\"\n",
        "    Custom collate function for DataLoader to handle variable-length sequences.\n",
        "    Pads sequences within a batch to the maximum length of that batch.\n",
        "    \"\"\"\n",
        "    # Filter out any None or empty items\n",
        "    batch = [item for item in batch if item is not None and item[0] is not None and item[1] is not None]\n",
        "    if not batch:\n",
        "        return None, None, None\n",
        "\n",
        "    source_seqs, target_seqs = zip(*batch)\n",
        "\n",
        "    # Filter out any empty sequences after zipping\n",
        "    valid_indices = [i for i, s in enumerate(source_seqs) if len(s) > 0]\n",
        "    if not valid_indices: return None, None, None\n",
        "\n",
        "    source_seqs = [source_seqs[i] for i in valid_indices]\n",
        "    target_seqs = [target_seqs[i] for i in valid_indices]\n",
        "    source_lengths = torch.tensor([len(s) for s in source_seqs], dtype=torch.long)\n",
        "\n",
        "    # Pad sequences to the length of the longest sequence in the batch\n",
        "    padded_sources = pad_sequence(source_seqs, batch_first=True, padding_value=pad_idx_source)\n",
        "    padded_targets = pad_sequence(target_seqs, batch_first=True, padding_value=pad_idx_target)\n",
        "    return padded_sources, source_lengths, padded_targets\n",
        "\n",
        "# --- Training and Evaluation Functions ---\n",
        "\n",
        "def _train_one_epoch(model, dataloader, optimizer, criterion, device, clip_value, teacher_forcing_ratio, target_vocab):\n",
        "    \"\"\"\n",
        "    Trains the model for a single epoch.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The Seq2Seq model.\n",
        "        dataloader (DataLoader): DataLoader for the training set.\n",
        "        optimizer (optim.Optimizer): Optimizer for model parameters.\n",
        "        criterion (nn.Module): Loss function (e.g., CrossEntropyLoss).\n",
        "        device (torch.device): Device to run the model on.\n",
        "        clip_value (float): Gradient clipping value.\n",
        "        teacher_forcing_ratio (float): Probability of using teacher forcing.\n",
        "        target_vocab (Vocabulary): Vocabulary for the target language.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Average epoch loss and training accuracy.\n",
        "    \"\"\"\n",
        "    model.train() # Set model to training mode\n",
        "    epoch_loss = 0\n",
        "    total_correct_train = 0\n",
        "    total_samples_train = 0\n",
        "\n",
        "    if len(dataloader) == 0:\n",
        "        print(\"Warning: Training dataloader is empty.\")\n",
        "        return 0.0, 0.0\n",
        "\n",
        "    for batch_data in tqdm(dataloader, desc=\"Training\", leave=False):\n",
        "        if batch_data[0] is None: continue\n",
        "        sources, source_lengths, targets = batch_data\n",
        "\n",
        "        if sources is None or source_lengths is None or targets is None or source_lengths.numel() == 0 or sources.shape[0] == 0:\n",
        "            print(\"Warning: Empty or invalid batch data after collate_fn in training. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        sources, targets, source_lengths = sources.to(device), targets.to(device), source_lengths.to(device)\n",
        "        optimizer.zero_grad() # Clear gradients\n",
        "\n",
        "        # Forward pass: model predicts logits for the target sequence\n",
        "        outputs_logits = model(sources, source_lengths, targets, teacher_forcing_ratio)\n",
        "\n",
        "        # Reshape outputs and targets for CrossEntropyLoss\n",
        "        # We ignore the first token (SOS) in the target for loss calculation\n",
        "        output_dim = outputs_logits.shape[-1]\n",
        "        flat_outputs = outputs_logits[:, 1:].reshape(-1, output_dim)\n",
        "        flat_targets = targets[:, 1:].reshape(-1)\n",
        "\n",
        "        loss = criterion(flat_outputs, flat_targets)\n",
        "\n",
        "        # Handle potential NaN/Inf loss (e.g., due to bad gradients)\n",
        "        if torch.isnan(loss) or torch.isinf(loss):\n",
        "            print(\"Warning: NaN or Inf loss detected in training. Skipping batch.\")\n",
        "            continue\n",
        "        loss.backward() # Backpropagation\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value) # Clip gradients\n",
        "        optimizer.step() # Update model parameters\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        # Calculate training accuracy (exact sequence match)\n",
        "        predictions_indices_train = outputs_logits.argmax(dim=2)\n",
        "        for i in range(targets.shape[0]):\n",
        "            pred_str_train = target_vocab.indices_to_sequence(predictions_indices_train[i, 1:].tolist())\n",
        "            true_str_train = target_vocab.indices_to_sequence(targets[i, 1:].tolist())\n",
        "            if pred_str_train == true_str_train:\n",
        "                total_correct_train += 1\n",
        "            total_samples_train += 1\n",
        "\n",
        "    avg_epoch_loss = epoch_loss / len(dataloader) if len(dataloader) > 0 else 0.0\n",
        "    train_accuracy = total_correct_train / total_samples_train if total_samples_train > 0 else 0.0\n",
        "    return avg_epoch_loss, train_accuracy\n",
        "\n",
        "\n",
        "def _evaluate_one_epoch(model, dataloader, criterion, device, source_vocab, target_vocab, beam_width=1, is_test_set=False):\n",
        "    \"\"\"\n",
        "    Evaluates the model's performance on a given dataset (validation or test).\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The Seq2Seq model.\n",
        "        dataloader (DataLoader): DataLoader for the evaluation set.\n",
        "        criterion (nn.Module): Loss function.\n",
        "        device (torch.device): Device to run the model on.\n",
        "        source_vocab (Vocabulary): Vocabulary for the source language (used for debug prints).\n",
        "        target_vocab (Vocabulary): Vocabulary for the target language.\n",
        "        beam_width (int): Beam width for decoding (1 for greedy, >1 for beam search).\n",
        "        is_test_set (bool): Flag to indicate if this is the final test evaluation (for logging and samples).\n",
        "\n",
        "    Returns:\n",
        "        tuple: Average epoch loss and evaluation accuracy.\n",
        "    \"\"\"\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    epoch_loss = 0\n",
        "    total_correct, total_samples = 0, 0\n",
        "    if len(dataloader) == 0:\n",
        "        print(\"WARNING: Evaluation dataloader is empty. Returning 0 loss and 0 accuracy.\")\n",
        "        return 0.0, 0.0\n",
        "    desc_prefix = \"Testing\" if is_test_set else \"Validating\"\n",
        "\n",
        "    with torch.no_grad(): # Disable gradient calculations during evaluation\n",
        "        for batch_idx, batch_data in enumerate(tqdm(dataloader, desc=desc_prefix, leave=False)):\n",
        "            if batch_data[0] is None: continue\n",
        "            sources, source_lengths, targets = batch_data\n",
        "\n",
        "            if sources is None or source_lengths is None or targets is None or source_lengths.numel() == 0 or sources.shape[0] == 0:\n",
        "                print(\"Warning: Empty or invalid batch data in eval after collate_fn. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            sources, targets, source_lengths = sources.to(device), targets.to(device), source_lengths.to(device)\n",
        "\n",
        "            # Forward pass for loss calculation (using teacher forcing=0.0)\n",
        "            outputs_for_loss = model(sources, source_lengths, targets, teacher_forcing_ratio=0.0)\n",
        "            output_dim = outputs_for_loss.shape[-1]\n",
        "            flat_outputs_for_loss = outputs_for_loss[:, 1:].reshape(-1, output_dim)\n",
        "            flat_targets_for_loss = targets[:, 1:].reshape(-1)\n",
        "            loss = criterion(flat_outputs_for_loss, flat_targets_for_loss)\n",
        "            epoch_loss += loss.item() if not (torch.isnan(loss) or torch.isinf(loss)) else 0\n",
        "\n",
        "            # Generate predictions for accuracy calculation for each item in batch\n",
        "            for i in range(sources.shape[0]):\n",
        "                src_single, src_len_single = sources[i:i+1], source_lengths[i:i+1]\n",
        "\n",
        "                # Choose decoding strategy (beam search or greedy)\n",
        "                if beam_width > 1 and hasattr(model, 'predict_beam_search'):\n",
        "                    predicted_indices = model.predict_beam_search(src_single, src_len_single,\n",
        "                                                                  max_output_len=targets.size(1) + 5,\n",
        "                                                                  target_eos_idx=target_vocab.eos_idx,\n",
        "                                                                  beam_width=beam_width)\n",
        "                elif hasattr(model, 'predict_greedy'):\n",
        "                     predicted_indices = model.predict_greedy(src_single, src_len_single,\n",
        "                                                             max_output_len=targets.size(1) + 5,\n",
        "                                                             target_eos_idx=target_vocab.eos_idx)\n",
        "                else: # Fallback to argmax on outputs_for_loss if predict methods are not available\n",
        "                    predicted_indices = outputs_for_loss[i:i+1].argmax(dim=2)[0, 1:].tolist()\n",
        "\n",
        "                # Convert predicted and true indices to strings for comparison\n",
        "                pred_str = target_vocab.indices_to_sequence(predicted_indices)\n",
        "                true_str = target_vocab.indices_to_sequence(targets[i, 1:].tolist())\n",
        "\n",
        "                # Check for exact match\n",
        "                if pred_str == true_str: total_correct += 1\n",
        "                total_samples += 1\n",
        "\n",
        "                # Print a single debug sample from the first batch\n",
        "                if is_test_set and batch_idx == 0 and i < 3:\n",
        "                    print(f\"  Test Example {i} - Source: '{source_vocab.indices_to_sequence(src_single[0].tolist())}'\")\n",
        "                    print(f\"    Pred: '{pred_str}', True: '{true_str}', Match: {pred_str == true_str}\")\n",
        "\n",
        "    avg_epoch_loss = epoch_loss / len(dataloader) if len(dataloader) > 0 else 0.0\n",
        "    accuracy = total_correct / total_samples if total_samples > 0 else 0.0\n",
        "    print(f\"{desc_prefix} - Total Correct: {total_correct}, Total Samples: {total_samples}, Accuracy: {accuracy:.4f}, Avg Loss: {avg_epoch_loss:.4f}\")\n",
        "    return avg_epoch_loss, accuracy\n",
        "\n",
        "# --- Function to Train and Save the Best Model ---\n",
        "\n",
        "def train_and_save_best_model(config_params, model_save_path, device):\n",
        "    \"\"\"\n",
        "    Performs a dedicated training run using the best hyperparameters found from a sweep.\n",
        "    Saves the model checkpoint with the best validation accuracy.\n",
        "\n",
        "    Args:\n",
        "        config_params (dict): Dictionary of hyperparameters for this specific training run.\n",
        "        model_save_path (str): Path to save the best model's state_dict.\n",
        "        device (torch.device): Device to run the training on.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if training was successful and a model was saved, False otherwise.\n",
        "    \"\"\"\n",
        "    run_name_train_best = f\"TRAIN_BEST_{config_params['cell_type']}_emb{config_params['embedding_dim']}_hid{config_params['hidden_dim']}\"\n",
        "\n",
        "    with wandb.init(project=\"DL_A3\", name=run_name_train_best, config=config_params, job_type=\"training_best_model_final\", reinit=True) as run:\n",
        "        cfg = wandb.config\n",
        "        print(f\"Starting dedicated training for best model with config: {cfg}\")\n",
        "\n",
        "        # --- Data Loading and Vocabulary Building ---\n",
        "        BASE_DATA_DIR = \"/kaggle/input/dakshina-dl-a3/dakshina_dataset_v1.0/hi/\"\n",
        "        DATA_DIR = os.path.join(BASE_DATA_DIR, \"lexicons/\")\n",
        "        train_file = os.path.join(DATA_DIR, \"hi.translit.sampled.train.tsv\")\n",
        "        dev_file = os.path.join(DATA_DIR, \"hi.translit.sampled.dev.tsv\")\n",
        "\n",
        "        source_vocab = Vocabulary(\"latin\")\n",
        "        target_vocab = Vocabulary(\"devanagari\")\n",
        "\n",
        "        # Build vocabulary from the training data\n",
        "        temp_train_ds_vocab = TransliterationDataset(train_file, source_vocab, target_vocab, max_len=cfg.max_seq_len)\n",
        "        if not temp_train_ds_vocab.pairs:\n",
        "            print(f\"ERROR: No training data loaded for vocabulary building from {train_file}.\")\n",
        "            return False\n",
        "        for src, tgt in temp_train_ds_vocab.pairs:\n",
        "            source_vocab.add_sequence(src)\n",
        "            target_vocab.add_sequence(tgt)\n",
        "        source_vocab.build_vocab(min_freq=cfg.vocab_min_freq)\n",
        "        target_vocab.build_vocab(min_freq=cfg.vocab_min_freq)\n",
        "        print(f\"Vocabs built. Source: {source_vocab.n_chars}, Target: {target_vocab.n_chars}\")\n",
        "\n",
        "        # Create actual datasets for training and validation\n",
        "        train_dataset = TransliterationDataset(train_file, source_vocab, target_vocab, max_len=cfg.max_seq_len)\n",
        "        dev_dataset = TransliterationDataset(dev_file, source_vocab, target_vocab, max_len=cfg.max_seq_len)\n",
        "        if not train_dataset.pairs or not dev_dataset.pairs:\n",
        "            print(f\"ERROR: Train or Dev dataset is empty after filtering. Train: {len(train_dataset.pairs)}, Dev: {len(dev_dataset.pairs)}\")\n",
        "            return False\n",
        "\n",
        "        # --- DataLoader Setup ---\n",
        "        num_w = 0 if device.type == 'cpu' else min(4, os.cpu_count() // 2 if os.cpu_count() else 0)\n",
        "        train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size_train, shuffle=True,\n",
        "                                  collate_fn=lambda b: collate_fn(b, source_vocab.pad_idx, target_vocab.pad_idx),\n",
        "                                  num_workers=num_w, pin_memory=True if device.type == 'cuda' else False, drop_last=True)\n",
        "        dev_loader = DataLoader(dev_dataset, batch_size=cfg.batch_size_train, shuffle=False,\n",
        "                                collate_fn=lambda b: collate_fn(b, source_vocab.pad_idx, target_vocab.pad_idx),\n",
        "                                num_workers=num_w, pin_memory=True if device.type == 'cuda' else False)\n",
        "        if not train_loader or not dev_loader:\n",
        "            print(f\"ERROR: Train or Dev DataLoader is empty. Train: {len(train_loader)}, Dev: {len(dev_loader)}\")\n",
        "            return False\n",
        "\n",
        "        # --- Model, Optimizer, Loss Function Setup ---\n",
        "        encoder = Encoder(source_vocab.n_chars, cfg.embedding_dim, cfg.hidden_dim,\n",
        "                          cfg.encoder_layers, cfg.cell_type, cfg.dropout_p,\n",
        "                          cfg.encoder_bidirectional, pad_idx=source_vocab.pad_idx).to(device)\n",
        "        decoder = Decoder(target_vocab.n_chars, cfg.embedding_dim, cfg.hidden_dim,\n",
        "                          cfg.decoder_layers, cfg.cell_type, cfg.dropout_p,\n",
        "                          pad_idx=target_vocab.pad_idx).to(device)\n",
        "        model = Seq2Seq(encoder, decoder, device, target_vocab.sos_idx).to(device)\n",
        "        print(f\"Best model parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
        "        wandb.watch(model, log=\"all\", log_freq=100)\n",
        "\n",
        "        optimizer = optim.Adam(model.parameters(), lr=cfg.learning_rate_train)\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=cfg.lr_scheduler_patience_train, factor=0.3, verbose=True)\n",
        "        criterion = nn.CrossEntropyLoss(ignore_index=target_vocab.pad_idx)\n",
        "\n",
        "        best_val_acc_this_training = -1.0\n",
        "        epochs_no_improve = 0\n",
        "\n",
        "        # --- Training Loop ---\n",
        "        for epoch in range(cfg.epochs_train):\n",
        "            train_loss, train_acc = _train_one_epoch(model, train_loader, optimizer, criterion, device,\n",
        "                                                 cfg.clip_value_train, cfg.teacher_forcing_train, target_vocab)\n",
        "\n",
        "            val_loss, val_acc = _evaluate_one_epoch(model, dev_loader, criterion, device,\n",
        "                                                    source_vocab, target_vocab,\n",
        "                                                    beam_width=1) # Use greedy for val during this training\n",
        "\n",
        "            scheduler.step(val_acc)\n",
        "            print(f\"Epoch {epoch+1}/{cfg.epochs_train} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "            wandb.log({\"epoch_train_best\": epoch + 1, \"train_loss_best\": train_loss, \"train_acc_best\": train_acc,\n",
        "                       \"val_loss_best\": val_loss, \"val_acc_best\": val_acc, \"lr_best\": optimizer.param_groups[0]['lr']})\n",
        "\n",
        "            # Early stopping logic\n",
        "            current_val_acc = val_acc if not np.isnan(val_acc) else -1.0\n",
        "            if current_val_acc > best_val_acc_this_training:\n",
        "                best_val_acc_this_training = current_val_acc\n",
        "                epochs_no_improve = 0\n",
        "                torch.save(model.state_dict(), model_save_path) # Save best model\n",
        "                print(f\"Saved new best model to {model_save_path} (Val Acc: {best_val_acc_this_training:.4f})\")\n",
        "            else:\n",
        "                epochs_no_improve += 1\n",
        "\n",
        "            if epochs_no_improve >= cfg.max_epochs_no_improve_train:\n",
        "                print(f\"Early stopping for best model training at epoch {epoch+1}.\")\n",
        "                break\n",
        "\n",
        "        # Save the final model state if no improvement happened over the initial checkpoint\n",
        "        if not os.path.exists(model_save_path):\n",
        "            torch.save(model.state_dict(), model_save_path)\n",
        "            print(f\"Saved final model state (no improvement over initial) to {model_save_path}\")\n",
        "\n",
        "        wandb.summary[\"final_best_val_accuracy_during_training\"] = best_val_acc_this_training\n",
        "        print(f\"Finished training best model. Best Val Acc: {best_val_acc_this_training:.4f}\")\n",
        "    return True\n",
        "\n",
        "# --- Main Execution Block for Training Best Model and Testing ---\n",
        "if __name__ == '__main__':\n",
        "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "    # --- W&B Login ---\n",
        "    try:\n",
        "        if 'KAGGLE_KERNEL_RUN_TYPE' in os.environ:\n",
        "            print(\"Detected Kaggle environment. Ensuring WANDB_API_KEY is set.\")\n",
        "            if \"WANDB_API_KEY\" in os.environ:\n",
        "                wandb.login()\n",
        "                print(\"W&B login using environment variable.\")\n",
        "            else:\n",
        "                from kaggle_secrets import UserSecretsClient\n",
        "                user_secrets = UserSecretsClient()\n",
        "                wandb_api_key = user_secrets.get_secret(\"WANDB_API_KEY\")\n",
        "                wandb.login(key=wandb_api_key)\n",
        "                print(\"W&B login for Kaggle successful.\")\n",
        "        else:\n",
        "            wandb.login()\n",
        "        print(\"W&B login process attempted/completed.\")\n",
        "    except Exception as e:\n",
        "        print(f\"W&B login failed: {e}. Ensure API key is configured.\")\n",
        "        exit()\n",
        "\n",
        "    # --- Best Hyperparameters (obtained from a previous sweep) ---\n",
        "    # These hyperparameters are chosen based on the provided sweep results.\n",
        "    BEST_HYPERPARAMETERS_FOR_TRAINING = {\n",
        "        'embedding_dim': 300,\n",
        "        'hidden_dim': 512,\n",
        "        'encoder_layers': 2,\n",
        "        'decoder_layers': 1,\n",
        "        'cell_type': 'LSTM',\n",
        "        'dropout_p': 0.5,\n",
        "        'encoder_bidirectional': True,\n",
        "        'learning_rate_train': 0.000376,\n",
        "        'batch_size_train': 128,\n",
        "        'epochs_train': 20,\n",
        "        'clip_value_train': 1.0,\n",
        "        'teacher_forcing_train': 0.5,\n",
        "        'max_epochs_no_improve_train': 7,\n",
        "        'lr_scheduler_patience_train': 3,\n",
        "        'vocab_min_freq': 1,\n",
        "        'max_seq_len': 50,\n",
        "        'eval_batch_size': 128,\n",
        "        'beam_width_eval': 3\n",
        "    }\n",
        "    MODEL_SAVE_PATH = \"/kaggle/working/best_model_for_testing.pt\"\n",
        "\n",
        "    print(f\"Best hyperparameters selected for training: {BEST_HYPERPARAMETERS_FOR_TRAINING}\")\n",
        "\n",
        "    # --- Phase 1: Train and Save the Best Model ---\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"--- Phase 1: Training and Saving Best Model Configuration ---\".center(80))\n",
        "    print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "    training_successful = train_and_save_best_model(BEST_HYPERPARAMETERS_FOR_TRAINING, MODEL_SAVE_PATH, DEVICE)\n",
        "\n",
        "    if not training_successful or not os.path.exists(MODEL_SAVE_PATH):\n",
        "        print(\"ERROR: Failed to train and save the best model. Exiting before test evaluation.\")\n",
        "        exit()\n",
        "    print(f\"Best model trained and saved to {MODEL_SAVE_PATH}\")\n",
        "\n",
        "    # --- Phase 2: Load and Evaluate on Test Set ---\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"--- Phase 2: Loading and Evaluating Best Model on Test Set ---\".center(80))\n",
        "    print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "    BASE_DATA_DIR = \"/kaggle/input/dakshina-dl-a3/dakshina_dataset_v1.0/hi/\"\n",
        "    DATA_DIR = os.path.join(BASE_DATA_DIR, \"lexicons/\")\n",
        "    train_file_for_vocab = os.path.join(DATA_DIR, \"hi.translit.sampled.train.tsv\")\n",
        "    test_file = os.path.join(DATA_DIR, \"hi.translit.sampled.test.tsv\")\n",
        "\n",
        "    source_vocab_test = Vocabulary(\"latin_test\")\n",
        "    target_vocab_test = Vocabulary(\"devanagari_test\")\n",
        "\n",
        "    # Build vocabulary using the training dataset (crucial for consistent tokenization)\n",
        "    temp_train_ds_test_vocab = TransliterationDataset(train_file_for_vocab, source_vocab_test, target_vocab_test,\n",
        "                                                max_len=BEST_HYPERPARAMETERS_FOR_TRAINING['max_seq_len'])\n",
        "    if not temp_train_ds_test_vocab.pairs:\n",
        "        print(f\"ERROR: No data loaded for vocabulary building from {train_file_for_vocab}.\")\n",
        "        exit()\n",
        "    for src, tgt in temp_train_ds_test_vocab.pairs:\n",
        "        source_vocab_test.add_sequence(src)\n",
        "        target_vocab_test.add_sequence(tgt)\n",
        "    source_vocab_test.build_vocab(min_freq=BEST_HYPERPARAMETERS_FOR_TRAINING['vocab_min_freq'])\n",
        "    target_vocab_test.build_vocab(min_freq=BEST_HYPERPARAMETERS_FOR_TRAINING['vocab_min_freq'])\n",
        "    print(f\"Test Vocab built from training data. Source: {source_vocab_test.n_chars} chars. Target: {target_vocab_test.n_chars} chars.\")\n",
        "\n",
        "    # Create the test dataset\n",
        "    test_dataset = TransliterationDataset(test_file, source_vocab_test, target_vocab_test,\n",
        "                                          max_len=BEST_HYPERPARAMETERS_FOR_TRAINING['max_seq_len'])\n",
        "    if not test_dataset.pairs:\n",
        "        print(f\"ERROR: Test dataset is empty from {test_file}.\")\n",
        "        exit()\n",
        "\n",
        "    # Setup test DataLoader\n",
        "    num_w_test = 0 if DEVICE.type == 'cpu' else min(4, os.cpu_count() // 2 if os.cpu_count() else 0)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=BEST_HYPERPARAMETERS_FOR_TRAINING['eval_batch_size'], shuffle=False,\n",
        "                                 collate_fn=lambda b: collate_fn(b, source_vocab_test.pad_idx, target_vocab_test.pad_idx),\n",
        "                                 num_workers=num_w_test, pin_memory=True if DEVICE.type == 'cuda' else False)\n",
        "    if not test_dataloader or len(test_dataloader) == 0:\n",
        "        print(f\"ERROR: Test Dataloader is empty. Dataset size: {len(test_dataset)}, Batch size: {BEST_HYPERPARAMETERS_FOR_TRAINING['eval_batch_size']}\")\n",
        "        exit()\n",
        "\n",
        "    # --- Initialize Model for Testing and Load Weights ---\n",
        "    encoder_test = Encoder(source_vocab_test.n_chars, BEST_HYPERPARAMETERS_FOR_TRAINING['embedding_dim'], BEST_HYPERPARAMETERS_FOR_TRAINING['hidden_dim'],\n",
        "                           BEST_HYPERPARAMETERS_FOR_TRAINING['encoder_layers'], BEST_HYPERPARAMETERS_FOR_TRAINING['cell_type'], BEST_HYPERPARAMETERS_FOR_TRAINING['dropout_p'],\n",
        "                           BEST_HYPERPARAMETERS_FOR_TRAINING['encoder_bidirectional'], pad_idx=source_vocab_test.pad_idx).to(DEVICE)\n",
        "    decoder_test = Decoder(target_vocab_test.n_chars, BEST_HYPERPARAMETERS_FOR_TRAINING['embedding_dim'], BEST_HYPERPARAMETERS_FOR_TRAINING['hidden_dim'],\n",
        "                           BEST_HYPERPARAMETERS_FOR_TRAINING['decoder_layers'], BEST_HYPERPARAMETERS_FOR_TRAINING['cell_type'], BEST_HYPERPARAMETERS_FOR_TRAINING['dropout_p'],\n",
        "                           pad_idx=target_vocab_test.pad_idx).to(DEVICE)\n",
        "    model_test = Seq2Seq(encoder_test, decoder_test, DEVICE, target_vocab_test.sos_idx).to(DEVICE)\n",
        "\n",
        "    print(f\"Loading weights into test model from: {MODEL_SAVE_PATH}\")\n",
        "    model_test.load_state_dict(torch.load(MODEL_SAVE_PATH, map_location=DEVICE))\n",
        "    print(\"Weights loaded successfully for test evaluation.\")\n",
        "\n",
        "    # --- Evaluate on Test Set ---\n",
        "    criterion_test = nn.CrossEntropyLoss(ignore_index=target_vocab_test.pad_idx)\n",
        "    test_loss, test_accuracy = _evaluate_one_epoch(model_test, test_dataloader, criterion_test, DEVICE,\n",
        "                                                   source_vocab_test, target_vocab_test,\n",
        "                                                   beam_width=BEST_HYPERPARAMETERS_FOR_TRAINING.get('beam_width_eval', 1),\n",
        "                                                   is_test_set=True)\n",
        "\n",
        "    print(f\"\\n--- FINAL TEST SET PERFORMANCE ---\")\n",
        "    print(f\"Model Checkpoint: '{MODEL_SAVE_PATH}'\")\n",
        "    # Print only relevant hyperparameters for the loaded model, not training-specific ones\n",
        "    eval_config_to_print = {k: v for k, v in BEST_HYPERPARAMETERS_FOR_TRAINING.items() if not k.endswith('_train')}\n",
        "    print(f\"Hyperparameters: {eval_config_to_print}\")\n",
        "    print(f\"  Test Loss (avg per batch): {test_loss:.4f}\")\n",
        "    print(f\"  Test Accuracy (Exact Match): {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
        "\n",
        "    # --- Log Test Results to W&B ---\n",
        "    try:\n",
        "        run_name_final_test = f\"TEST_BEST_MODEL_{BEST_HYPERPARAMETERS_FOR_TRAINING['cell_type']}\"\n",
        "        with wandb.init(project=\"DL_A3\", name=run_name_final_test, config=BEST_HYPERPARAMETERS_FOR_TRAINING, job_type=\"final_test_evaluation\", reinit=True) as test_run:\n",
        "            test_run.summary[\"final_test_accuracy\"] = test_accuracy\n",
        "            test_run.summary[\"final_test_loss\"] = test_loss\n",
        "            test_run.summary[\"model_checkpoint_used\"] = MODEL_SAVE_PATH\n",
        "            print(\"Final test results logged to W&B.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not log final test results to W&B: {e}\")"
      ],
      "metadata": {
        "id": "t8Fbh50UqpOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# testing"
      ],
      "metadata": {
        "id": "9mgdhQLVBgpP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TvW8gQiQRBz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 5 testing attention"
      ],
      "metadata": {
        "id": "ZnuJU-rcT2oO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import numpy as np\n",
        "import wandb\n",
        "import os\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "import heapq\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# --- Constants for Special Tokens ---\n",
        "SOS_TOKEN = \"<sos>\"  # Start-of-sequence token\n",
        "EOS_TOKEN = \"<eos>\"  # End-of-sequence token\n",
        "PAD_TOKEN = \"<pad>\"  # Padding token\n",
        "UNK_TOKEN = \"<unk>\"  # Unknown token\n",
        "\n",
        "# --- Attention Mechanism ---\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    \"\"\"Computes alignment scores between decoder's hidden state and encoder's outputs.\"\"\"\n",
        "    def __init__(self, encoder_hidden_dim_eff, decoder_hidden_dim):\n",
        "        super(Attention, self).__init__()\n",
        "        self.attn_W = nn.Linear(encoder_hidden_dim_eff + decoder_hidden_dim, decoder_hidden_dim)\n",
        "        self.attn_v = nn.Linear(decoder_hidden_dim, 1, bias=False)\n",
        "\n",
        "    def forward(self, decoder_hidden_top_layer, encoder_outputs):\n",
        "        \"\"\"Calculates attention weights (probabilities).\"\"\"\n",
        "        src_len = encoder_outputs.size(1)\n",
        "        repeated_decoder_hidden = decoder_hidden_top_layer.unsqueeze(1).repeat(1, src_len, 1)\n",
        "        energy_input = torch.cat((repeated_decoder_hidden, encoder_outputs), dim=2)\n",
        "        energy = torch.tanh(self.attn_W(energy_input))\n",
        "        attention_scores = self.attn_v(energy).squeeze(2)\n",
        "        return F.softmax(attention_scores, dim=1)\n",
        "\n",
        "# --- Model Definition (Encoder, DecoderWithAttention, Seq2SeqWithAttention) ---\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"Encodes input sequence into a context vector and output states.\"\"\"\n",
        "    def __init__(self, input_vocab_size, embedding_dim, hidden_dim, n_layers,\n",
        "                 cell_type='LSTM', dropout_p=0.1, bidirectional=False, pad_idx=0):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.input_vocab_size = input_vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.cell_type = cell_type.upper()\n",
        "        self.bidirectional = bidirectional\n",
        "        self.num_directions = 2 if bidirectional else 1\n",
        "\n",
        "        self.embedding = nn.Embedding(input_vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "        rnn_dropout = dropout_p if n_layers > 1 else 0\n",
        "        if self.cell_type == 'RNN':\n",
        "            self.rnn = nn.RNN(embedding_dim, hidden_dim, n_layers,\n",
        "                              dropout=rnn_dropout, batch_first=True, bidirectional=self.bidirectional)\n",
        "        elif self.cell_type == 'GRU':\n",
        "            self.rnn = nn.GRU(embedding_dim, hidden_dim, n_layers,\n",
        "                              dropout=rnn_dropout, batch_first=True, bidirectional=self.bidirectional)\n",
        "        elif self.cell_type == 'LSTM':\n",
        "            self.rnn = nn.LSTM(embedding_dim, hidden_dim, n_layers,\n",
        "                               dropout=rnn_dropout, batch_first=True, bidirectional=self.bidirectional)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported cell type. Choose from 'RNN', 'GRU', 'LSTM'.\")\n",
        "\n",
        "    def forward(self, input_seq, input_lengths):\n",
        "        \"\"\"Processes input sequence, returns all encoder outputs and final hidden state.\"\"\"\n",
        "        embedded = self.embedding(input_seq)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "        packed_outputs, hidden = self.rnn(packed_embedded)\n",
        "        encoder_outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs, batch_first=True)\n",
        "        return encoder_outputs, hidden\n",
        "\n",
        "class DecoderWithAttention(nn.Module):\n",
        "    \"\"\"Generates output sequence one token at a time using an attention mechanism.\"\"\"\n",
        "    def __init__(self, output_vocab_size, embedding_dim, encoder_hidden_dim_eff,\n",
        "                 decoder_hidden_dim, n_layers, cell_type='LSTM', dropout_p=0.1, pad_idx=0):\n",
        "        super(DecoderWithAttention, self).__init__()\n",
        "        self.output_vocab_size = output_vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.encoder_hidden_dim_eff = encoder_hidden_dim_eff\n",
        "        self.decoder_hidden_dim = decoder_hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.cell_type = cell_type.upper()\n",
        "\n",
        "        self.attention = Attention(encoder_hidden_dim_eff, decoder_hidden_dim)\n",
        "        self.embedding = nn.Embedding(output_vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "        rnn_input_dim = embedding_dim + encoder_hidden_dim_eff\n",
        "\n",
        "        rnn_dropout = dropout_p if n_layers > 1 else 0\n",
        "        if self.cell_type == 'RNN':\n",
        "            self.rnn = nn.RNN(rnn_input_dim, decoder_hidden_dim, n_layers,\n",
        "                              dropout=rnn_dropout, batch_first=True)\n",
        "        elif self.cell_type == 'GRU':\n",
        "            self.rnn = nn.GRU(rnn_input_dim, decoder_hidden_dim, n_layers,\n",
        "                              dropout=rnn_dropout, batch_first=True)\n",
        "        elif self.cell_type == 'LSTM':\n",
        "            self.rnn = nn.LSTM(rnn_input_dim, decoder_hidden_dim, n_layers,\n",
        "                               dropout=rnn_dropout, batch_first=True)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported cell type. Choose from 'RNN', 'GRU', 'LSTM'.\")\n",
        "\n",
        "        self.fc_out = nn.Linear(decoder_hidden_dim, output_vocab_size)\n",
        "\n",
        "    def forward(self, input_char, prev_decoder_hidden, encoder_outputs):\n",
        "        \"\"\"Decodes one step, applying attention and returning logits, new hidden state, and attention weights.\"\"\"\n",
        "        input_char = input_char.unsqueeze(1)\n",
        "        embedded = self.embedding(input_char)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        if self.cell_type == 'LSTM':\n",
        "            attention_query_hidden = prev_decoder_hidden[0][-1, :, :]\n",
        "        else:\n",
        "            attention_query_hidden = prev_decoder_hidden[-1, :, :]\n",
        "\n",
        "        attention_weights = self.attention(attention_query_hidden, encoder_outputs)\n",
        "        context_vector = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)\n",
        "        rnn_input = torch.cat((embedded, context_vector), dim=2)\n",
        "        rnn_output, current_decoder_hidden = self.rnn(rnn_input, prev_decoder_hidden)\n",
        "\n",
        "        rnn_output_squeezed = rnn_output.squeeze(1)\n",
        "        prediction_logits = self.fc_out(rnn_output_squeezed)\n",
        "\n",
        "        return prediction_logits, current_decoder_hidden, attention_weights\n",
        "\n",
        "\n",
        "class Seq2SeqWithAttention(nn.Module):\n",
        "    \"\"\"Full Attention-based Sequence-to-Sequence model.\"\"\"\n",
        "    def __init__(self, encoder, decoder, device, target_sos_idx):\n",
        "        super(Seq2SeqWithAttention, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "        self.target_sos_idx = target_sos_idx\n",
        "\n",
        "        encoder_effective_final_hidden_dim = self.encoder.hidden_dim * self.encoder.num_directions\n",
        "        decoder_rnn_hidden_dim = self.decoder.decoder_hidden_dim\n",
        "\n",
        "        self.fc_adapt_hidden = None\n",
        "        self.fc_adapt_cell = None\n",
        "\n",
        "        if encoder_effective_final_hidden_dim != decoder_rnn_hidden_dim:\n",
        "            self.fc_adapt_hidden = nn.Linear(encoder_effective_final_hidden_dim, decoder_rnn_hidden_dim)\n",
        "            if self.encoder.cell_type == 'LSTM':\n",
        "                self.fc_adapt_cell = nn.Linear(encoder_effective_final_hidden_dim, decoder_rnn_hidden_dim)\n",
        "\n",
        "    def _adapt_encoder_hidden_for_decoder(self, encoder_final_hidden_state):\n",
        "        \"\"\"Adapts encoder's final hidden state to decoder's initial hidden state dimensions and layers.\"\"\"\n",
        "        is_lstm = self.encoder.cell_type == 'LSTM'\n",
        "        if is_lstm:\n",
        "            h_from_enc, c_from_enc = encoder_final_hidden_state\n",
        "        else:\n",
        "            h_from_enc = encoder_final_hidden_state\n",
        "            c_from_enc = None\n",
        "\n",
        "        batch_size = h_from_enc.size(1)\n",
        "\n",
        "        h_processed = h_from_enc.view(self.encoder.n_layers, self.encoder.num_directions,\n",
        "                                      batch_size, self.encoder.hidden_dim)\n",
        "        if self.encoder.bidirectional:\n",
        "            h_processed = torch.cat([h_processed[:, 0, :, :], h_processed[:, 1, :, :]], dim=2)\n",
        "        else:\n",
        "            h_processed = h_processed.squeeze(1)\n",
        "\n",
        "        c_processed = None\n",
        "        if is_lstm and c_from_enc is not None:\n",
        "            c_processed = c_from_enc.view(self.encoder.n_layers, self.encoder.num_directions,\n",
        "                                          batch_size, self.encoder.hidden_dim)\n",
        "            if self.encoder.bidirectional:\n",
        "                c_processed = torch.cat([c_processed[:, 0, :, :], c_processed[:, 1, :, :]], dim=2)\n",
        "            else:\n",
        "                c_processed = c_processed.squeeze(1)\n",
        "\n",
        "        if self.fc_adapt_hidden:\n",
        "            h_processed = self.fc_adapt_hidden(h_processed)\n",
        "            if is_lstm and c_processed is not None and self.fc_adapt_cell:\n",
        "                c_processed = self.fc_adapt_cell(c_processed)\n",
        "\n",
        "        final_h = torch.zeros(self.decoder.n_layers, batch_size, self.decoder.decoder_hidden_dim, device=self.device)\n",
        "        final_c = torch.zeros(self.decoder.n_layers, batch_size, self.decoder.decoder_hidden_dim, device=self.device) if is_lstm else None\n",
        "\n",
        "        num_layers_to_copy = min(self.encoder.n_layers, self.decoder.n_layers)\n",
        "\n",
        "        final_h[:num_layers_to_copy, :, :] = h_processed[:num_layers_to_copy, :, :]\n",
        "        if is_lstm and c_processed is not None:\n",
        "            final_c[:num_layers_to_copy, :, :] = c_processed[:num_layers_to_copy, :, :]\n",
        "\n",
        "        if self.decoder.n_layers > self.encoder.n_layers and self.encoder.n_layers > 0:\n",
        "            last_h_layer_to_repeat = h_processed[self.encoder.n_layers-1, :, :]\n",
        "            for i in range(self.encoder.n_layers, self.decoder.n_layers):\n",
        "                final_h[i, :, :] = last_h_layer_to_repeat\n",
        "                if is_lstm and c_processed is not None:\n",
        "                    last_c_layer_to_repeat = c_processed[self.encoder.n_layers-1, :, :]\n",
        "                    final_c[i, :, :] = last_c_layer_to_repeat\n",
        "\n",
        "        return (final_h, final_c) if is_lstm else final_h\n",
        "\n",
        "    def forward(self, source_seq, source_lengths, target_seq, teacher_forcing_ratio=0.5):\n",
        "        \"\"\"Performs a forward pass during training with teacher forcing.\"\"\"\n",
        "        batch_size = source_seq.shape[0]\n",
        "        target_len = target_seq.shape[1]\n",
        "        target_vocab_size = self.decoder.output_vocab_size\n",
        "        outputs_logits = torch.zeros(batch_size, target_len, target_vocab_size).to(self.device)\n",
        "\n",
        "        encoder_outputs, encoder_final_hidden = self.encoder(source_seq, source_lengths)\n",
        "        decoder_hidden = self._adapt_encoder_hidden_for_decoder(encoder_final_hidden)\n",
        "        decoder_input = target_seq[:, 0]\n",
        "\n",
        "        for t in range(target_len - 1):\n",
        "            decoder_output_logits, decoder_hidden, _ = \\\n",
        "                self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            outputs_logits[:, t+1] = decoder_output_logits\n",
        "\n",
        "            teacher_force_this_step = random.random() < teacher_forcing_ratio\n",
        "            top1_predicted_token = decoder_output_logits.argmax(1)\n",
        "\n",
        "            decoder_input = target_seq[:, t+1] if teacher_force_this_step else top1_predicted_token\n",
        "        return outputs_logits\n",
        "\n",
        "    def predict_greedy(self, source_seq, source_lengths, max_output_len=50, target_eos_idx=None):\n",
        "        \"\"\"Generates sequence using greedy decoding.\"\"\"\n",
        "        self.eval()\n",
        "        if source_seq.dim() == 1:\n",
        "            source_seq = source_seq.unsqueeze(0)\n",
        "            source_lengths = torch.tensor([source_lengths if isinstance(source_lengths, int) else len(source_lengths)], device=self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            encoder_outputs, encoder_final_hidden = self.encoder(source_seq, source_lengths)\n",
        "            decoder_hidden = self._adapt_encoder_hidden_for_decoder(encoder_final_hidden)\n",
        "            decoder_input = torch.tensor([self.target_sos_idx], device=self.device)\n",
        "            predicted_indices = []\n",
        "            for _ in range(max_output_len):\n",
        "                decoder_output_logits, decoder_hidden, _ = \\\n",
        "                    self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "                top1_predicted_token = decoder_output_logits.argmax(1)\n",
        "                predicted_idx = top1_predicted_token.item()\n",
        "                if target_eos_idx is not None and predicted_idx == target_eos_idx: break\n",
        "                predicted_indices.append(predicted_idx)\n",
        "                decoder_input = top1_predicted_token\n",
        "        return predicted_indices\n",
        "\n",
        "    def predict_beam_search(self, source_seq, source_lengths, max_output_len=50, target_eos_idx=None, beam_width=3):\n",
        "        \"\"\"Generates sequence using beam search decoding.\"\"\"\n",
        "        self.eval()\n",
        "        if source_seq.dim() == 1:\n",
        "            source_seq = source_seq.unsqueeze(0)\n",
        "            source_lengths = torch.tensor([source_lengths if isinstance(source_lengths, int) else len(source_lengths)], device=self.device)\n",
        "\n",
        "        if source_seq.shape[0] != 1: raise ValueError(\"Beam search predict function currently supports batch_size=1.\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            encoder_outputs, encoder_final_hidden = self.encoder(source_seq, source_lengths)\n",
        "            decoder_hidden_init = self._adapt_encoder_hidden_for_decoder(encoder_final_hidden)\n",
        "\n",
        "            beams = [(0.0, [self.target_sos_idx], decoder_hidden_init)]\n",
        "            completed_sequences = []\n",
        "\n",
        "            for _ in range(max_output_len):\n",
        "                new_beams = []\n",
        "                all_current_beams_ended = True\n",
        "                for log_prob_beam, seq_beam, hidden_beam in beams:\n",
        "                    if not seq_beam or seq_beam[-1] == target_eos_idx:\n",
        "                        completed_sequences.append((log_prob_beam / len(seq_beam) if len(seq_beam) > 0 else -float('inf'), seq_beam))\n",
        "                        continue\n",
        "                    all_current_beams_ended = False\n",
        "\n",
        "                    decoder_input = torch.tensor([seq_beam[-1]], device=self.device)\n",
        "                    decoder_output_logits, next_hidden_beam, _ = \\\n",
        "                        self.decoder(decoder_input, hidden_beam, encoder_outputs)\n",
        "\n",
        "                    log_probs_next_token = F.log_softmax(decoder_output_logits, dim=1)\n",
        "                    topk_log_probs, topk_indices = torch.topk(log_probs_next_token, beam_width, dim=1)\n",
        "\n",
        "                    for k in range(beam_width):\n",
        "                        next_token_idx = topk_indices[0, k].item()\n",
        "                        token_log_prob = topk_log_probs[0, k].item()\n",
        "                        new_seq = seq_beam + [next_token_idx]\n",
        "                        new_log_prob = log_prob_beam + token_log_prob\n",
        "                        new_beams.append((new_log_prob, new_seq, next_hidden_beam))\n",
        "\n",
        "                if not new_beams or all_current_beams_ended : break\n",
        "\n",
        "                new_beams.sort(key=lambda x: x[0], reverse=True)\n",
        "                beams = new_beams[:beam_width]\n",
        "\n",
        "            for log_prob_beam, seq_beam, _ in beams:\n",
        "                if not seq_beam or seq_beam[-1] != target_eos_idx:\n",
        "                    completed_sequences.append((log_prob_beam / len(seq_beam) if len(seq_beam) > 0 else -float('inf'), seq_beam))\n",
        "\n",
        "            if not completed_sequences: return [target_eos_idx] if target_eos_idx is not None else []\n",
        "            completed_sequences.sort(key=lambda x: x[0], reverse=True)\n",
        "            best_sequence_indices = completed_sequences[0][1]\n",
        "            return best_sequence_indices[1:] if best_sequence_indices and best_sequence_indices[0] == self.target_sos_idx else best_sequence_indices\n",
        "\n",
        "# --- Data Loading and Preprocessing ---\n",
        "class Vocabulary:\n",
        "    \"\"\"Manages char-to-index and index-to-char mappings.\"\"\"\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.char2index = {PAD_TOKEN: 0, SOS_TOKEN: 1, EOS_TOKEN: 2, UNK_TOKEN: 3}\n",
        "        self.index2char = {0: PAD_TOKEN, 1: SOS_TOKEN, 2: EOS_TOKEN, 3: UNK_TOKEN}\n",
        "        self.char_counts = Counter()\n",
        "        self.n_chars = 4\n",
        "        self.pad_idx = self.char2index[PAD_TOKEN]\n",
        "        self.sos_idx = self.char2index[SOS_TOKEN]\n",
        "        self.eos_idx = self.char2index[EOS_TOKEN]\n",
        "\n",
        "    def add_sequence(self, sequence):\n",
        "        \"\"\"Adds characters from a sequence to the character counts.\"\"\"\n",
        "        for char in list(sequence): self.char_counts[char] += 1\n",
        "\n",
        "    def build_vocab(self, min_freq=1):\n",
        "        \"\"\"Builds vocabulary based on char counts and min frequency.\"\"\"\n",
        "        sorted_chars = sorted(self.char_counts.keys(), key=lambda char: (-self.char_counts[char], char))\n",
        "        for char in sorted_chars:\n",
        "            if self.char_counts[char] >= min_freq and char not in self.char2index:\n",
        "                self.char2index[char] = self.n_chars\n",
        "                self.index2char[self.n_chars] = char\n",
        "                self.n_chars += 1\n",
        "\n",
        "    def sequence_to_indices(self, sequence, add_eos=False, add_sos=False):\n",
        "        \"\"\"Converts char sequence to numerical indices.\"\"\"\n",
        "        indices = [self.sos_idx] if add_sos else []\n",
        "        for char in list(sequence): indices.append(self.char2index.get(char, self.char2index[UNK_TOKEN]))\n",
        "        if add_eos: indices.append(self.eos_idx)\n",
        "        return indices\n",
        "\n",
        "    def indices_to_sequence(self, indices):\n",
        "        \"\"\"Converts numerical indices back to char sequence.\"\"\"\n",
        "        chars = []\n",
        "        for index_val in indices:\n",
        "            if index_val == self.eos_idx: break\n",
        "            if index_val not in [self.sos_idx, self.pad_idx]: chars.append(self.index2char.get(index_val, UNK_TOKEN))\n",
        "        return \"\".join(chars)\n",
        "\n",
        "class TransliterationDataset(Dataset):\n",
        "    \"\"\"Loads and preprocesses transliteration pairs from a TSV file.\"\"\"\n",
        "    def __init__(self, file_path, source_vocab, target_vocab, max_len=50):\n",
        "        self.pairs = []\n",
        "        self.source_vocab = source_vocab\n",
        "        self.target_vocab = target_vocab\n",
        "        if not os.path.exists(file_path):\n",
        "            print(f\"ERROR: Data file not found: {file_path}\")\n",
        "            return\n",
        "        print(f\"Loading data from: {file_path}\")\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                parts = line.strip().split('\\t')\n",
        "                if len(parts) >= 2:\n",
        "                    target, source = parts[0], parts[1]\n",
        "                    if not source or not target or \\\n",
        "                       (max_len and (len(source) > max_len or len(target) > max_len)):\n",
        "                        continue\n",
        "                    self.pairs.append((source, target))\n",
        "        print(f\"Loaded {len(self.pairs)} pairs from {file_path}.\")\n",
        "\n",
        "    def __len__(self): return len(self.pairs)\n",
        "    def __getitem__(self, idx):\n",
        "        source_str, target_str = self.pairs[idx]\n",
        "        source_indices = self.source_vocab.sequence_to_indices(source_str, add_eos=True)\n",
        "        target_indices = self.target_vocab.sequence_to_indices(target_str, add_sos=True, add_eos=True)\n",
        "        return torch.tensor(source_indices, dtype=torch.long), torch.tensor(target_indices, dtype=torch.long)\n",
        "\n",
        "def collate_fn(batch, pad_idx_source, pad_idx_target):\n",
        "    \"\"\"Pads sequences in a batch for DataLoader.\"\"\"\n",
        "    batch = [item for item in batch if item is not None and len(item[0]) > 0 and len(item[1]) > 0]\n",
        "    if not batch: return None, None, None\n",
        "    source_seqs, target_seqs = zip(*batch)\n",
        "    source_lengths = torch.tensor([len(s) for s in source_seqs], dtype=torch.long)\n",
        "    padded_sources = pad_sequence(source_seqs, batch_first=True, padding_value=pad_idx_source)\n",
        "    padded_targets = pad_sequence(target_seqs, batch_first=True, padding_value=pad_idx_target)\n",
        "    return padded_sources, source_lengths, padded_targets\n",
        "\n",
        "# --- Training and Evaluation Functions ---\n",
        "def _train_one_epoch(model, dataloader, optimizer, criterion, device, clip_value, teacher_forcing_ratio, target_vocab):\n",
        "    \"\"\"Trains the model for one epoch.\"\"\"\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    total_correct_train = 0\n",
        "    total_samples_train = 0\n",
        "    if len(dataloader) == 0: return 0.0, 0.0\n",
        "\n",
        "    for batch_data in tqdm(dataloader, desc=\"Training\", leave=False):\n",
        "        if batch_data[0] is None: continue\n",
        "        sources, source_lengths, targets = batch_data\n",
        "        if sources is None or sources.shape[0] == 0: continue\n",
        "\n",
        "        sources, targets, source_lengths = sources.to(device), targets.to(device), source_lengths.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs_logits = model(sources, source_lengths, targets, teacher_forcing_ratio)\n",
        "\n",
        "        output_dim = outputs_logits.shape[-1]\n",
        "        flat_outputs = outputs_logits[:, 1:].reshape(-1, output_dim)\n",
        "        flat_targets = targets[:, 1:].reshape(-1)\n",
        "\n",
        "        loss = criterion(flat_outputs, flat_targets)\n",
        "        if not (torch.isnan(loss) or torch.isinf(loss)):\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        predictions_indices_train = outputs_logits.argmax(dim=2)\n",
        "        for i in range(targets.shape[0]):\n",
        "            pred_str_train = target_vocab.indices_to_sequence(predictions_indices_train[i, 1:].tolist())\n",
        "            true_str_train = target_vocab.indices_to_sequence(targets[i, 1:].tolist())\n",
        "            if pred_str_train == true_str_train: total_correct_train += 1\n",
        "            total_samples_train += 1\n",
        "\n",
        "    avg_epoch_loss = epoch_loss / len(dataloader) if len(dataloader) > 0 else 0.0\n",
        "    train_accuracy = total_correct_train / total_samples_train if total_samples_train > 0 else 0.0\n",
        "    return avg_epoch_loss, train_accuracy\n",
        "\n",
        "def _evaluate_one_epoch(model, dataloader, criterion, device, source_vocab, target_vocab, beam_width=1, is_test_set=False):\n",
        "    \"\"\"Evaluates the model on a dataset, collecting predictions and character-level data.\"\"\"\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    total_correct, total_samples = 0, 0\n",
        "    all_predictions_for_file_eval = []\n",
        "    all_true_chars_flat_eval = []\n",
        "    all_pred_chars_flat_eval = []\n",
        "\n",
        "    if len(dataloader) == 0: return 0.0, 0.0, [], [], []\n",
        "    desc_prefix = \"Testing\" if is_test_set else \"Validating\"\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch_data in enumerate(tqdm(dataloader, desc=desc_prefix, leave=False)):\n",
        "            if batch_data[0] is None: continue\n",
        "            sources, source_lengths, targets = batch_data\n",
        "            if sources is None or sources.shape[0] == 0: continue\n",
        "            sources, targets, source_lengths = sources.to(device), targets.to(device), source_lengths.to(device)\n",
        "\n",
        "            outputs_for_loss = model(sources, source_lengths, targets, teacher_forcing_ratio=0.0)\n",
        "            output_dim = outputs_for_loss.shape[-1]\n",
        "            flat_outputs_for_loss = outputs_for_loss[:, 1:].reshape(-1, output_dim)\n",
        "            flat_targets_for_loss = targets[:, 1:].reshape(-1)\n",
        "            loss = criterion(flat_outputs_for_loss, flat_targets_for_loss)\n",
        "            epoch_loss += loss.item() if not (torch.isnan(loss) or torch.isinf(loss)) else 0\n",
        "\n",
        "            for i in range(sources.shape[0]):\n",
        "                src_single_indices = sources[i].tolist()\n",
        "                src_single_tensor, src_len_single = sources[i:i+1], source_lengths[i:i+1]\n",
        "                original_source_str = source_vocab.indices_to_sequence(src_single_indices)\n",
        "\n",
        "                if beam_width > 1 and hasattr(model, 'predict_beam_search'):\n",
        "                    predicted_indices = model.predict_beam_search(src_single_tensor, src_len_single,\n",
        "                                                                  max_output_len=targets.size(1) + 5,\n",
        "                                                                  target_eos_idx=target_vocab.eos_idx,\n",
        "                                                                  beam_width=beam_width)\n",
        "                else:\n",
        "                    predicted_indices = model.predict_greedy(src_single_tensor, src_len_single,\n",
        "                                                             max_output_len=targets.size(1) + 5,\n",
        "                                                             target_eos_idx=target_vocab.eos_idx)\n",
        "\n",
        "                pred_str = target_vocab.indices_to_sequence(predicted_indices)\n",
        "                true_str = target_vocab.indices_to_sequence(targets[i, 1:].tolist())\n",
        "\n",
        "                if is_test_set:\n",
        "                    all_predictions_for_file_eval.append((original_source_str, pred_str, true_str))\n",
        "                    true_chars_cm = list(true_str)\n",
        "                    pred_chars_cm = list(pred_str)\n",
        "                    min_len_cm = min(len(true_chars_cm), len(pred_chars_cm))\n",
        "                    all_true_chars_flat_eval.extend(true_chars_cm[:min_len_cm])\n",
        "                    all_pred_chars_flat_eval.extend(pred_chars_cm[:min_len_cm])\n",
        "\n",
        "                if pred_str == true_str: total_correct += 1\n",
        "                total_samples += 1\n",
        "\n",
        "                if is_test_set and batch_idx == 0 and i < 3:\n",
        "                    print(f\"  Test Example {i} - Source: '{original_source_str}'\")\n",
        "                    print(f\"    Pred: '{pred_str}', True: '{true_str}', Match: {pred_str == true_str}\")\n",
        "\n",
        "    avg_epoch_loss = epoch_loss / len(dataloader) if len(dataloader) > 0 else 0.0\n",
        "    accuracy = total_correct / total_samples if total_samples > 0 else 0.0\n",
        "    print(f\"{desc_prefix} - Total Correct: {total_correct}, Total Samples: {total_samples}, Accuracy: {accuracy:.4f}, Avg Loss: {avg_epoch_loss:.4f}\")\n",
        "\n",
        "    if is_test_set:\n",
        "        return avg_epoch_loss, accuracy, all_predictions_for_file_eval, all_true_chars_flat_eval, all_pred_chars_flat_eval\n",
        "    else:\n",
        "        return avg_epoch_loss, accuracy\n",
        "\n",
        "\n",
        "# --- Function to Train and Save the Best Attention Model ---\n",
        "def train_and_save_attention_model(config_params, model_save_path, device):\n",
        "    \"\"\"Trains and saves the best attention model using provided hyperparameters.\"\"\"\n",
        "    run_name_train_best_attn = f\"TRAIN_BEST_ATTN_{config_params['cell_type']}_emb{config_params['embedding_dim']}_hid{config_params['hidden_dim']}\"\n",
        "\n",
        "    with wandb.init(project=\"DL_A3_Attention_Training\", name=run_name_train_best_attn, config=config_params, job_type=\"training_best_attention_model\", reinit=True) as run:\n",
        "        cfg = wandb.config\n",
        "        print(f\"Starting dedicated training for BEST ATTENTION model with config: {cfg}\")\n",
        "\n",
        "        BASE_DATA_DIR = \"/kaggle/input/dakshina-dl-a3/dakshina_dataset_v1.0/hi/\"\n",
        "        DATA_DIR = os.path.join(BASE_DATA_DIR, \"lexicons/\")\n",
        "        train_file = os.path.join(DATA_DIR, \"hi.translit.sampled.train.tsv\")\n",
        "        dev_file = os.path.join(DATA_DIR, \"hi.translit.sampled.dev.tsv\")\n",
        "\n",
        "        source_vocab = Vocabulary(\"latin\")\n",
        "        target_vocab = Vocabulary(\"devanagari\")\n",
        "        temp_train_ds_vocab = TransliterationDataset(train_file, source_vocab, target_vocab, max_len=cfg.max_seq_len)\n",
        "        if not temp_train_ds_vocab.pairs: return False\n",
        "        for src, tgt in temp_train_ds_vocab.pairs:\n",
        "            source_vocab.add_sequence(src)\n",
        "            target_vocab.add_sequence(tgt)\n",
        "        source_vocab.build_vocab(min_freq=cfg.vocab_min_freq)\n",
        "        target_vocab.build_vocab(min_freq=cfg.vocab_min_freq)\n",
        "\n",
        "        train_dataset = TransliterationDataset(train_file, source_vocab, target_vocab, max_len=cfg.max_seq_len)\n",
        "        dev_dataset = TransliterationDataset(dev_file, source_vocab, target_vocab, max_len=cfg.max_seq_len)\n",
        "        if not train_dataset.pairs or not dev_dataset.pairs: return False\n",
        "\n",
        "        num_w = 0 if device.type == 'cpu' else min(4, os.cpu_count()//2 if os.cpu_count() else 0)\n",
        "        train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size_train, shuffle=True,\n",
        "                                  collate_fn=lambda b: collate_fn(b, source_vocab.pad_idx, target_vocab.pad_idx),\n",
        "                                  num_workers=num_w, pin_memory=True if device.type == 'cuda' else False, drop_last=True)\n",
        "        dev_loader = DataLoader(dev_dataset, batch_size=cfg.batch_size_train, shuffle=False,\n",
        "                                collate_fn=lambda b: collate_fn(b, source_vocab.pad_idx, target_vocab.pad_idx),\n",
        "                                num_workers=num_w, pin_memory=True if device.type == 'cuda' else False)\n",
        "        if not train_loader or not dev_loader: return False\n",
        "\n",
        "        encoder_hidden_dim_eff = cfg.hidden_dim * (2 if cfg.encoder_bidirectional else 1)\n",
        "        encoder = Encoder(source_vocab.n_chars, cfg.embedding_dim, cfg.hidden_dim,\n",
        "                          cfg.encoder_layers, cfg.cell_type, cfg.dropout_p,\n",
        "                          cfg.encoder_bidirectional, pad_idx=source_vocab.pad_idx).to(device)\n",
        "        decoder = DecoderWithAttention(target_vocab.n_chars, cfg.embedding_dim, encoder_hidden_dim_eff,\n",
        "                                       cfg.hidden_dim, cfg.decoder_layers, cfg.cell_type,\n",
        "                                       cfg.dropout_p, pad_idx=target_vocab.pad_idx).to(device)\n",
        "        model = Seq2SeqWithAttention(encoder, decoder, device, target_vocab.sos_idx).to(device)\n",
        "        print(f\"Best Attention Model parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
        "        wandb.watch(model, log=\"all\", log_freq=100)\n",
        "\n",
        "        optimizer = optim.Adam(model.parameters(), lr=cfg.learning_rate_train)\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=cfg.lr_scheduler_patience_train, factor=0.3, verbose=True)\n",
        "        criterion = nn.CrossEntropyLoss(ignore_index=target_vocab.pad_idx)\n",
        "\n",
        "        best_val_acc_this_training = -1.0\n",
        "        epochs_no_improve = 0\n",
        "\n",
        "        for epoch in range(cfg.epochs_train):\n",
        "            train_loss, train_acc = _train_one_epoch(model, train_loader, optimizer, criterion, device,\n",
        "                                                     cfg.clip_value_train, cfg.teacher_forcing_train, target_vocab)\n",
        "            val_loss, val_acc = _evaluate_one_epoch(model, dev_loader, criterion, device,\n",
        "                                                    source_vocab, target_vocab, beam_width=1)\n",
        "\n",
        "            scheduler.step(val_acc)\n",
        "            print(f\"Epoch {epoch+1}/{cfg.epochs_train} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "            wandb.log({\"epoch_train_best_attn\": epoch + 1, \"train_loss_best_attn\": train_loss, \"train_acc_best_attn\": train_acc,\n",
        "                       \"val_loss_best_attn\": val_loss, \"val_acc_best_attn\": val_acc, \"lr_best_attn\": optimizer.param_groups[0]['lr']})\n",
        "\n",
        "            if val_acc > best_val_acc_this_training:\n",
        "                best_val_acc_this_training = val_acc\n",
        "                epochs_no_improve = 0\n",
        "                torch.save(model.state_dict(), model_save_path)\n",
        "                print(f\"Saved new best attention model to {model_save_path} (Val Acc: {best_val_acc_this_training:.4f})\")\n",
        "            else:\n",
        "                epochs_no_improve += 1\n",
        "\n",
        "            if epochs_no_improve >= cfg.max_epochs_no_improve_train:\n",
        "                print(f\"Early stopping for best attention model training at epoch {epoch+1}.\")\n",
        "                break\n",
        "\n",
        "        if not os.path.exists(model_save_path):\n",
        "            torch.save(model.state_dict(), model_save_path)\n",
        "            print(f\"Saved final attention model state to {model_save_path}\")\n",
        "\n",
        "        wandb.summary[\"final_best_val_accuracy_during_attn_training\"] = best_val_acc_this_training\n",
        "        print(f\"Finished training best attention model. Best Val Acc: {best_val_acc_this_training:.4f}\")\n",
        "    return True\n",
        "\n",
        "\n",
        "def display_sample_predictions_markdown(predictions_data, num_samples=15):\n",
        "    \"\"\"Generates a Markdown table for displaying a sample of predictions.\"\"\"\n",
        "    print(f\"\\n--- Displaying {min(num_samples, len(predictions_data))} Sample Predictions ---\")\n",
        "    markdown_table = \"| Input (Latin) | True Output (Devanagari) | Model Prediction (Devanagari) | Correct? |\\n\"\n",
        "    markdown_table += \"|---|---|---|---|\\n\"\n",
        "    for i, (source_str, predicted_str, true_target_str) in enumerate(predictions_data[:num_samples]):\n",
        "        is_correct = \" Yes\" if predicted_str == true_target_str else \" No\"\n",
        "        markdown_table += f\"| {source_str} | {true_target_str} | {predicted_str} | {is_correct} |\\n\"\n",
        "    print(markdown_table)\n",
        "    return markdown_table\n",
        "\n",
        "def save_all_predictions_to_file(predictions_data, output_file_path):\n",
        "    \"\"\"Saves all predictions to a TSV file.\"\"\"\n",
        "    print(f\"\\n--- Saving all predictions to file: {output_file_path} ---\")\n",
        "    os.makedirs(os.path.dirname(output_file_path), exist_ok=True)\n",
        "    count = 0\n",
        "    with open(output_file_path, 'w', encoding='utf-8') as f_out:\n",
        "        for source_str, predicted_str, _ in predictions_data:\n",
        "            f_out.write(f\"{source_str}\\t{predicted_str}\\n\")\n",
        "            count +=1\n",
        "    print(f\"Generated {count} predictions and saved to {output_file_path}\")\n",
        "\n",
        "\n",
        "# --- Main Execution Block for Q5 ---\n",
        "if __name__ == '__main__':\n",
        "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "    # --- W&B Login ---\n",
        "    try:\n",
        "        if 'KAGGLE_KERNEL_RUN_TYPE' in os.environ:\n",
        "            print(\"Detected Kaggle environment. Ensure WANDB_API_KEY is set.\")\n",
        "            if \"WANDB_API_KEY\" in os.environ: wandb.login()\n",
        "            else:\n",
        "                from kaggle_secrets import UserSecretsClient\n",
        "                user_secrets = UserSecretsClient()\n",
        "                wandb_api_key = user_secrets.get_secret(\"WANDB_API_KEY\")\n",
        "                wandb.login(key=wandb_api_key)\n",
        "            print(\"W&B login for Kaggle successful.\")\n",
        "        else:\n",
        "            wandb.login()\n",
        "        print(\"W&B login process attempted/completed.\")\n",
        "    except Exception as e:\n",
        "        print(f\"W&B login failed: {e}. Ensure API key is configured.\")\n",
        "        exit()\n",
        "\n",
        "    # --- Best Attention Model Hyperparameters (UPDATE THIS SECTION) ---\n",
        "    # These should come from a NEW W&B sweep specifically for the Attention Model.\n",
        "    # The values below are placeholders and need to be replaced with your findings.\n",
        "    BEST_ATTN_HYPERPARAMETERS = {\n",
        "        'embedding_dim': 256,\n",
        "        'hidden_dim': 512,\n",
        "        'encoder_layers': 1,\n",
        "        'decoder_layers': 1,\n",
        "        'cell_type': 'LSTM',\n",
        "        'dropout_p': 0.3,\n",
        "        'encoder_bidirectional': True,\n",
        "        'learning_rate_train': 0.001,\n",
        "        'batch_size_train': 64,\n",
        "        'epochs_train': 20,\n",
        "        'clip_value_train': 1.0,\n",
        "        'teacher_forcing_train': 0.5,\n",
        "        'max_epochs_no_improve_train': 5,\n",
        "        'lr_scheduler_patience_train': 2,\n",
        "        'vocab_min_freq': 1,\n",
        "        'max_seq_len': 50,\n",
        "        'eval_batch_size': 64,\n",
        "        'beam_width_eval': 3\n",
        "    }\n",
        "    ATTN_MODEL_SAVE_PATH = \"/kaggle/working/best_attention_model_q5.pt\"\n",
        "    PREDICTIONS_ATTN_OUTPUT_DIR = \"/kaggle/working/predictions_attention\"\n",
        "    PREDICTIONS_ATTN_FILE_PATH = os.path.join(PREDICTIONS_ATTN_OUTPUT_DIR, \"predictions_attention.tsv\")\n",
        "\n",
        "    print(f\"Target hyperparameters for best attention model: {BEST_ATTN_HYPERPARAMETERS}\")\n",
        "    print(\"IMPORTANT: The above BEST_ATTN_HYPERPARAMETERS are placeholders if you haven't run an attention-specific sweep.\")\n",
        "\n",
        "    # --- 1. Train and Save the Best Attention Model ---\n",
        "    print(\"\\n--- Phase 1: Training and Saving Best Attention Model Configuration ---\")\n",
        "    training_successful = train_and_save_attention_model(BEST_ATTN_HYPERPARAMETERS, ATTN_MODEL_SAVE_PATH, DEVICE)\n",
        "\n",
        "    if not training_successful or not os.path.exists(ATTN_MODEL_SAVE_PATH):\n",
        "        print(f\"ERROR: Failed to train/save the attention model, or checkpoint not found at {ATTN_MODEL_SAVE_PATH}. Exiting.\")\n",
        "        exit()\n",
        "    print(f\"Best attention model trained and saved to {ATTN_MODEL_SAVE_PATH}\")\n",
        "\n",
        "    # --- 2. Load and Evaluate on Test Set ---\n",
        "    print(\"\\n--- Phase 2: Loading and Evaluating Best Attention Model on Test Set ---\")\n",
        "\n",
        "    BASE_DATA_DIR = \"/kaggle/input/dakshina-dl-a3/dakshina_dataset_v1.0/hi/\"\n",
        "    DATA_DIR = os.path.join(BASE_DATA_DIR, \"lexicons/\")\n",
        "    train_file_for_vocab = os.path.join(DATA_DIR, \"hi.translit.sampled.train.tsv\")\n",
        "    test_file = os.path.join(DATA_DIR, \"hi.translit.sampled.test.tsv\")\n",
        "\n",
        "    source_vocab_test_attn = Vocabulary(\"latin_test_attn\")\n",
        "    target_vocab_test_attn = Vocabulary(\"devanagari_test_attn\")\n",
        "\n",
        "    temp_train_ds_test_vocab_attn = TransliterationDataset(train_file_for_vocab, source_vocab_test_attn, target_vocab_test_attn,\n",
        "                                                max_len=BEST_ATTN_HYPERPARAMETERS['max_seq_len'])\n",
        "    if not temp_train_ds_test_vocab_attn.pairs: exit()\n",
        "    for src, tgt in temp_train_ds_test_vocab_attn.pairs:\n",
        "        source_vocab_test_attn.add_sequence(src)\n",
        "        target_vocab_test_attn.add_sequence(tgt)\n",
        "    source_vocab_test_attn.build_vocab(min_freq=BEST_ATTN_HYPERPARAMETERS['vocab_min_freq'])\n",
        "    target_vocab_test_attn.build_vocab(min_freq=BEST_ATTN_HYPERPARAMETERS['vocab_min_freq'])\n",
        "\n",
        "    test_dataset_attn = TransliterationDataset(test_file, source_vocab_test_attn, target_vocab_test_attn,\n",
        "                                                  max_len=BEST_ATTN_HYPERPARAMETERS['max_seq_len'])\n",
        "    if not test_dataset_attn.pairs: exit()\n",
        "\n",
        "    num_w_test_attn = 0 if DEVICE.type == 'cpu' else min(4, os.cpu_count()//2 if os.cpu_count() else 0)\n",
        "    test_dataloader_attn = DataLoader(test_dataset_attn, batch_size=BEST_ATTN_HYPERPARAMETERS['eval_batch_size'], shuffle=False,\n",
        "                                 collate_fn=lambda b: collate_fn(b, source_vocab_test_attn.pad_idx, target_vocab_test_attn.pad_idx),\n",
        "                                 num_workers=num_w_test_attn, pin_memory=True if DEVICE.type == 'cuda' else False)\n",
        "    if not test_dataloader_attn or len(test_dataloader_attn) == 0: exit()\n",
        "\n",
        "    encoder_hidden_dim_eff_test = BEST_ATTN_HYPERPARAMETERS['hidden_dim'] * (2 if BEST_ATTN_HYPERPARAMETERS['encoder_bidirectional'] else 1)\n",
        "    encoder_test_attn = Encoder(source_vocab_test_attn.n_chars, BEST_ATTN_HYPERPARAMETERS['embedding_dim'], BEST_ATTN_HYPERPARAMETERS['hidden_dim'],\n",
        "                                BEST_ATTN_HYPERPARAMETERS['encoder_layers'], BEST_ATTN_HYPERPARAMETERS['cell_type'], BEST_ATTN_HYPERPARAMETERS['dropout_p'],\n",
        "                                BEST_ATTN_HYPERPARAMETERS['encoder_bidirectional'], pad_idx=source_vocab_test_attn.pad_idx).to(DEVICE)\n",
        "    decoder_test_attn = DecoderWithAttention(target_vocab_test_attn.n_chars, BEST_ATTN_HYPERPARAMETERS['embedding_dim'],\n",
        "                                             encoder_hidden_dim_eff_test, BEST_ATTN_HYPERPARAMETERS['hidden_dim'],\n",
        "                                             BEST_ATTN_HYPERPARAMETERS['decoder_layers'], BEST_ATTN_HYPERPARAMETERS['cell_type'], BEST_ATTN_HYPERPARAMETERS['dropout_p'],\n",
        "                                             pad_idx=target_vocab_test_attn.pad_idx).to(DEVICE)\n",
        "    model_test_attn = Seq2SeqWithAttention(encoder_test_attn, decoder_test_attn, DEVICE, target_vocab_test_attn.sos_idx).to(DEVICE)\n",
        "\n",
        "    print(f\"Loading weights into attention model from: {ATTN_MODEL_SAVE_PATH}\")\n",
        "    model_test_attn.load_state_dict(torch.load(ATTN_MODEL_SAVE_PATH, map_location=DEVICE))\n",
        "    print(\"Attention model weights loaded successfully for test evaluation.\")\n",
        "\n",
        "    criterion_test_attn = nn.CrossEntropyLoss(ignore_index=target_vocab_test_attn.pad_idx)\n",
        "\n",
        "    # (5b) Evaluate Attention Model on Test Set\n",
        "    test_loss_attn, test_accuracy_attn, all_test_predictions_data_attn, \\\n",
        "    all_true_chars_cm_attn, all_pred_chars_cm_attn = _evaluate_one_epoch(\n",
        "                                                            model_test_attn, test_dataloader_attn, criterion_test_attn, DEVICE,\n",
        "                                                            source_vocab_test_attn, target_vocab_test_attn,\n",
        "                                                            beam_width=BEST_ATTN_HYPERPARAMETERS.get('beam_width_eval', 1),\n",
        "                                                            is_test_set=True)\n",
        "\n",
        "    print(f\"\\n--- (5b) FINAL TEST SET PERFORMANCE (ATTENTION MODEL) ---\")\n",
        "    print(f\"Model Checkpoint: '{ATTN_MODEL_SAVE_PATH}'\")\n",
        "    print(f\"Hyperparameters: {BEST_ATTN_HYPERPARAMETERS}\")\n",
        "    print(f\"  Test Loss (avg per batch): {test_loss_attn:.4f}\")\n",
        "    print(f\"  Test Accuracy (Exact Match): {test_accuracy_attn:.4f} ({test_accuracy_attn*100:.2f}%)\")\n",
        "\n",
        "    # (5b) Save all predictions to file\n",
        "    if not os.path.exists(PREDICTIONS_ATTN_OUTPUT_DIR):\n",
        "        os.makedirs(PREDICTIONS_ATTN_OUTPUT_DIR)\n",
        "    save_all_predictions_to_file(all_test_predictions_data_attn, PREDICTIONS_ATTN_FILE_PATH)\n",
        "    print(f\"All attention model test predictions saved to: {PREDICTIONS_ATTN_FILE_PATH}\")\n",
        "    print(f\"Please upload the file '{os.path.basename(PREDICTIONS_ATTN_FILE_PATH)}' to a folder named 'predictions_attention' in your GitHub project.\")\n",
        "\n",
        "    # --- Log Test Results and Artifacts to W&B ---\n",
        "    try:\n",
        "        run_name_final_test_attn = f\"FINAL_TEST_ATTN_{BEST_ATTN_HYPERPARAMETERS['cell_type']}\"\n",
        "        with wandb.init(project=\"DL_A3_Attention_Test\", name=run_name_final_test_attn, config=BEST_ATTN_HYPERPARAMETERS, job_type=\"final_attention_evaluation\", reinit=True) as test_run_attn:\n",
        "            test_run_attn.summary[\"final_test_accuracy_attention\"] = test_accuracy_attn\n",
        "            test_run_attn.summary[\"final_test_loss_attention\"] = test_loss_attn\n",
        "            test_run_attn.summary[\"model_checkpoint_used_attention\"] = ATTN_MODEL_SAVE_PATH\n",
        "\n",
        "            # Log sample predictions table\n",
        "            wandb_table_cols_attn = [\"Input (Latin)\", \"True Output (Devanagari)\", \"Model Prediction (Devanagari)\", \"Correct?\"]\n",
        "            wandb_table_data_attn = []\n",
        "            for src, pred, true_tgt in all_test_predictions_data_attn[:50]: # Log first 50 samples\n",
        "                wandb_table_data_attn.append([src, true_tgt, pred, \"Yes\" if pred == true_tgt else \"No\"])\n",
        "            test_run_attn.log({\"test_sample_predictions_attention\": wandb.Table(columns=wandb_table_cols_attn, data=wandb_table_data_attn)})\n",
        "\n",
        "            # Log predictions file as an artifact\n",
        "            if os.path.exists(PREDICTIONS_ATTN_FILE_PATH):\n",
        "                predictions_artifact_attn = wandb.Artifact(\"test_predictions_attention_file\", type=\"predictions\")\n",
        "                predictions_artifact_attn.add_file(PREDICTIONS_ATTN_FILE_PATH)\n",
        "                test_run_attn.log_artifact(predictions_artifact_attn)\n",
        "\n",
        "            # Log character-level confusion matrix\n",
        "            if all_true_chars_cm_attn and all_pred_chars_cm_attn:\n",
        "                # Get unique characters that are actually in both true and pred lists and in vocab\n",
        "                cm_labels_indices_attn = sorted(list(set(target_vocab_test_attn.char2index[c] for c in all_true_chars_cm_attn + all_pred_chars_cm_attn\n",
        "                                                          if c not in [SOS_TOKEN, EOS_TOKEN, PAD_TOKEN, UNK_TOKEN] and c in target_vocab_test_attn.char2index)))\n",
        "                cm_labels_chars_attn = [target_vocab_test_attn.index2char[i] for i in cm_labels_indices_attn]\n",
        "\n",
        "                if cm_labels_chars_attn: # Only plot if there are valid characters to plot\n",
        "                    # Filter true/pred chars to only include those in cm_labels_chars_attn to avoid errors with unknown/special tokens\n",
        "                    filtered_true_chars_attn = [c for c in all_true_chars_cm_attn if c in cm_labels_chars_attn]\n",
        "                    filtered_pred_chars_attn = [c for c in all_pred_chars_cm_attn if c in cm_labels_chars_attn]\n",
        "\n",
        "                    min_len_for_cm_attn = min(len(filtered_true_chars_attn), len(filtered_pred_chars_attn))\n",
        "                    if min_len_for_cm_attn > 0:\n",
        "                        test_run_attn.log({\"char_confusion_matrix_attention\": wandb.plot.confusion_matrix(\n",
        "                                                        preds=filtered_pred_chars_attn[:min_len_for_cm_attn],\n",
        "                                                        y_true=filtered_true_chars_attn[:min_len_for_cm_attn],\n",
        "                                                        class_names=cm_labels_chars_attn\n",
        "                                                    )})\n",
        "                        print(\"Attention model character-level confusion matrix logged to W&B.\")\n",
        "            print(\"Attention model final test results and artifacts logged to W&B.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not log attention model final test results to W&B: {e}\")"
      ],
      "metadata": {
        "id": "ku3C94anT3BT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 5c table"
      ],
      "metadata": {
        "id": "qCQm_szrT3RZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import numpy as np\n",
        "import wandb\n",
        "import os\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "import heapq\n",
        "import matplotlib\n",
        "matplotlib.use('Agg') # Use a non-interactive backend for Kaggle, crucial for saving plots\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "# --- Constants for Special Tokens ---\n",
        "SOS_TOKEN = \"<sos>\"  # Start-of-sequence token\n",
        "EOS_TOKEN = \"<eos>\"  # End-of-sequence token\n",
        "PAD_TOKEN = \"<pad>\"  # Padding token\n",
        "UNK_TOKEN = \"<unk>\"  # Unknown token\n",
        "\n",
        "# --- Attention Mechanism ---\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements a general (Luong-style) attention mechanism to compute alignment scores\n",
        "    between the decoder's current hidden state and all encoder's output states.\n",
        "    \"\"\"\n",
        "    def __init__(self, encoder_hidden_dim_eff, decoder_hidden_dim):\n",
        "        super(Attention, self).__init__()\n",
        "        self.attn_W = nn.Linear(encoder_hidden_dim_eff + decoder_hidden_dim, decoder_hidden_dim)\n",
        "        self.attn_v = nn.Linear(decoder_hidden_dim, 1, bias=False)\n",
        "\n",
        "    def forward(self, decoder_hidden_top_layer, encoder_outputs):\n",
        "        \"\"\"\n",
        "        Calculates attention weights.\n",
        "\n",
        "        Args:\n",
        "            decoder_hidden_top_layer (torch.Tensor): The top layer's hidden state of the decoder RNN (batch_size, dec_hidden_dim).\n",
        "            encoder_outputs (torch.Tensor): All hidden states from the encoder (batch_size, src_len, enc_hidden_dim).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Attention weights (batch_size, src_len) representing alignment probabilities.\n",
        "        \"\"\"\n",
        "        src_len = encoder_outputs.size(1)\n",
        "        repeated_decoder_hidden = decoder_hidden_top_layer.unsqueeze(1).repeat(1, src_len, 1)\n",
        "\n",
        "        energy_input = torch.cat((repeated_decoder_hidden, encoder_outputs), dim=2)\n",
        "        energy = torch.tanh(self.attn_W(energy_input))\n",
        "\n",
        "        attention_scores = self.attn_v(energy).squeeze(2)\n",
        "        return F.softmax(attention_scores, dim=1)\n",
        "\n",
        "# --- Model Definition (Encoder, DecoderWithAttention, Seq2SeqWithAttention) ---\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    The Encoder processes the input sequence, producing a context vector (final hidden state)\n",
        "    and a sequence of output states for the attention mechanism.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_vocab_size, embedding_dim, hidden_dim, n_layers,\n",
        "                 cell_type='LSTM', dropout_p=0.1, bidirectional=False, pad_idx=0):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.input_vocab_size = input_vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.cell_type = cell_type.upper()\n",
        "        self.bidirectional = bidirectional\n",
        "        self.num_directions = 2 if bidirectional else 1\n",
        "\n",
        "        self.embedding = nn.Embedding(input_vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "        rnn_dropout = dropout_p if n_layers > 1 else 0\n",
        "        if self.cell_type == 'RNN':\n",
        "            self.rnn = nn.RNN(embedding_dim, hidden_dim, n_layers,\n",
        "                              dropout=rnn_dropout, batch_first=True, bidirectional=self.bidirectional)\n",
        "        elif self.cell_type == 'GRU':\n",
        "            self.rnn = nn.GRU(embedding_dim, hidden_dim, n_layers,\n",
        "                              dropout=rnn_dropout, batch_first=True, bidirectional=self.bidirectional)\n",
        "        elif self.cell_type == 'LSTM':\n",
        "            self.rnn = nn.LSTM(embedding_dim, hidden_dim, n_layers,\n",
        "                               dropout=rnn_dropout, batch_first=True, bidirectional=self.bidirectional)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported cell type. Choose from 'RNN', 'GRU', 'LSTM'.\")\n",
        "\n",
        "    def forward(self, input_seq, input_lengths):\n",
        "        \"\"\"\n",
        "        Forward pass for the encoder.\n",
        "\n",
        "        Args:\n",
        "            input_seq (torch.Tensor): Padded input sequences (batch_size, seq_len).\n",
        "            input_lengths (torch.Tensor): Lengths of the original sequences (batch_size,).\n",
        "\n",
        "        Returns:\n",
        "            tuple: (`encoder_outputs`, `hidden_state`).\n",
        "                   `encoder_outputs`: All hidden states from the last layer (batch_size, src_len, hidden_dim * num_directions).\n",
        "                   `hidden_state`: The final hidden state (and cell state for LSTM) from all layers.\n",
        "        \"\"\"\n",
        "        embedded = self.embedding(input_seq)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "        packed_outputs, hidden = self.rnn(packed_embedded)\n",
        "        encoder_outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs, batch_first=True)\n",
        "        return encoder_outputs, hidden\n",
        "\n",
        "class DecoderWithAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    The Decoder generates the output sequence one token at a time,\n",
        "    incorporating an attention mechanism over the encoder's output states.\n",
        "    \"\"\"\n",
        "    def __init__(self, output_vocab_size, embedding_dim, encoder_hidden_dim_eff,\n",
        "                 decoder_hidden_dim, n_layers, cell_type='LSTM', dropout_p=0.1, pad_idx=0):\n",
        "        super(DecoderWithAttention, self).__init__()\n",
        "        self.output_vocab_size = output_vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.encoder_hidden_dim_eff = encoder_hidden_dim_eff\n",
        "        self.decoder_hidden_dim = decoder_hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.cell_type = cell_type.upper()\n",
        "\n",
        "        self.attention = Attention(encoder_hidden_dim_eff, decoder_hidden_dim)\n",
        "        self.embedding = nn.Embedding(output_vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "        # RNN input is concatenation of token embedding and context vector\n",
        "        rnn_input_dim = embedding_dim + encoder_hidden_dim_eff\n",
        "\n",
        "        rnn_dropout = dropout_p if n_layers > 1 else 0\n",
        "        if self.cell_type == 'RNN':\n",
        "            self.rnn = nn.RNN(rnn_input_dim, decoder_hidden_dim, n_layers,\n",
        "                              dropout=rnn_dropout, batch_first=True)\n",
        "        elif self.cell_type == 'GRU':\n",
        "            self.rnn = nn.GRU(rnn_input_dim, decoder_hidden_dim, n_layers,\n",
        "                              dropout=rnn_dropout, batch_first=True)\n",
        "        elif self.cell_type == 'LSTM':\n",
        "            self.rnn = nn.LSTM(rnn_input_dim, decoder_hidden_dim, n_layers,\n",
        "                               dropout=rnn_dropout, batch_first=True)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported cell type. Choose from 'RNN', 'GRU', 'LSTM'.\")\n",
        "\n",
        "        self.fc_out = nn.Linear(decoder_hidden_dim, output_vocab_size)\n",
        "\n",
        "    def forward(self, input_char, prev_decoder_hidden, encoder_outputs):\n",
        "        \"\"\"\n",
        "        Forward pass for the attention-based decoder.\n",
        "\n",
        "        Args:\n",
        "            input_char (torch.Tensor): Current target token (batch_size,).\n",
        "            prev_decoder_hidden (torch.Tensor or tuple): Previous hidden state(s) of the decoder RNN.\n",
        "            encoder_outputs (torch.Tensor): All hidden states from the encoder (batch_size, src_len, enc_hidden_dim_eff).\n",
        "\n",
        "        Returns:\n",
        "            tuple: (`prediction_logits`, `current_decoder_hidden`, `attention_weights`).\n",
        "                   `prediction_logits`: Raw scores for each token in the vocabulary.\n",
        "                   `current_decoder_hidden`: Updated hidden state of the decoder RNN.\n",
        "                   `attention_weights`: Attention probabilities (batch_size, src_len).\n",
        "        \"\"\"\n",
        "        input_char = input_char.unsqueeze(1)\n",
        "        embedded = self.embedding(input_char)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        # Get the hidden state for attention query (top layer's hidden state)\n",
        "        if self.cell_type == 'LSTM':\n",
        "            attention_query_hidden = prev_decoder_hidden[0][-1, :, :]\n",
        "        else:\n",
        "            attention_query_hidden = prev_decoder_hidden[-1, :, :]\n",
        "\n",
        "        # Calculate attention weights and context vector\n",
        "        attention_weights = self.attention(attention_query_hidden, encoder_outputs)\n",
        "        context_vector = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)\n",
        "\n",
        "        # Concatenate embedded input and context vector to form RNN input\n",
        "        rnn_input = torch.cat((embedded, context_vector), dim=2)\n",
        "\n",
        "        # Pass through decoder RNN\n",
        "        rnn_output, current_decoder_hidden = self.rnn(rnn_input, prev_decoder_hidden)\n",
        "\n",
        "        # Project RNN output to vocabulary size\n",
        "        rnn_output_squeezed = rnn_output.squeeze(1)\n",
        "        prediction_logits = self.fc_out(rnn_output_squeezed)\n",
        "\n",
        "        return prediction_logits, current_decoder_hidden, attention_weights\n",
        "\n",
        "\n",
        "class Seq2SeqWithAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    The main Sequence-to-Sequence model integrating the Encoder and Attention-based Decoder.\n",
        "    \"\"\"\n",
        "    def __init__(self, encoder, decoder, device, target_sos_idx):\n",
        "        super(Seq2SeqWithAttention, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "        self.target_sos_idx = target_sos_idx\n",
        "\n",
        "        # Layers to adapt encoder's final hidden state to decoder's initial hidden state\n",
        "        encoder_effective_final_hidden_dim = self.encoder.hidden_dim * self.encoder.num_directions\n",
        "        decoder_rnn_hidden_dim = self.decoder.decoder_hidden_dim\n",
        "\n",
        "        self.fc_adapt_hidden = None\n",
        "        self.fc_adapt_cell = None\n",
        "        if encoder_effective_final_hidden_dim != decoder_rnn_hidden_dim:\n",
        "            self.fc_adapt_hidden = nn.Linear(encoder_effective_final_hidden_dim, decoder_rnn_hidden_dim)\n",
        "            if self.encoder.cell_type == 'LSTM':\n",
        "                self.fc_adapt_cell = nn.Linear(encoder_effective_final_hidden_dim, decoder_rnn_hidden_dim)\n",
        "\n",
        "    def _adapt_encoder_hidden_for_decoder(self, encoder_final_hidden_state):\n",
        "        \"\"\"\n",
        "        Adapts the encoder's final hidden state(s) to match the decoder's expected dimensions and layer count.\n",
        "        Handles bidirectionality and differing layer counts.\n",
        "        \"\"\"\n",
        "        is_lstm = self.encoder.cell_type == 'LSTM'\n",
        "        if is_lstm:\n",
        "            h_from_enc, c_from_enc = encoder_final_hidden_state\n",
        "        else:\n",
        "            h_from_enc = encoder_final_hidden_state\n",
        "            c_from_enc = None\n",
        "\n",
        "        batch_size = h_from_enc.size(1)\n",
        "\n",
        "        # Reshape and concatenate bidirectional states if applicable\n",
        "        h_processed = h_from_enc.view(self.encoder.n_layers, self.encoder.num_directions,\n",
        "                                      batch_size, self.encoder.hidden_dim)\n",
        "        if self.encoder.bidirectional:\n",
        "            h_processed = torch.cat([h_processed[:, 0, :, :], h_processed[:, 1, :, :]], dim=2)\n",
        "        else:\n",
        "            h_processed = h_processed.squeeze(1)\n",
        "\n",
        "        c_processed = None\n",
        "        if is_lstm and c_from_enc is not None:\n",
        "            c_processed = c_from_enc.view(self.encoder.n_layers, self.encoder.num_directions,\n",
        "                                          batch_size, self.encoder.hidden_dim)\n",
        "            if self.encoder.bidirectional:\n",
        "                c_processed = torch.cat([c_processed[:, 0, :, :], c_processed[:, 1, :, :]], dim=2)\n",
        "            else:\n",
        "                c_processed = c_processed.squeeze(1)\n",
        "\n",
        "        # Apply linear transformation for dimension adaptation if needed\n",
        "        if self.fc_adapt_hidden:\n",
        "            h_processed = self.fc_adapt_hidden(h_processed)\n",
        "            if is_lstm and c_processed is not None and self.fc_adapt_cell:\n",
        "                c_processed = self.fc_adapt_cell(c_processed)\n",
        "\n",
        "        # Adapt number of layers for the decoder's RNN\n",
        "        final_h = torch.zeros(self.decoder.n_layers, batch_size, self.decoder.decoder_hidden_dim, device=self.device)\n",
        "        final_c = torch.zeros(self.decoder.n_layers, batch_size, self.decoder.decoder_hidden_dim, device=self.device) if is_lstm else None\n",
        "\n",
        "        num_layers_to_copy = min(self.encoder.n_layers, self.decoder.n_layers)\n",
        "\n",
        "        final_h[:num_layers_to_copy, :, :] = h_processed[:num_layers_to_copy, :, :]\n",
        "        if is_lstm and c_processed is not None:\n",
        "            final_c[:num_layers_to_copy, :, :] = c_processed[:num_layers_to_copy, :, :]\n",
        "\n",
        "        # If decoder has more layers than encoder, repeat the last encoder layer's state\n",
        "        if self.decoder.n_layers > self.encoder.n_layers and self.encoder.n_layers > 0:\n",
        "            last_h_layer_to_repeat = h_processed[self.encoder.n_layers-1, :, :]\n",
        "            for i in range(self.encoder.n_layers, self.decoder.n_layers):\n",
        "                final_h[i, :, :] = last_h_layer_to_repeat\n",
        "                if is_lstm and c_processed is not None:\n",
        "                    last_c_layer_to_repeat = c_processed[self.encoder.n_layers-1, :, :]\n",
        "                    final_c[i, :, :] = last_c_layer_to_repeat\n",
        "\n",
        "        return (final_h, final_c) if is_lstm else final_h\n",
        "\n",
        "    def forward(self, source_seq, source_lengths, target_seq, teacher_forcing_ratio=0.5):\n",
        "        \"\"\"\n",
        "        Forward pass for the Seq2SeqWithAttention model during training.\n",
        "\n",
        "        Args:\n",
        "            source_seq (torch.Tensor): Padded source sequences.\n",
        "            source_lengths (torch.Tensor): Lengths of source sequences.\n",
        "            target_seq (torch.Tensor): Padded target sequences (including SOS token).\n",
        "            teacher_forcing_ratio (float): Probability of using actual target token as next input.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Logits for the predicted target sequence.\n",
        "        \"\"\"\n",
        "        batch_size = source_seq.shape[0]\n",
        "        target_len = target_seq.shape[1]\n",
        "        target_vocab_size = self.decoder.output_vocab_size\n",
        "        outputs_logits = torch.zeros(batch_size, target_len, target_vocab_size).to(self.device)\n",
        "\n",
        "        # Encode the source sequence to get all encoder outputs and final hidden state\n",
        "        encoder_outputs, encoder_final_hidden = self.encoder(source_seq, source_lengths)\n",
        "        # Adapt encoder's final hidden state to initialize the decoder's hidden state\n",
        "        decoder_hidden = self._adapt_encoder_hidden_for_decoder(encoder_final_hidden)\n",
        "        decoder_input = target_seq[:, 0]\n",
        "\n",
        "        # Iterate through the target sequence, predicting one token at a time\n",
        "        for t in range(target_len - 1):\n",
        "            decoder_output_logits, decoder_hidden, _ = \\\n",
        "                self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            outputs_logits[:, t+1] = decoder_output_logits\n",
        "\n",
        "            # Apply teacher forcing: use true target or predicted token for next input\n",
        "            teacher_force_this_step = random.random() < teacher_forcing_ratio\n",
        "            top1_predicted_token = decoder_output_logits.argmax(1)\n",
        "\n",
        "            decoder_input = target_seq[:, t+1] if teacher_force_this_step else top1_predicted_token\n",
        "        return outputs_logits\n",
        "\n",
        "    def predict_with_attention(self, source_seq, source_lengths, max_output_len=50, target_eos_idx=None):\n",
        "        \"\"\"\n",
        "        Generates a sequence using greedy decoding and records attention weights.\n",
        "\n",
        "        Args:\n",
        "            source_seq (torch.Tensor): Input source sequence (batch_size=1).\n",
        "            source_lengths (torch.Tensor): Length of the source sequence.\n",
        "            max_output_len (int): Maximum length of the sequence to generate.\n",
        "            target_eos_idx (int, optional): Index of the EOS token.\n",
        "\n",
        "        Returns:\n",
        "            tuple: Predicted token indices and attention matrices (numpy array).\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        if source_seq.dim() == 1:\n",
        "            source_seq = source_seq.unsqueeze(0)\n",
        "            source_lengths = torch.tensor([source_lengths if isinstance(source_lengths, int) else len(source_lengths)], device=self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            encoder_outputs, encoder_final_hidden = self.encoder(source_seq, source_lengths)\n",
        "            decoder_hidden = self._adapt_encoder_hidden_for_decoder(encoder_final_hidden)\n",
        "            decoder_input = torch.tensor([self.target_sos_idx], device=self.device)\n",
        "\n",
        "            predicted_indices = []\n",
        "            attention_matrices = []\n",
        "\n",
        "            for _ in range(max_output_len):\n",
        "                decoder_output_logits, decoder_hidden, attention_weights = \\\n",
        "                    self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "\n",
        "                attention_matrices.append(attention_weights.squeeze(0).cpu().numpy())\n",
        "\n",
        "                top1_predicted_token = decoder_output_logits.argmax(1)\n",
        "                predicted_idx = top1_predicted_token.item()\n",
        "\n",
        "                predicted_indices.append(predicted_idx)\n",
        "                if target_eos_idx is not None and predicted_idx == target_eos_idx:\n",
        "                    break\n",
        "\n",
        "                if len(predicted_indices) >= max_output_len: break\n",
        "                decoder_input = top1_predicted_token\n",
        "        return predicted_indices, np.array(attention_matrices) if attention_matrices else None\n",
        "\n",
        "# --- Data Loading and Preprocessing ---\n",
        "\n",
        "class Vocabulary:\n",
        "    \"\"\"\n",
        "    Manages the mapping between characters and their numerical indices.\n",
        "    Includes special tokens for padding, start-of-sequence, end-of-sequence, and unknown characters.\n",
        "    \"\"\"\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.char2index = {PAD_TOKEN: 0, SOS_TOKEN: 1, EOS_TOKEN: 2, UNK_TOKEN: 3}\n",
        "        self.index2char = {0: PAD_TOKEN, 1: SOS_TOKEN, 2: EOS_TOKEN, 3: UNK_TOKEN}\n",
        "        self.char_counts = Counter()\n",
        "        self.n_chars = 4\n",
        "        self.pad_idx = self.char2index[PAD_TOKEN]\n",
        "        self.sos_idx = self.char2index[SOS_TOKEN]\n",
        "        self.eos_idx = self.char2index[EOS_TOKEN]\n",
        "\n",
        "    def add_sequence(self, sequence):\n",
        "        \"\"\"Adds characters from a sequence to the character counts.\"\"\"\n",
        "        for char in list(sequence):\n",
        "            self.char_counts[char] += 1\n",
        "\n",
        "    def build_vocab(self, min_freq=1):\n",
        "        \"\"\"\n",
        "        Builds the vocabulary mapping based on character counts and a minimum frequency.\n",
        "        Characters appearing less than `min_freq` will be treated as UNK_TOKEN.\n",
        "        \"\"\"\n",
        "        sorted_chars = sorted(self.char_counts.keys(), key=lambda char: (-self.char_counts[char], char))\n",
        "        for char in sorted_chars:\n",
        "            if self.char_counts[char] >= min_freq and char not in self.char2index:\n",
        "                self.char2index[char] = self.n_chars\n",
        "                self.index2char[self.n_chars] = char\n",
        "                self.n_chars += 1\n",
        "\n",
        "    def sequence_to_indices(self, sequence, add_eos=False, add_sos=False, max_len=None):\n",
        "        \"\"\"Converts a character sequence into a list of numerical indices.\"\"\"\n",
        "        indices = []\n",
        "        if add_sos:\n",
        "            indices.append(self.sos_idx)\n",
        "\n",
        "        seq_to_process = list(sequence)\n",
        "        if max_len:\n",
        "            effective_max_len = max_len\n",
        "            if add_sos: effective_max_len -=1\n",
        "            if add_eos: effective_max_len -=1\n",
        "            seq_to_process = seq_to_process[:max(0, effective_max_len)] # Ensure non-negative slice\n",
        "\n",
        "        for char in seq_to_process:\n",
        "            indices.append(self.char2index.get(char, self.char2index[UNK_TOKEN]))\n",
        "\n",
        "        if add_eos: indices.append(self.eos_idx)\n",
        "        return indices\n",
        "\n",
        "    def indices_to_sequence(self, indices, strip_special=True):\n",
        "        \"\"\"Converts a list of numerical indices back into a character sequence.\"\"\"\n",
        "        chars = []\n",
        "        for index_val in indices:\n",
        "            if strip_special and index_val == self.eos_idx: break\n",
        "            if strip_special and index_val in [self.sos_idx, self.pad_idx]: continue\n",
        "            chars.append(self.index2char.get(index_val, UNK_TOKEN))\n",
        "        return \"\".join(chars)\n",
        "\n",
        "class TransliterationDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A PyTorch Dataset for loading and preparing transliteration pairs.\n",
        "    Reads data from a TSV file and converts text sequences to token indices.\n",
        "    \"\"\"\n",
        "    def __init__(self, file_path, source_vocab, target_vocab, max_len=50):\n",
        "        self.pairs = []\n",
        "        self.source_vocab = source_vocab\n",
        "        self.target_vocab = target_vocab\n",
        "        self.max_len = max_len\n",
        "\n",
        "        if not os.path.exists(file_path):\n",
        "            print(f\"ERROR: Data file not found: {file_path}\")\n",
        "            return\n",
        "\n",
        "        print(f\"Loading data from: {file_path}\")\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                for line_num, line in enumerate(f):\n",
        "                    parts = line.strip().split('\\t')\n",
        "                    if len(parts) >= 2:\n",
        "                        target, source = parts[0], parts[1]\n",
        "                        if not source or not target or \\\n",
        "                           (self.max_len and (len(source) > self.max_len or len(target) > self.max_len)):\n",
        "                            continue\n",
        "                        self.pairs.append((source, target))\n",
        "            print(f\"Loaded {len(self.pairs)} pairs from {file_path}.\")\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR: Could not read or process file {file_path}. Error: {e}\")\n",
        "\n",
        "    def __len__(self): return len(self.pairs)\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Returns a source-target pair as Tensors of indices.\"\"\"\n",
        "        source_str, target_str = self.pairs[idx]\n",
        "        source_indices = self.source_vocab.sequence_to_indices(source_str, add_eos=True, max_len=self.max_len)\n",
        "        target_indices = self.target_vocab.sequence_to_indices(target_str, add_sos=True, add_eos=True, max_len=self.max_len)\n",
        "        return torch.tensor(source_indices, dtype=torch.long), torch.tensor(target_indices, dtype=torch.long)\n",
        "\n",
        "def collate_fn(batch, pad_idx_source, pad_idx_target):\n",
        "    \"\"\"\n",
        "    Custom collate function for DataLoader to handle variable-length sequences.\n",
        "    Pads sequences within a batch to the maximum length of that batch.\n",
        "    \"\"\"\n",
        "    batch = [item for item in batch if item is not None and len(item[0]) > 0 and len(item[1]) > 0]\n",
        "    if not batch: return None, None, None\n",
        "    source_seqs, target_seqs = zip(*batch)\n",
        "\n",
        "    source_lengths = torch.tensor([len(s) for s in source_seqs], dtype=torch.long)\n",
        "    padded_sources = pad_sequence(source_seqs, batch_first=True, padding_value=pad_idx_source)\n",
        "    padded_targets = pad_sequence(target_seqs, batch_first=True, padding_value=pad_idx_target)\n",
        "    return padded_sources, source_lengths, padded_targets\n",
        "\n",
        "# --- Training and Evaluation Functions ---\n",
        "\n",
        "def _train_one_epoch(model, dataloader, optimizer, criterion, device, clip_value, teacher_forcing_ratio, target_vocab):\n",
        "    \"\"\"\n",
        "    Trains the model for a single epoch.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The Seq2Seq model.\n",
        "        dataloader (DataLoader): DataLoader for the training set.\n",
        "        optimizer (optim.Optimizer): Optimizer for model parameters.\n",
        "        criterion (nn.Module): Loss function (e.g., CrossEntropyLoss).\n",
        "        device (torch.device): Device to run the model on.\n",
        "        clip_value (float): Gradient clipping value.\n",
        "        teacher_forcing_ratio (float): Probability of using teacher forcing.\n",
        "        target_vocab (Vocabulary): Vocabulary for the target language.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Average epoch loss and training accuracy (0.0 for this simplified version).\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    if len(dataloader) == 0: return 0.0, 0.0\n",
        "\n",
        "    for batch_data in tqdm(dataloader, desc=\"Training\", leave=False):\n",
        "        if batch_data[0] is None: continue\n",
        "        sources, source_lengths, targets = batch_data\n",
        "        if sources is None or sources.shape[0] == 0: continue\n",
        "\n",
        "        sources, targets, source_lengths = sources.to(device), targets.to(device), source_lengths.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs_logits = model(sources, source_lengths, targets, teacher_forcing_ratio)\n",
        "\n",
        "        output_dim = outputs_logits.shape[-1]\n",
        "        flat_outputs = outputs_logits[:, 1:].reshape(-1, output_dim)\n",
        "        flat_targets = targets[:, 1:].reshape(-1)\n",
        "\n",
        "        loss = criterion(flat_outputs, flat_targets)\n",
        "        if not (torch.isnan(loss) or torch.isinf(loss)):\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "    # For simplicity in this heatmap script, training accuracy is not computed here\n",
        "    return epoch_loss / len(dataloader) if len(dataloader) > 0 else 0.0, 0.0\n",
        "\n",
        "\n",
        "def _evaluate_one_epoch(model, dataloader, criterion, device, source_vocab, target_vocab, beam_width=1, is_test_set=False):\n",
        "    \"\"\"\n",
        "    Evaluates the model's performance on a given dataset (validation or test).\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The Seq2Seq model.\n",
        "        dataloader (DataLoader): DataLoader for the evaluation set.\n",
        "        criterion (nn.Module): Loss function.\n",
        "        device (torch.device): Device to run the model on.\n",
        "        source_vocab (Vocabulary): Vocabulary for the source language (used for debug prints).\n",
        "        target_vocab (Vocabulary): Vocabulary for the target language.\n",
        "        beam_width (int): Beam width for decoding (1 for greedy, >1 for beam search).\n",
        "        is_test_set (bool): Flag to indicate if this is the final test evaluation (for debug prints).\n",
        "\n",
        "    Returns:\n",
        "        tuple: Average epoch loss and evaluation accuracy.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "    if len(dataloader) == 0: return 0.0, 0.0\n",
        "    desc_prefix = \"Testing\" if is_test_set else \"Validating\"\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_data in tqdm(dataloader, desc=desc_prefix, leave=False):\n",
        "            if batch_data[0] is None: continue\n",
        "            sources, source_lengths, targets = batch_data\n",
        "            if sources is None or sources.shape[0] == 0: continue\n",
        "\n",
        "            sources, targets, source_lengths = sources.to(device), targets.to(device), source_lengths.to(device)\n",
        "            outputs = model(sources, source_lengths, targets, teacher_forcing_ratio=0.0)\n",
        "            output_dim = outputs.shape[-1]\n",
        "            flat_outputs = outputs[:, 1:].reshape(-1, output_dim)\n",
        "            flat_targets = targets[:, 1:].reshape(-1)\n",
        "            loss = criterion(flat_outputs, flat_targets)\n",
        "            epoch_loss += loss.item() if not (torch.isnan(loss) or torch.isinf(loss)) else 0\n",
        "\n",
        "            # For accuracy, we need to generate predictions (greedy or beam search)\n",
        "            for i in range(targets.shape[0]):\n",
        "                src_single, src_len_single = sources[i:i+1], source_lengths[i:i+1]\n",
        "\n",
        "                # Use greedy for attention visualization\n",
        "                if hasattr(model, 'predict_with_attention') and beam_width == 1:\n",
        "                    predicted_indices, _ = model.predict_with_attention(\n",
        "                        src_single, src_len_single,\n",
        "                        max_output_len=targets.size(1) + 5,\n",
        "                        target_eos_idx=target_vocab.eos_idx\n",
        "                    )\n",
        "                # Use beam search if specified and available\n",
        "                elif hasattr(model, 'predict_beam_search') and beam_width > 1:\n",
        "                    predicted_indices = model.predict_beam_search(\n",
        "                        src_single, src_len_single,\n",
        "                        max_output_len=targets.size(1) + 5,\n",
        "                        target_eos_idx=target_vocab.eos_idx,\n",
        "                        beam_width=beam_width\n",
        "                    )\n",
        "                else: # Fallback to argmax from teacher-forced outputs if no specific prediction method\n",
        "                    predicted_indices = outputs[i:i+1].argmax(dim=2)[0, 1:].tolist()\n",
        "\n",
        "                pred_str = target_vocab.indices_to_sequence(predicted_indices, strip_special=True)\n",
        "                true_str = target_vocab.indices_to_sequence(targets[i, 1:].tolist(), strip_special=True)\n",
        "\n",
        "                if pred_str == true_str: total_correct += 1\n",
        "                total_samples += 1\n",
        "\n",
        "                if is_test_set and batch_idx == 0 and i < 3:\n",
        "                    print(f\"  Test Example {i} - Source: '{source_vocab.indices_to_sequence(src_single[0].tolist(), strip_special=True)}'\")\n",
        "                    print(f\"    Pred: '{pred_str}', True: '{true_str}', Match: {pred_str == true_str}\")\n",
        "\n",
        "    accuracy = total_correct / total_samples if total_samples > 0 else 0.0\n",
        "    return epoch_loss / len(dataloader) if len(dataloader) > 0 else 0.0, accuracy\n",
        "\n",
        "# --- Function to Train and Save the Best Attention Model ---\n",
        "\n",
        "def train_and_save_attention_model(config_params, model_save_path, device):\n",
        "    \"\"\"\n",
        "    Performs a dedicated training run using the best hyperparameters for the attention model.\n",
        "    Saves the model checkpoint with the best validation accuracy.\n",
        "\n",
        "    Args:\n",
        "        config_params (dict): Dictionary of hyperparameters for this specific training run.\n",
        "        model_save_path (str): Path to save the best model's state_dict.\n",
        "        device (torch.device): Device to run the training on.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if training was successful and a model was saved, False otherwise.\n",
        "    \"\"\"\n",
        "    run_name_train_best_attn = f\"TRAIN_BEST_ATTN_{config_params['cell_type']}_emb{config_params['embedding_dim']}_hid{config_params['hidden_dim']}\"\n",
        "\n",
        "    with wandb.init(project=\"DL_A3_Attention_Training\", name=run_name_train_best_attn, config=config_params, job_type=\"training_best_attention_model\", reinit=True) as run:\n",
        "        cfg = wandb.config\n",
        "        print(f\"Starting dedicated training for BEST ATTENTION model with config: {cfg}\")\n",
        "\n",
        "        # --- Data Loading and Vocabulary Building ---\n",
        "        BASE_DATA_DIR = \"/kaggle/input/dakshina-dl-a3/dakshina_dataset_v1.0/hi/\"\n",
        "        DATA_DIR = os.path.join(BASE_DATA_DIR, \"lexicons/\")\n",
        "        train_file = os.path.join(DATA_DIR, \"hi.translit.sampled.train.tsv\")\n",
        "        dev_file = os.path.join(DATA_DIR, \"hi.translit.sampled.dev.tsv\")\n",
        "\n",
        "        source_vocab = Vocabulary(\"latin\")\n",
        "        target_vocab = Vocabulary(\"devanagari\")\n",
        "        temp_train_ds_vocab = TransliterationDataset(train_file, source_vocab, target_vocab, max_len=cfg.max_seq_len)\n",
        "        if not temp_train_ds_vocab.pairs:\n",
        "            print(f\"ERROR: No data for vocab from {train_file}\")\n",
        "            return False\n",
        "        for src, tgt in temp_train_ds_vocab.pairs:\n",
        "            source_vocab.add_sequence(src)\n",
        "            target_vocab.add_sequence(tgt)\n",
        "        source_vocab.build_vocab(min_freq=cfg.vocab_min_freq)\n",
        "        target_vocab.build_vocab(min_freq=cfg.vocab_min_freq)\n",
        "\n",
        "        train_dataset = TransliterationDataset(train_file, source_vocab, target_vocab, max_len=cfg.max_seq_len)\n",
        "        dev_dataset = TransliterationDataset(dev_file, source_vocab, target_vocab, max_len=cfg.max_seq_len)\n",
        "        if not train_dataset.pairs or not dev_dataset.pairs:\n",
        "            print(f\"ERROR: Train or Dev dataset empty. Train: {len(train_dataset)}, Dev: {len(dev_dataset)}\")\n",
        "            return False\n",
        "\n",
        "        # --- DataLoader Setup ---\n",
        "        num_w = 0 if device.type == 'cpu' else min(4, os.cpu_count()//2 if os.cpu_count() else 0)\n",
        "        train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size_train, shuffle=True,\n",
        "                                  collate_fn=lambda b: collate_fn(b, source_vocab.pad_idx, target_vocab.pad_idx),\n",
        "                                  num_workers=num_w, pin_memory=True if device.type == 'cuda' else False, drop_last=True)\n",
        "        dev_loader = DataLoader(dev_dataset, batch_size=cfg.batch_size_train, shuffle=False,\n",
        "                                collate_fn=lambda b: collate_fn(b, source_vocab.pad_idx, target_vocab.pad_idx),\n",
        "                                num_workers=num_w, pin_memory=True if device.type == 'cuda' else False)\n",
        "        if not train_loader or len(train_loader)==0 or not dev_loader or len(dev_loader)==0:\n",
        "            print(f\"ERROR: Train/Dev Dataloader empty. Train: {len(train_loader)}, Dev: {len(dev_loader)}\")\n",
        "            return False\n",
        "\n",
        "        # --- Model, Optimizer, Loss Function Setup ---\n",
        "        encoder_hidden_dim_eff = cfg.hidden_dim * (2 if cfg.encoder_bidirectional else 1)\n",
        "        encoder = Encoder(source_vocab.n_chars, cfg.embedding_dim, cfg.hidden_dim,\n",
        "                          cfg.encoder_layers, cfg.cell_type, cfg.dropout_p,\n",
        "                          cfg.encoder_bidirectional, pad_idx=source_vocab.pad_idx).to(device)\n",
        "        decoder = DecoderWithAttention(target_vocab.n_chars, cfg.embedding_dim, encoder_hidden_dim_eff,\n",
        "                                       cfg.hidden_dim, cfg.decoder_layers, cfg.cell_type,\n",
        "                                       cfg.dropout_p, pad_idx=target_vocab.pad_idx).to(device)\n",
        "        model = Seq2SeqWithAttention(encoder, decoder, device, target_vocab.sos_idx).to(device)\n",
        "        print(f\"Best Attention Model parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
        "        wandb.watch(model, log=\"all\", log_freq=100)\n",
        "\n",
        "        optimizer = optim.Adam(model.parameters(), lr=cfg.learning_rate_train)\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=cfg.lr_scheduler_patience_train, factor=0.3, verbose=True)\n",
        "        criterion = nn.CrossEntropyLoss(ignore_index=target_vocab.pad_idx)\n",
        "\n",
        "        best_val_acc_this_training = -1.0\n",
        "        epochs_no_improve = 0\n",
        "        max_epochs_no_improve_val = cfg.get('max_epochs_no_improve_train', 7)\n",
        "\n",
        "        for epoch in range(cfg.epochs_train):\n",
        "            train_loss, _ = _train_one_epoch(model, train_loader, optimizer, criterion, device,\n",
        "                                             cfg.clip_value_train, cfg.teacher_forcing_train, target_vocab)\n",
        "            val_loss, val_acc = _evaluate_one_epoch(model, dev_loader, criterion, device,\n",
        "                                                    source_vocab, target_vocab, beam_width=1, is_test_set=False)\n",
        "\n",
        "            scheduler.step(val_acc)\n",
        "            print(f\"Epoch {epoch+1}/{cfg.epochs_train} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "            wandb.log({\"epoch_train_best_attn\": epoch + 1, \"train_loss_best_attn\": train_loss,\n",
        "                       \"val_loss_best_attn\": val_loss, \"val_acc_best_attn\": val_acc, \"lr_best_attn\": optimizer.param_groups[0]['lr']})\n",
        "\n",
        "            # Early stopping logic\n",
        "            if val_acc > best_val_acc_this_training:\n",
        "                best_val_acc_this_training = val_acc\n",
        "                epochs_no_improve = 0\n",
        "                torch.save(model.state_dict(), model_save_path)\n",
        "                print(f\"Saved new best attention model to {model_save_path} (Val Acc: {best_val_acc_this_training:.4f})\")\n",
        "            else:\n",
        "                epochs_no_improve += 1\n",
        "\n",
        "            if epochs_no_improve >= max_epochs_no_improve_val:\n",
        "                print(f\"Early stopping for best attention model training at epoch {epoch+1}.\")\n",
        "                break\n",
        "\n",
        "        # Save the final model state if no improvement happened over the initial checkpoint\n",
        "        if not os.path.exists(model_save_path):\n",
        "            torch.save(model.state_dict(), model_save_path)\n",
        "            print(f\"Saved final attention model state to {model_save_path}\")\n",
        "\n",
        "        wandb.summary[\"final_best_val_accuracy_during_attn_training\"] = best_val_acc_this_training\n",
        "        print(f\"Finished training best attention model. Best Val Acc during this training: {best_val_acc_this_training:.4f}\")\n",
        "    return True\n",
        "\n",
        "\n",
        "def plot_attention_heatmap(source_chars, predicted_chars, attention_matrix, file_path=\"attention_heatmap.png\", title=\"Attention Heatmap\"):\n",
        "    \"\"\"\n",
        "    Plots an attention heatmap and saves it to a file.\n",
        "\n",
        "    Args:\n",
        "        source_chars (list): List of characters from the source sequence.\n",
        "        predicted_chars (list): List of characters from the predicted target sequence.\n",
        "        attention_matrix (np.ndarray): 2D NumPy array of attention weights (rows=target, cols=source).\n",
        "        file_path (str): Path to save the generated heatmap image.\n",
        "        title (str): Title for the heatmap plot.\n",
        "    \"\"\"\n",
        "    if attention_matrix is None or not isinstance(attention_matrix, np.ndarray) or attention_matrix.ndim != 2 or attention_matrix.shape[0] == 0 or attention_matrix.shape[1] == 0:\n",
        "        print(f\"Warning: Invalid attention matrix for '{''.join(source_chars)}' -> '{''.join(predicted_chars)}'. Skipping plot.\")\n",
        "        return\n",
        "\n",
        "    plot_pred_len = len(predicted_chars)\n",
        "    plot_src_len = len(source_chars)\n",
        "\n",
        "    if plot_pred_len == 0 or plot_src_len == 0:\n",
        "        print(f\"Warning: Empty source or predicted characters for heatmap. Source: {plot_src_len}, Pred: {plot_pred_len}. Skipping.\")\n",
        "        return\n",
        "\n",
        "    current_attention_matrix = attention_matrix[:plot_pred_len, :plot_src_len]\n",
        "\n",
        "    if current_attention_matrix.shape[0] != plot_pred_len or current_attention_matrix.shape[1] != plot_src_len :\n",
        "        print(f\"Warning: Attention matrix shape ({attention_matrix.shape}) \"\n",
        "              f\"could not be perfectly aligned with char lists (Pred:{plot_pred_len}, Src:{plot_src_len}). \"\n",
        "              f\"Using shape {current_attention_matrix.shape} for plot. Input: '{''.join(source_chars)}'\")\n",
        "        if not (current_attention_matrix.shape[0] > 0 and current_attention_matrix.shape[1] > 0):\n",
        "             print(\"  Skipping plot due to zero dimension after alignment.\")\n",
        "             return\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(max(6, plot_src_len*0.7), max(4, plot_pred_len*0.7)))\n",
        "    cax = ax.matshow(current_attention_matrix, cmap='viridis')\n",
        "    fig.colorbar(cax)\n",
        "\n",
        "    try: # Attempt to set Devanagari font\n",
        "        ax.set_yticklabels([''] + predicted_chars, fontfamily='Arial Unicode MS', fontsize=10)\n",
        "    except:\n",
        "        print(\"Warning: Arial Unicode MS font not found. Using default sans-serif for Devanagari labels.\")\n",
        "        ax.set_yticklabels([''] + predicted_chars, fontfamily='sans-serif', fontsize=10)\n",
        "    ax.set_xticklabels([''] + source_chars, rotation=90, fontfamily='sans-serif', fontsize=10)\n",
        "\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.xlabel(\"Source (Latin)\")\n",
        "    plt.ylabel(\"Prediction (Devanagari)\")\n",
        "    plt.title(title, fontsize=12)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    try:\n",
        "        plt.savefig(file_path)\n",
        "        print(f\"Saved attention heatmap to {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving heatmap: {e}\")\n",
        "    plt.close(fig)\n",
        "\n",
        "# --- Main Execution Block for Heatmap Generation ---\n",
        "if __name__ == '__main__':\n",
        "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "    # --- W&B Login ---\n",
        "    try:\n",
        "        if 'KAGGLE_KERNEL_RUN_TYPE' in os.environ:\n",
        "            print(\"Detected Kaggle environment. Ensuring WANDB_API_KEY is set.\")\n",
        "            if \"WANDB_API_KEY\" in os.environ: wandb.login()\n",
        "            else:\n",
        "                from kaggle_secrets import UserSecretsClient\n",
        "                user_secrets = UserSecretsClient()\n",
        "                wandb_api_key = user_secrets.get_secret(\"WANDB_API_KEY\")\n",
        "                wandb.login(key=wandb_api_key)\n",
        "            print(\"W&B login for Kaggle successful.\")\n",
        "        else:\n",
        "            wandb.login()\n",
        "        print(\"W&B login process attempted/completed.\")\n",
        "    except Exception as e:\n",
        "        print(f\"W&B login failed: {e}. Ensure API key is configured.\")\n",
        "        wandb = None # Disable wandb if login fails\n",
        "\n",
        "    # --- Best Attention Model Hyperparameters (UPDATE THIS SECTION) ---\n",
        "    # These hyperparameters should come from your W&B sweep for the Attention Model.\n",
        "    BEST_ATTN_HYPERPARAMETERS = {\n",
        "        'embedding_dim': 256,\n",
        "        'hidden_dim': 512,\n",
        "        'encoder_layers': 1,\n",
        "        'decoder_layers': 1,\n",
        "        'cell_type': 'LSTM',\n",
        "        'dropout_p': 0.3,\n",
        "        'encoder_bidirectional': True,\n",
        "        'learning_rate_train': 0.0008,\n",
        "        'batch_size_train': 64,\n",
        "        'epochs_train': 20,\n",
        "        'clip_value_train': 1.0,\n",
        "        'teacher_forcing_train': 0.5,\n",
        "        'max_epochs_no_improve_train': 7,\n",
        "        'lr_scheduler_patience_train': 3,\n",
        "        'vocab_min_freq': 1,\n",
        "        'max_seq_len': 50,\n",
        "        'eval_batch_size': 64,\n",
        "        # beam_width_eval is not directly used by predict_with_attention, which is greedy for this heatmap purpose\n",
        "    }\n",
        "    ATTN_MODEL_SAVE_PATH = \"/kaggle/working/best_attention_model_q5.pt\"\n",
        "    HEATMAP_OUTPUT_DIR = \"/kaggle/working/attention_heatmaps_q5d\"\n",
        "\n",
        "    print(f\"Target hyperparameters for best attention model: {BEST_ATTN_HYPERPARAMETERS}\")\n",
        "\n",
        "    # --- Phase 1: Train and Save the Best Attention Model (if checkpoint doesn't exist) ---\n",
        "    if not os.path.exists(ATTN_MODEL_SAVE_PATH):\n",
        "        print(f\"\\n--- Attention Model Checkpoint NOT FOUND at {ATTN_MODEL_SAVE_PATH} ---\")\n",
        "        print(\"--- Attempting to TRAIN AND SAVE the best attention model using BEST_ATTN_HYPERPARAMETERS ---\")\n",
        "        training_successful = train_and_save_attention_model(BEST_ATTN_HYPERPARAMETERS, ATTN_MODEL_SAVE_PATH, DEVICE)\n",
        "        if not training_successful or not os.path.exists(ATTN_MODEL_SAVE_PATH):\n",
        "            print(\"ERROR: Failed to train and save the best attention model. Exiting.\")\n",
        "            exit()\n",
        "        print(f\"Best attention model trained and saved to {ATTN_MODEL_SAVE_PATH}\")\n",
        "    else:\n",
        "        print(f\"Found existing attention model checkpoint at {ATTN_MODEL_SAVE_PATH}. Will use this for heatmaps.\")\n",
        "\n",
        "    # --- Phase 2: Load Model and Prepare for Heatmap Generation ---\n",
        "    print(\"\\n--- Preparing for Heatmap Generation ---\")\n",
        "\n",
        "    BASE_DATA_DIR = \"/kaggle/input/dakshina-dl-a3/dakshina_dataset_v1.0/hi/\"\n",
        "    DATA_DIR = os.path.join(BASE_DATA_DIR, \"lexicons/\")\n",
        "    train_file_for_vocab = os.path.join(DATA_DIR, \"hi.translit.sampled.train.tsv\")\n",
        "    test_file = os.path.join(DATA_DIR, \"hi.translit.sampled.test.tsv\")\n",
        "\n",
        "    source_vocab = Vocabulary(\"latin_attn_heatmap\")\n",
        "    target_vocab = Vocabulary(\"devanagari_attn_heatmap\")\n",
        "\n",
        "    temp_train_ds_attn_vocab = TransliterationDataset(train_file_for_vocab, source_vocab, target_vocab,\n",
        "                                                max_len=BEST_ATTN_HYPERPARAMETERS['max_seq_len'])\n",
        "    if not temp_train_ds_attn_vocab.pairs: exit()\n",
        "    for src, tgt in temp_train_ds_attn_vocab.pairs:\n",
        "        source_vocab.add_sequence(src)\n",
        "        target_vocab.add_sequence(tgt)\n",
        "    source_vocab.build_vocab(min_freq=BEST_ATTN_HYPERPARAMETERS['vocab_min_freq'])\n",
        "    target_vocab.build_vocab(min_freq=BEST_ATTN_HYPERPARAMETERS['vocab_min_freq'])\n",
        "\n",
        "    test_dataset_for_heatmaps = TransliterationDataset(test_file, source_vocab, target_vocab,\n",
        "                                                  max_len=BEST_ATTN_HYPERPARAMETERS['max_seq_len'])\n",
        "    if not test_dataset_for_heatmaps.pairs:\n",
        "        print(\"Test dataset for heatmaps is empty. Exiting.\")\n",
        "        exit()\n",
        "\n",
        "    encoder_hidden_dim_eff_test = BEST_ATTN_HYPERPARAMETERS['hidden_dim'] * (2 if BEST_ATTN_HYPERPARAMETERS['encoder_bidirectional'] else 1)\n",
        "    encoder = Encoder(source_vocab.n_chars, BEST_ATTN_HYPERPARAMETERS['embedding_dim'], BEST_ATTN_HYPERPARAMETERS['hidden_dim'],\n",
        "                      BEST_ATTN_HYPERPARAMETERS['encoder_layers'], BEST_ATTN_HYPERPARAMETERS['cell_type'], BEST_ATTN_HYPERPARAMETERS['dropout_p'],\n",
        "                      BEST_ATTN_HYPERPARAMETERS['encoder_bidirectional'], pad_idx=source_vocab.pad_idx).to(DEVICE)\n",
        "    decoder = DecoderWithAttention(target_vocab.n_chars, BEST_ATTN_HYPERPARAMETERS['embedding_dim'],\n",
        "                                   encoder_hidden_dim_eff_test, BEST_ATTN_HYPERPARAMETERS['hidden_dim'],\n",
        "                                   BEST_ATTN_HYPERPARAMETERS['decoder_layers'], BEST_ATTN_HYPERPARAMETERS['cell_type'], BEST_ATTN_HYPERPARAMETERS['dropout_p'],\n",
        "                                   pad_idx=target_vocab.pad_idx).to(DEVICE)\n",
        "    model = Seq2SeqWithAttention(encoder, decoder, DEVICE, target_vocab.sos_idx).to(DEVICE)\n",
        "\n",
        "    print(f\"Loading weights into attention model from: {ATTN_MODEL_SAVE_PATH}\")\n",
        "    model.load_state_dict(torch.load(ATTN_MODEL_SAVE_PATH, map_location=DEVICE))\n",
        "    print(\"Attention model weights loaded successfully.\")\n",
        "    model.eval() # Ensure model is in evaluation mode\n",
        "\n",
        "    # --- Generate and Plot Attention Heatmaps ---\n",
        "    if not os.path.exists(HEATMAP_OUTPUT_DIR):\n",
        "        os.makedirs(HEATMAP_OUTPUT_DIR)\n",
        "\n",
        "    num_heatmap_samples = 10\n",
        "    actual_num_samples_heatmap = min(num_heatmap_samples, len(test_dataset_for_heatmaps.pairs))\n",
        "    sample_indices_heatmap = []\n",
        "    if len(test_dataset_for_heatmaps.pairs) > 0:\n",
        "        sample_indices_heatmap = random.sample(range(len(test_dataset_for_heatmaps.pairs)), actual_num_samples_heatmap)\n",
        "\n",
        "    generated_heatmap_files = []\n",
        "\n",
        "    print(f\"\\n--- Generating {len(sample_indices_heatmap)} Attention Heatmaps ---\")\n",
        "    for i, data_idx in enumerate(sample_indices_heatmap):\n",
        "        source_str, true_target_str = test_dataset_for_heatmaps.pairs[data_idx]\n",
        "\n",
        "        # Prepare input for predict_with_attention\n",
        "        source_indices_for_pred = source_vocab.sequence_to_indices(source_str, add_eos=True, max_len=BEST_ATTN_HYPERPARAMETERS['max_seq_len'])\n",
        "        source_tensor_hm = torch.tensor(source_indices_for_pred, dtype=torch.long).unsqueeze(0).to(DEVICE)\n",
        "        source_length_hm = torch.tensor([len(source_indices_for_pred)], dtype=torch.long).to(DEVICE)\n",
        "\n",
        "        predicted_indices_hm, attention_matrix = model.predict_with_attention(\n",
        "            source_tensor_hm,\n",
        "            source_length_hm,\n",
        "            max_output_len=len(true_target_str) + 10,\n",
        "            target_eos_idx=target_vocab.eos_idx\n",
        "        )\n",
        "\n",
        "        # Convert indices to characters for plotting\n",
        "        source_chars_for_plot = list(source_str) # Use original source string characters\n",
        "        predicted_chars_for_plot = list(target_vocab.indices_to_sequence(predicted_indices_hm, strip_special=True))\n",
        "\n",
        "        if attention_matrix is not None and len(predicted_chars_for_plot) > 0 and len(source_chars_for_plot) > 0:\n",
        "            # Slice attention_matrix to match the lengths of displayed characters\n",
        "            valid_pred_len = min(len(predicted_chars_for_plot), attention_matrix.shape[0])\n",
        "            valid_src_len = min(len(source_chars_for_plot), attention_matrix.shape[1])\n",
        "\n",
        "            if valid_pred_len > 0 and valid_src_len > 0:\n",
        "                plot_attn_matrix = attention_matrix[:valid_pred_len, :valid_src_len]\n",
        "\n",
        "                heatmap_file_path = os.path.join(HEATMAP_OUTPUT_DIR, f\"attn_heatmap_{i+1}_{source_str[:15].replace(' ','_').replace('/','')}.png\")\n",
        "                plot_attention_heatmap(source_chars_for_plot[:valid_src_len],\n",
        "                                       predicted_chars_for_plot[:valid_pred_len],\n",
        "                                       plot_attn_matrix,\n",
        "                                       heatmap_file_path,\n",
        "                                       title=f\"Input: {source_str} -> Pred: {''.join(predicted_chars_for_plot)}\")\n",
        "                generated_heatmap_files.append(heatmap_file_path)\n",
        "            else:\n",
        "                print(f\"Warning: Not enough data in attention matrix or char lists for '{source_str}'. Skipping heatmap.\")\n",
        "        else:\n",
        "            print(f\"Warning: Could not generate attention matrix or empty prediction/source for '{source_str}'. Skipping heatmap.\")\n",
        "\n",
        "    print(f\"\\n--- Markdown for Displaying Attention Heatmaps (Saved in '{HEATMAP_OUTPUT_DIR}') ---\")\n",
        "    if generated_heatmap_files:\n",
        "        print(\"You can use the following Markdown in your report (adjust paths if needed):\")\n",
        "        md_grid_rows = []\n",
        "        num_full_rows = len(generated_heatmap_files) // 3\n",
        "        for r_idx in range(num_full_rows):\n",
        "            md_row_headers_str = \" | \".join([f\"Heatmap {r_idx*3+j+1}\" for j in range(3)])\n",
        "            md_row_images_str = \" | \".join([f\"![Attention {r_idx*3+j+1}]({os.path.relpath(generated_heatmap_files[r_idx*3+j], '/kaggle/working/')})\" for j in range(3)])\n",
        "            if r_idx == 0:\n",
        "                md_grid_rows.append(f\"| {md_row_headers_str} |\")\n",
        "                md_grid_rows.append(f\"|---|---|---|\")\n",
        "            md_grid_rows.append(f\"| {md_row_images_str} |\")\n",
        "\n",
        "        remaining_idx_start = num_full_rows * 3\n",
        "        if remaining_idx_start < len(generated_heatmap_files):\n",
        "            md_row_headers_list = []\n",
        "            md_row_images_list = []\n",
        "            for j, file_idx in enumerate(range(remaining_idx_start, len(generated_heatmap_files))):\n",
        "                md_row_headers_list.append(f\"Heatmap {file_idx+1}\")\n",
        "                md_row_images_list.append(f\"![Attention {file_idx+1}]({os.path.relpath(generated_heatmap_files[file_idx], '/kaggle/working/')})\")\n",
        "\n",
        "            # Pad to 3 columns if necessary\n",
        "            while len(md_row_headers_list) < 3: md_row_headers_list.append(\" \")\n",
        "            while len(md_row_images_list) < 3: md_row_images_list.append(\" \")\n",
        "\n",
        "            if num_full_rows == 0:\n",
        "                md_grid_rows.append(f\"| {md_row_headers_list[0]} | {md_row_headers_list[1]} | {md_row_headers_list[2]} |\")\n",
        "                md_grid_rows.append(f\"|---|---|---|\")\n",
        "            md_grid_rows.append(f\"| {md_row_images_list[0]} | {md_row_images_list[1]} | {md_row_images_list[2]} |\")\n",
        "\n",
        "        print(\"\\n\".join(md_grid_rows))\n",
        "        print(\"\\n(Note: Paths are relative to '/kaggle/working/'. Adjust if your report is viewed elsewhere.)\")\n",
        "    else:\n",
        "        print(\"No heatmaps were generated successfully.\")\n",
        "\n",
        "    # --- Log Heatmaps to W&B ---\n",
        "    if wandb and wandb.run is None:\n",
        "        wandb.init(project=\"DL_A3\", name=f\"Q5d_Attention_Heatmaps\", config=BEST_ATTN_HYPERPARAMETERS, job_type=\"q5d_heatmap_generation\", reinit=True)\n",
        "\n",
        "    if wandb and wandb.run and generated_heatmap_files:\n",
        "        for i, f_path in enumerate(generated_heatmap_files):\n",
        "            if os.path.exists(f_path):\n",
        "                wandb.log({f\"q5d_attention_heatmap_sample_{i+1}\": wandb.Image(f_path, caption=f\"Heatmap for sample {i+1}\")})\n",
        "        print(\"Attention heatmaps logged to W&B.\")\n",
        "        if wandb.run: wandb.finish()\n",
        "\n",
        "    print(\"Script finished.\")"
      ],
      "metadata": {
        "id": "oJKI50cVT3i0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q5 d heat map"
      ],
      "metadata": {
        "id": "hzDpi3BCT32h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import numpy as np\n",
        "import wandb\n",
        "import os\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "import heapq\n",
        "import matplotlib\n",
        "matplotlib.use('Agg') # Use a non-interactive backend for Kaggle, crucial for saving plots\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "# --- Constants for Special Tokens ---\n",
        "SOS_TOKEN = \"<sos>\"  # Start-of-sequence token\n",
        "EOS_TOKEN = \"<eos>\"  # End-of-sequence token\n",
        "PAD_TOKEN = \"<pad>\"  # Padding token\n",
        "UNK_TOKEN = \"<unk>\"  # Unknown token\n",
        "\n",
        "# --- Attention Mechanism ---\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements a general (Luong-style) attention mechanism to compute alignment scores\n",
        "    between the decoder's current hidden state and all encoder's output states.\n",
        "    \"\"\"\n",
        "    def __init__(self, encoder_hidden_dim_eff, decoder_hidden_dim):\n",
        "        super(Attention, self).__init__()\n",
        "        self.attn_W = nn.Linear(encoder_hidden_dim_eff + decoder_hidden_dim, decoder_hidden_dim)\n",
        "        self.attn_v = nn.Linear(decoder_hidden_dim, 1, bias=False)\n",
        "\n",
        "    def forward(self, decoder_hidden_top_layer, encoder_outputs):\n",
        "        \"\"\"\n",
        "        Calculates attention weights.\n",
        "\n",
        "        Args:\n",
        "            decoder_hidden_top_layer (torch.Tensor): The top layer's hidden state of the decoder RNN (batch_size, dec_hidden_dim).\n",
        "            encoder_outputs (torch.Tensor): All hidden states from the encoder (batch_size, src_len, enc_hidden_dim).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Attention weights (batch_size, src_len) representing alignment probabilities.\n",
        "        \"\"\"\n",
        "        src_len = encoder_outputs.size(1)\n",
        "        repeated_decoder_hidden = decoder_hidden_top_layer.unsqueeze(1).repeat(1, src_len, 1)\n",
        "\n",
        "        energy_input = torch.cat((repeated_decoder_hidden, encoder_outputs), dim=2)\n",
        "        energy = torch.tanh(self.attn_W(energy_input))\n",
        "\n",
        "        attention_scores = self.attn_v(energy).squeeze(2)\n",
        "        return F.softmax(attention_scores, dim=1)\n",
        "\n",
        "# --- Model Definition (Encoder, DecoderWithAttention, Seq2SeqWithAttention) ---\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    The Encoder processes the input sequence, producing a context vector (final hidden state)\n",
        "    and a sequence of output states for the attention mechanism.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_vocab_size, embedding_dim, hidden_dim, n_layers,\n",
        "                 cell_type='LSTM', dropout_p=0.1, bidirectional=False, pad_idx=0):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.input_vocab_size = input_vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.cell_type = cell_type.upper()\n",
        "        self.bidirectional = bidirectional\n",
        "        self.num_directions = 2 if bidirectional else 1\n",
        "\n",
        "        self.embedding = nn.Embedding(input_vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "        rnn_dropout = dropout_p if n_layers > 1 else 0\n",
        "        if self.cell_type == 'RNN':\n",
        "            self.rnn = nn.RNN(embedding_dim, hidden_dim, n_layers,\n",
        "                              dropout=rnn_dropout, batch_first=True, bidirectional=self.bidirectional)\n",
        "        elif self.cell_type == 'GRU':\n",
        "            self.rnn = nn.GRU(embedding_dim, hidden_dim, n_layers,\n",
        "                              dropout=rnn_dropout, batch_first=True, bidirectional=self.bidirectional)\n",
        "        elif self.cell_type == 'LSTM':\n",
        "            self.rnn = nn.LSTM(embedding_dim, hidden_dim, n_layers,\n",
        "                               dropout=rnn_dropout, batch_first=True, bidirectional=self.bidirectional)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported cell type. Choose from 'RNN', 'GRU', 'LSTM'.\")\n",
        "\n",
        "    def forward(self, input_seq, input_lengths):\n",
        "        \"\"\"\n",
        "        Forward pass for the encoder.\n",
        "\n",
        "        Args:\n",
        "            input_seq (torch.Tensor): Padded input sequences (batch_size, seq_len).\n",
        "            input_lengths (torch.Tensor): Lengths of the original sequences (batch_size,).\n",
        "\n",
        "        Returns:\n",
        "            tuple: (`encoder_outputs`, `hidden_state`).\n",
        "                   `encoder_outputs`: All hidden states from the last layer (batch_size, src_len, hidden_dim * num_directions).\n",
        "                   `hidden_state`: The final hidden state (and cell state for LSTM) from all layers.\n",
        "        \"\"\"\n",
        "        embedded = self.embedding(input_seq)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "        packed_outputs, hidden = self.rnn(packed_embedded)\n",
        "        encoder_outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs, batch_first=True)\n",
        "        return encoder_outputs, hidden\n",
        "\n",
        "class DecoderWithAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    The Decoder generates the output sequence one token at a time,\n",
        "    incorporating an attention mechanism over the encoder's output states.\n",
        "    \"\"\"\n",
        "    def __init__(self, output_vocab_size, embedding_dim, encoder_hidden_dim_eff,\n",
        "                 decoder_hidden_dim, n_layers, cell_type='LSTM', dropout_p=0.1, pad_idx=0):\n",
        "        super(DecoderWithAttention, self).__init__()\n",
        "        self.output_vocab_size = output_vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.encoder_hidden_dim_eff = encoder_hidden_dim_eff\n",
        "        self.decoder_hidden_dim = decoder_hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.cell_type = cell_type.upper()\n",
        "\n",
        "        self.attention = Attention(encoder_hidden_dim_eff, decoder_hidden_dim)\n",
        "        self.embedding = nn.Embedding(output_vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "        # RNN input is concatenation of token embedding and context vector\n",
        "        rnn_input_dim = embedding_dim + encoder_hidden_dim_eff\n",
        "\n",
        "        rnn_dropout = dropout_p if n_layers > 1 else 0\n",
        "        if self.cell_type == 'RNN':\n",
        "            self.rnn = nn.RNN(rnn_input_dim, decoder_hidden_dim, n_layers,\n",
        "                              dropout=rnn_dropout, batch_first=True)\n",
        "        elif self.cell_type == 'GRU':\n",
        "            self.rnn = nn.GRU(rnn_input_dim, decoder_hidden_dim, n_layers,\n",
        "                              dropout=rnn_dropout, batch_first=True)\n",
        "        elif self.cell_type == 'LSTM':\n",
        "            self.rnn = nn.LSTM(rnn_input_dim, decoder_hidden_dim, n_layers,\n",
        "                               dropout=rnn_dropout, batch_first=True)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported cell type. Choose from 'RNN', 'GRU', 'LSTM'.\")\n",
        "\n",
        "        self.fc_out = nn.Linear(decoder_hidden_dim, output_vocab_size)\n",
        "\n",
        "    def forward(self, input_char, prev_decoder_hidden, encoder_outputs):\n",
        "        \"\"\"\n",
        "        Forward pass for the attention-based decoder.\n",
        "\n",
        "        Args:\n",
        "            input_char (torch.Tensor): Current target token (batch_size,).\n",
        "            prev_decoder_hidden (torch.Tensor or tuple): Previous hidden state(s) of the decoder RNN.\n",
        "            encoder_outputs (torch.Tensor): All hidden states from the encoder (batch_size, src_len, enc_hidden_dim_eff).\n",
        "\n",
        "        Returns:\n",
        "            tuple: (`prediction_logits`, `current_decoder_hidden`, `attention_weights`).\n",
        "                   `prediction_logits`: Raw scores for each token in the vocabulary.\n",
        "                   `current_decoder_hidden`: Updated hidden state of the decoder RNN.\n",
        "                   `attention_weights`: Attention probabilities (batch_size, src_len).\n",
        "        \"\"\"\n",
        "        input_char = input_char.unsqueeze(1)\n",
        "        embedded = self.embedding(input_char)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        # Get the hidden state for attention query (top layer's hidden state)\n",
        "        if self.cell_type == 'LSTM':\n",
        "            attention_query_hidden = prev_decoder_hidden[0][-1, :, :]\n",
        "        else:\n",
        "            attention_query_hidden = prev_decoder_hidden[-1, :, :]\n",
        "\n",
        "        # Calculate attention weights and context vector\n",
        "        attention_weights = self.attention(attention_query_hidden, encoder_outputs)\n",
        "        context_vector = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)\n",
        "\n",
        "        # Concatenate embedded input and context vector to form RNN input\n",
        "        rnn_input = torch.cat((embedded, context_vector), dim=2)\n",
        "\n",
        "        # Pass through decoder RNN\n",
        "        rnn_output, current_decoder_hidden = self.rnn(rnn_input, prev_decoder_hidden)\n",
        "\n",
        "        # Project RNN output to vocabulary size\n",
        "        rnn_output_squeezed = rnn_output.squeeze(1)\n",
        "        prediction_logits = self.fc_out(rnn_output_squeezed)\n",
        "\n",
        "        return prediction_logits, current_decoder_hidden, attention_weights\n",
        "\n",
        "\n",
        "class Seq2SeqWithAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    The main Sequence-to-Sequence model integrating the Encoder and Attention-based Decoder.\n",
        "    \"\"\"\n",
        "    def __init__(self, encoder, decoder, device, target_sos_idx):\n",
        "        super(Seq2SeqWithAttention, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "        self.target_sos_idx = target_sos_idx\n",
        "\n",
        "        # Layers to adapt encoder's final hidden state to decoder's initial hidden state\n",
        "        encoder_effective_final_hidden_dim = self.encoder.hidden_dim * self.encoder.num_directions\n",
        "        decoder_rnn_hidden_dim = self.decoder.decoder_hidden_dim\n",
        "\n",
        "        self.fc_adapt_hidden = None\n",
        "        self.fc_adapt_cell = None\n",
        "        if encoder_effective_final_hidden_dim != decoder_rnn_hidden_dim:\n",
        "            self.fc_adapt_hidden = nn.Linear(encoder_effective_final_hidden_dim, decoder_rnn_hidden_dim)\n",
        "            if self.encoder.cell_type == 'LSTM':\n",
        "                self.fc_adapt_cell = nn.Linear(encoder_effective_final_hidden_dim, decoder_rnn_hidden_dim)\n",
        "\n",
        "    def _adapt_encoder_hidden_for_decoder(self, encoder_final_hidden_state):\n",
        "        \"\"\"\n",
        "        Adapts the encoder's final hidden state(s) to match the decoder's expected dimensions and layer count.\n",
        "        Handles bidirectionality and differing layer counts.\n",
        "        \"\"\"\n",
        "        is_lstm = self.encoder.cell_type == 'LSTM'\n",
        "        if is_lstm:\n",
        "            h_from_enc, c_from_enc = encoder_final_hidden_state\n",
        "        else:\n",
        "            h_from_enc = encoder_final_hidden_state\n",
        "            c_from_enc = None\n",
        "\n",
        "        batch_size = h_from_enc.size(1)\n",
        "\n",
        "        # Reshape and concatenate bidirectional states if applicable\n",
        "        h_processed = h_from_enc.view(self.encoder.n_layers, self.encoder.num_directions,\n",
        "                                      batch_size, self.encoder.hidden_dim)\n",
        "        if self.encoder.bidirectional:\n",
        "            h_processed = torch.cat([h_processed[:, 0, :, :], h_processed[:, 1, :, :]], dim=2)\n",
        "        else:\n",
        "            h_processed = h_processed.squeeze(1)\n",
        "\n",
        "        c_processed = None\n",
        "        if is_lstm and c_from_enc is not None:\n",
        "            c_processed = c_from_enc.view(self.encoder.n_layers, self.encoder.num_directions,\n",
        "                                          batch_size, self.encoder.hidden_dim)\n",
        "            if self.encoder.bidirectional:\n",
        "                c_processed = torch.cat([c_processed[:, 0, :, :], c_processed[:, 1, :, :]], dim=2)\n",
        "            else:\n",
        "                c_processed = c_processed.squeeze(1)\n",
        "\n",
        "        # Apply linear transformation for dimension adaptation if needed\n",
        "        if self.fc_adapt_hidden:\n",
        "            h_processed = self.fc_adapt_hidden(h_processed)\n",
        "            if is_lstm and c_processed is not None and self.fc_adapt_cell:\n",
        "                c_processed = self.fc_adapt_cell(c_processed)\n",
        "\n",
        "        # Adapt number of layers for the decoder's RNN\n",
        "        final_h = torch.zeros(self.decoder.n_layers, batch_size, self.decoder.decoder_hidden_dim, device=self.device)\n",
        "        final_c = torch.zeros(self.decoder.n_layers, batch_size, self.decoder.decoder_hidden_dim, device=self.device) if is_lstm else None\n",
        "\n",
        "        num_layers_to_copy = min(self.encoder.n_layers, self.decoder.n_layers)\n",
        "\n",
        "        final_h[:num_layers_to_copy, :, :] = h_processed[:num_layers_to_copy, :, :]\n",
        "        if is_lstm and c_processed is not None:\n",
        "            final_c[:num_layers_to_copy, :, :] = c_processed[:num_layers_to_copy, :, :]\n",
        "\n",
        "        # If decoder has more layers than encoder, repeat the last encoder layer's state\n",
        "        if self.decoder.n_layers > self.encoder.n_layers and self.encoder.n_layers > 0:\n",
        "            last_h_layer_to_repeat = h_processed[self.encoder.n_layers-1, :, :]\n",
        "            for i in range(self.encoder.n_layers, self.decoder.n_layers):\n",
        "                final_h[i, :, :] = last_h_layer_to_repeat\n",
        "                if is_lstm and c_processed is not None:\n",
        "                    last_c_layer_to_repeat = c_processed[self.encoder.n_layers-1, :, :]\n",
        "                    final_c[i, :, :] = last_c_layer_to_repeat\n",
        "\n",
        "        return (final_h, final_c) if is_lstm else final_h\n",
        "\n",
        "    def forward(self, source_seq, source_lengths, target_seq, teacher_forcing_ratio=0.5):\n",
        "        \"\"\"\n",
        "        Forward pass for the Seq2SeqWithAttention model during training.\n",
        "\n",
        "        Args:\n",
        "            source_seq (torch.Tensor): Padded source sequences.\n",
        "            source_lengths (torch.Tensor): Lengths of source sequences.\n",
        "            target_seq (torch.Tensor): Padded target sequences (including SOS token).\n",
        "            teacher_forcing_ratio (float): Probability of using actual target token as next input.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Logits for the predicted target sequence.\n",
        "        \"\"\"\n",
        "        batch_size = source_seq.shape[0]\n",
        "        target_len = target_seq.shape[1]\n",
        "        target_vocab_size = self.decoder.output_vocab_size\n",
        "        outputs_logits = torch.zeros(batch_size, target_len, target_vocab_size).to(self.device)\n",
        "\n",
        "        # Encode the source sequence to get all encoder outputs and final hidden state\n",
        "        encoder_outputs, encoder_final_hidden = self.encoder(source_seq, source_lengths)\n",
        "        # Adapt encoder's final hidden state to initialize the decoder's hidden state\n",
        "        decoder_hidden = self._adapt_encoder_hidden_for_decoder(encoder_final_hidden)\n",
        "        decoder_input = target_seq[:, 0]\n",
        "\n",
        "        # Iterate through the target sequence, predicting one token at a time\n",
        "        for t in range(target_len - 1):\n",
        "            decoder_output_logits, decoder_hidden, _ = \\\n",
        "                self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            outputs_logits[:, t+1] = decoder_output_logits\n",
        "\n",
        "            # Apply teacher forcing: use true target or predicted token for next input\n",
        "            teacher_force_this_step = random.random() < teacher_forcing_ratio\n",
        "            top1_predicted_token = decoder_output_logits.argmax(1)\n",
        "\n",
        "            decoder_input = target_seq[:, t+1] if teacher_force_this_step else top1_predicted_token\n",
        "        return outputs_logits\n",
        "\n",
        "    def predict_with_attention(self, source_seq, source_lengths, max_output_len=50, target_eos_idx=None):\n",
        "        \"\"\"\n",
        "        Generates a sequence using greedy decoding and records attention weights.\n",
        "\n",
        "        Args:\n",
        "            source_seq (torch.Tensor): Input source sequence (batch_size=1).\n",
        "            source_lengths (torch.Tensor): Length of the source sequence.\n",
        "            max_output_len (int): Maximum length of the sequence to generate.\n",
        "            target_eos_idx (int, optional): Index of the EOS token.\n",
        "\n",
        "        Returns:\n",
        "            tuple: Predicted token indices and attention matrices (numpy array).\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        if source_seq.dim() == 1:\n",
        "            source_seq = source_seq.unsqueeze(0)\n",
        "            source_lengths = torch.tensor([source_lengths if isinstance(source_lengths, int) else len(source_lengths)], device=self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            encoder_outputs, encoder_final_hidden = self.encoder(source_seq, source_lengths)\n",
        "            decoder_hidden = self._adapt_encoder_hidden_for_decoder(encoder_final_hidden)\n",
        "            decoder_input = torch.tensor([self.target_sos_idx], device=self.device)\n",
        "\n",
        "            predicted_indices = []\n",
        "            attention_matrices = []\n",
        "\n",
        "            for _ in range(max_output_len):\n",
        "                decoder_output_logits, decoder_hidden, attention_weights = \\\n",
        "                    self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "\n",
        "                attention_matrices.append(attention_weights.squeeze(0).cpu().numpy())\n",
        "\n",
        "                top1_predicted_token = decoder_output_logits.argmax(1)\n",
        "                predicted_idx = top1_predicted_token.item()\n",
        "\n",
        "                predicted_indices.append(predicted_idx)\n",
        "                if target_eos_idx is not None and predicted_idx == target_eos_idx:\n",
        "                    break\n",
        "\n",
        "                if len(predicted_indices) >= max_output_len: break\n",
        "                decoder_input = top1_predicted_token\n",
        "        return predicted_indices, np.array(attention_matrices) if attention_matrices else None\n",
        "\n",
        "# --- Data Loading and Preprocessing ---\n",
        "\n",
        "class Vocabulary:\n",
        "    \"\"\"\n",
        "    Manages the mapping between characters and their numerical indices.\n",
        "    Includes special tokens for padding, start-of-sequence, end-of-sequence, and unknown characters.\n",
        "    \"\"\"\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.char2index = {PAD_TOKEN: 0, SOS_TOKEN: 1, EOS_TOKEN: 2, UNK_TOKEN: 3}\n",
        "        self.index2char = {0: PAD_TOKEN, 1: SOS_TOKEN, 2: EOS_TOKEN, 3: UNK_TOKEN}\n",
        "        self.char_counts = Counter()\n",
        "        self.n_chars = 4\n",
        "        self.pad_idx = self.char2index[PAD_TOKEN]\n",
        "        self.sos_idx = self.char2index[SOS_TOKEN]\n",
        "        self.eos_idx = self.char2index[EOS_TOKEN]\n",
        "\n",
        "    def add_sequence(self, sequence):\n",
        "        \"\"\"Adds characters from a sequence to the character counts.\"\"\"\n",
        "        for char in list(sequence):\n",
        "            self.char_counts[char] += 1\n",
        "\n",
        "    def build_vocab(self, min_freq=1):\n",
        "        \"\"\"\n",
        "        Builds the vocabulary mapping based on character counts and a minimum frequency.\n",
        "        Characters appearing less than `min_freq` will be treated as UNK_TOKEN.\n",
        "        \"\"\"\n",
        "        sorted_chars = sorted(self.char_counts.keys(), key=lambda char: (-self.char_counts[char], char))\n",
        "        for char in sorted_chars:\n",
        "            if self.char_counts[char] >= min_freq and char not in self.char2index:\n",
        "                self.char2index[char] = self.n_chars\n",
        "                self.index2char[self.n_chars] = char\n",
        "                self.n_chars += 1\n",
        "\n",
        "    def sequence_to_indices(self, sequence, add_eos=False, add_sos=False, max_len=None):\n",
        "        \"\"\"Converts a character sequence into a list of numerical indices.\"\"\"\n",
        "        indices = []\n",
        "        if add_sos:\n",
        "            indices.append(self.sos_idx)\n",
        "\n",
        "        seq_to_process = list(sequence)\n",
        "        if max_len:\n",
        "            effective_max_len = max_len\n",
        "            if add_sos: effective_max_len -=1\n",
        "            if add_eos: effective_max_len -=1\n",
        "            seq_to_process = seq_to_process[:max(0, effective_max_len)] # Ensure non-negative slice\n",
        "\n",
        "        for char in seq_to_process:\n",
        "            indices.append(self.char2index.get(char, self.char2index[UNK_TOKEN]))\n",
        "\n",
        "        if add_eos: indices.append(self.eos_idx)\n",
        "        return indices\n",
        "\n",
        "    def indices_to_sequence(self, indices, strip_special=True):\n",
        "        \"\"\"Converts a list of numerical indices back into a character sequence.\"\"\"\n",
        "        chars = []\n",
        "        for index_val in indices:\n",
        "            if strip_special and index_val == self.eos_idx: break\n",
        "            if strip_special and index_val in [self.sos_idx, self.pad_idx]: continue\n",
        "            chars.append(self.index2char.get(index_val, UNK_TOKEN))\n",
        "        return \"\".join(chars)\n",
        "\n",
        "class TransliterationDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A PyTorch Dataset for loading and preparing transliteration pairs.\n",
        "    Reads data from a TSV file and converts text sequences to token indices.\n",
        "    \"\"\"\n",
        "    def __init__(self, file_path, source_vocab, target_vocab, max_len=50):\n",
        "        self.pairs = []\n",
        "        self.source_vocab = source_vocab\n",
        "        self.target_vocab = target_vocab\n",
        "        self.max_len = max_len\n",
        "\n",
        "        if not os.path.exists(file_path):\n",
        "            print(f\"ERROR: Data file not found: {file_path}\")\n",
        "            return\n",
        "\n",
        "        print(f\"Loading data from: {file_path}\")\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                for line_num, line in enumerate(f):\n",
        "                    parts = line.strip().split('\\t')\n",
        "                    if len(parts) >= 2:\n",
        "                        target, source = parts[0], parts[1]\n",
        "                        if not source or not target or \\\n",
        "                           (self.max_len and (len(source) > self.max_len or len(target) > self.max_len)):\n",
        "                            continue\n",
        "                        self.pairs.append((source, target))\n",
        "            print(f\"Loaded {len(self.pairs)} pairs from {file_path}.\")\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR: Could not read or process file {file_path}. Error: {e}\")\n",
        "\n",
        "    def __len__(self): return len(self.pairs)\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Returns a source-target pair as Tensors of indices.\"\"\"\n",
        "        source_str, target_str = self.pairs[idx]\n",
        "        source_indices = self.source_vocab.sequence_to_indices(source_str, add_eos=True, max_len=self.max_len)\n",
        "        target_indices = self.target_vocab.sequence_to_indices(target_str, add_sos=True, add_eos=True, max_len=self.max_len)\n",
        "        return torch.tensor(source_indices, dtype=torch.long), torch.tensor(target_indices, dtype=torch.long)\n",
        "\n",
        "def collate_fn(batch, pad_idx_source, pad_idx_target):\n",
        "    \"\"\"\n",
        "    Custom collate function for DataLoader to handle variable-length sequences.\n",
        "    Pads sequences within a batch to the maximum length of that batch.\n",
        "    \"\"\"\n",
        "    batch = [item for item in batch if item is not None and len(item[0]) > 0 and len(item[1]) > 0]\n",
        "    if not batch: return None, None, None\n",
        "    source_seqs, target_seqs = zip(*batch)\n",
        "\n",
        "    source_lengths = torch.tensor([len(s) for s in source_seqs], dtype=torch.long)\n",
        "    padded_sources = pad_sequence(source_seqs, batch_first=True, padding_value=pad_idx_source)\n",
        "    padded_targets = pad_sequence(target_seqs, batch_first=True, padding_value=pad_idx_target)\n",
        "    return padded_sources, source_lengths, padded_targets\n",
        "\n",
        "# --- Training and Evaluation Functions ---\n",
        "\n",
        "def _train_one_epoch(model, dataloader, optimizer, criterion, device, clip_value, teacher_forcing_ratio, target_vocab):\n",
        "    \"\"\"\n",
        "    Trains the model for a single epoch.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The Seq2Seq model.\n",
        "        dataloader (DataLoader): DataLoader for the training set.\n",
        "        optimizer (optim.Optimizer): Optimizer for model parameters.\n",
        "        criterion (nn.Module): Loss function (e.g., CrossEntropyLoss).\n",
        "        device (torch.device): Device to run the model on.\n",
        "        clip_value (float): Gradient clipping value.\n",
        "        teacher_forcing_ratio (float): Probability of using teacher forcing.\n",
        "        target_vocab (Vocabulary): Vocabulary for the target language.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Average epoch loss and training accuracy (0.0 for this simplified version).\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    if len(dataloader) == 0: return 0.0, 0.0\n",
        "\n",
        "    for batch_data in tqdm(dataloader, desc=\"Training\", leave=False):\n",
        "        if batch_data[0] is None: continue\n",
        "        sources, source_lengths, targets = batch_data\n",
        "        if sources is None or sources.shape[0] == 0: continue\n",
        "\n",
        "        sources, targets, source_lengths = sources.to(device), targets.to(device), source_lengths.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs_logits = model(sources, source_lengths, targets, teacher_forcing_ratio)\n",
        "\n",
        "        output_dim = outputs_logits.shape[-1]\n",
        "        flat_outputs = outputs_logits[:, 1:].reshape(-1, output_dim)\n",
        "        flat_targets = targets[:, 1:].reshape(-1)\n",
        "\n",
        "        loss = criterion(flat_outputs, flat_targets)\n",
        "        if not (torch.isnan(loss) or torch.isinf(loss)):\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "    # For simplicity in this heatmap script, training accuracy is not computed here\n",
        "    return epoch_loss / len(dataloader) if len(dataloader) > 0 else 0.0, 0.0\n",
        "\n",
        "\n",
        "def _evaluate_one_epoch(model, dataloader, criterion, device, source_vocab, target_vocab, beam_width=1, is_test_set=False):\n",
        "    \"\"\"\n",
        "    Evaluates the model's performance on a given dataset (validation or test).\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The Seq2Seq model.\n",
        "        dataloader (DataLoader): DataLoader for the evaluation set.\n",
        "        criterion (nn.Module): Loss function.\n",
        "        device (torch.device): Device to run the model on.\n",
        "        source_vocab (Vocabulary): Vocabulary for the source language (used for debug prints).\n",
        "        target_vocab (Vocabulary): Vocabulary for the target language.\n",
        "        beam_width (int): Beam width for decoding (1 for greedy, >1 for beam search).\n",
        "        is_test_set (bool): Flag to indicate if this is the final test evaluation (for debug prints).\n",
        "\n",
        "    Returns:\n",
        "        tuple: Average epoch loss and evaluation accuracy.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "    if len(dataloader) == 0: return 0.0, 0.0\n",
        "    desc_prefix = \"Testing\" if is_test_set else \"Validating\"\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_data in tqdm(dataloader, desc=desc_prefix, leave=False):\n",
        "            if batch_data[0] is None: continue\n",
        "            sources, source_lengths, targets = batch_data\n",
        "            if sources is None or sources.shape[0] == 0: continue\n",
        "\n",
        "            sources, targets, source_lengths = sources.to(device), targets.to(device), source_lengths.to(device)\n",
        "            outputs = model(sources, source_lengths, targets, teacher_forcing_ratio=0.0)\n",
        "            output_dim = outputs.shape[-1]\n",
        "            flat_outputs = outputs[:, 1:].reshape(-1, output_dim)\n",
        "            flat_targets = targets[:, 1:].reshape(-1)\n",
        "            loss = criterion(flat_outputs, flat_targets)\n",
        "            epoch_loss += loss.item() if not (torch.isnan(loss) or torch.isinf(loss)) else 0\n",
        "\n",
        "            # For accuracy, we need to generate predictions (greedy or beam search)\n",
        "            for i in range(targets.shape[0]):\n",
        "                src_single, src_len_single = sources[i:i+1], source_lengths[i:i+1]\n",
        "\n",
        "                # Use greedy for attention visualization\n",
        "                if hasattr(model, 'predict_with_attention') and beam_width == 1:\n",
        "                    predicted_indices, _ = model.predict_with_attention(\n",
        "                        src_single, src_len_single,\n",
        "                        max_output_len=targets.size(1) + 5,\n",
        "                        target_eos_idx=target_vocab.eos_idx\n",
        "                    )\n",
        "                # Use beam search if specified and available\n",
        "                elif hasattr(model, 'predict_beam_search') and beam_width > 1:\n",
        "                    predicted_indices = model.predict_beam_search(\n",
        "                        src_single, src_len_single,\n",
        "                        max_output_len=targets.size(1) + 5,\n",
        "                        target_eos_idx=target_vocab.eos_idx,\n",
        "                        beam_width=beam_width\n",
        "                    )\n",
        "                else: # Fallback to argmax from teacher-forced outputs if no specific prediction method\n",
        "                    predicted_indices = outputs[i:i+1].argmax(dim=2)[0, 1:].tolist()\n",
        "\n",
        "                pred_str = target_vocab.indices_to_sequence(predicted_indices, strip_special=True)\n",
        "                true_str = target_vocab.indices_to_sequence(targets[i, 1:].tolist(), strip_special=True)\n",
        "\n",
        "                if pred_str == true_str: total_correct += 1\n",
        "                total_samples += 1\n",
        "\n",
        "                if is_test_set and batch_idx == 0 and i < 3:\n",
        "                    print(f\"  Test Example {i} - Source: '{source_vocab.indices_to_sequence(src_single[0].tolist(), strip_special=True)}'\")\n",
        "                    print(f\"    Pred: '{pred_str}', True: '{true_str}', Match: {pred_str == true_str}\")\n",
        "\n",
        "    accuracy = total_correct / total_samples if total_samples > 0 else 0.0\n",
        "    return epoch_loss / len(dataloader) if len(dataloader) > 0 else 0.0, accuracy\n",
        "\n",
        "# --- Function to Train and Save the Best Attention Model ---\n",
        "\n",
        "def train_and_save_attention_model(config_params, model_save_path, device):\n",
        "    \"\"\"\n",
        "    Performs a dedicated training run using the best hyperparameters for the attention model.\n",
        "    Saves the model checkpoint with the best validation accuracy.\n",
        "\n",
        "    Args:\n",
        "        config_params (dict): Dictionary of hyperparameters for this specific training run.\n",
        "        model_save_path (str): Path to save the best model's state_dict.\n",
        "        device (torch.device): Device to run the training on.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if training was successful and a model was saved, False otherwise.\n",
        "    \"\"\"\n",
        "    run_name_train_best_attn = f\"TRAIN_BEST_ATTN_{config_params['cell_type']}_emb{config_params['embedding_dim']}_hid{config_params['hidden_dim']}\"\n",
        "\n",
        "    with wandb.init(project=\"DL_A3_Attention_Training\", name=run_name_train_best_attn, config=config_params, job_type=\"training_best_attention_model\", reinit=True) as run:\n",
        "        cfg = wandb.config\n",
        "        print(f\"Starting dedicated training for BEST ATTENTION model with config: {cfg}\")\n",
        "\n",
        "        # --- Data Loading and Vocabulary Building ---\n",
        "        BASE_DATA_DIR = \"/kaggle/input/dakshina-dl-a3/dakshina_dataset_v1.0/hi/\"\n",
        "        DATA_DIR = os.path.join(BASE_DATA_DIR, \"lexicons/\")\n",
        "        train_file = os.path.join(DATA_DIR, \"hi.translit.sampled.train.tsv\")\n",
        "        dev_file = os.path.join(DATA_DIR, \"hi.translit.sampled.dev.tsv\")\n",
        "\n",
        "        source_vocab = Vocabulary(\"latin\")\n",
        "        target_vocab = Vocabulary(\"devanagari\")\n",
        "        temp_train_ds_vocab = TransliterationDataset(train_file, source_vocab, target_vocab, max_len=cfg.max_seq_len)\n",
        "        if not temp_train_ds_vocab.pairs:\n",
        "            print(f\"ERROR: No data for vocab from {train_file}\")\n",
        "            return False\n",
        "        for src, tgt in temp_train_ds_vocab.pairs:\n",
        "            source_vocab.add_sequence(src)\n",
        "            target_vocab.add_sequence(tgt)\n",
        "        source_vocab.build_vocab(min_freq=cfg.vocab_min_freq)\n",
        "        target_vocab.build_vocab(min_freq=cfg.vocab_min_freq)\n",
        "\n",
        "        train_dataset = TransliterationDataset(train_file, source_vocab, target_vocab, max_len=cfg.max_seq_len)\n",
        "        dev_dataset = TransliterationDataset(dev_file, source_vocab, target_vocab, max_len=cfg.max_seq_len)\n",
        "        if not train_dataset.pairs or not dev_dataset.pairs:\n",
        "            print(f\"ERROR: Train or Dev dataset empty. Train: {len(train_dataset)}, Dev: {len(dev_dataset)}\")\n",
        "            return False\n",
        "\n",
        "        # --- DataLoader Setup ---\n",
        "        num_w = 0 if device.type == 'cpu' else min(4, os.cpu_count()//2 if os.cpu_count() else 0)\n",
        "        train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size_train, shuffle=True,\n",
        "                                  collate_fn=lambda b: collate_fn(b, source_vocab.pad_idx, target_vocab.pad_idx),\n",
        "                                  num_workers=num_w, pin_memory=True if device.type == 'cuda' else False, drop_last=True)\n",
        "        dev_loader = DataLoader(dev_dataset, batch_size=cfg.batch_size_train, shuffle=False,\n",
        "                                collate_fn=lambda b: collate_fn(b, source_vocab.pad_idx, target_vocab.pad_idx),\n",
        "                                num_workers=num_w, pin_memory=True if device.type == 'cuda' else False)\n",
        "        if not train_loader or len(train_loader)==0 or not dev_loader or len(dev_loader)==0:\n",
        "            print(f\"ERROR: Train/Dev Dataloader empty. Train: {len(train_loader)}, Dev: {len(dev_loader)}\")\n",
        "            return False\n",
        "\n",
        "        # --- Model, Optimizer, Loss Function Setup ---\n",
        "        encoder_hidden_dim_eff = cfg.hidden_dim * (2 if cfg.encoder_bidirectional else 1)\n",
        "        encoder = Encoder(source_vocab.n_chars, cfg.embedding_dim, cfg.hidden_dim,\n",
        "                          cfg.encoder_layers, cfg.cell_type, cfg.dropout_p,\n",
        "                          cfg.encoder_bidirectional, pad_idx=source_vocab.pad_idx).to(device)\n",
        "        decoder = DecoderWithAttention(target_vocab.n_chars, cfg.embedding_dim, encoder_hidden_dim_eff,\n",
        "                                       cfg.hidden_dim, cfg.decoder_layers, cfg.cell_type,\n",
        "                                       cfg.dropout_p, pad_idx=target_vocab.pad_idx).to(device)\n",
        "        model = Seq2SeqWithAttention(encoder, decoder, device, target_vocab.sos_idx).to(device)\n",
        "        print(f\"Best Attention Model parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
        "        wandb.watch(model, log=\"all\", log_freq=100)\n",
        "\n",
        "        optimizer = optim.Adam(model.parameters(), lr=cfg.learning_rate_train)\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=cfg.lr_scheduler_patience_train, factor=0.3, verbose=True)\n",
        "        criterion = nn.CrossEntropyLoss(ignore_index=target_vocab.pad_idx)\n",
        "\n",
        "        best_val_acc_this_training = -1.0\n",
        "        epochs_no_improve = 0\n",
        "        max_epochs_no_improve_val = cfg.get('max_epochs_no_improve_train', 7)\n",
        "\n",
        "        for epoch in range(cfg.epochs_train):\n",
        "            train_loss, _ = _train_one_epoch(model, train_loader, optimizer, criterion, device,\n",
        "                                             cfg.clip_value_train, cfg.teacher_forcing_train, target_vocab)\n",
        "            val_loss, val_acc = _evaluate_one_epoch(model, dev_loader, criterion, device,\n",
        "                                                    source_vocab, target_vocab, beam_width=1, is_test_set=False)\n",
        "\n",
        "            scheduler.step(val_acc)\n",
        "            print(f\"Epoch {epoch+1}/{cfg.epochs_train} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "            wandb.log({\"epoch_train_best_attn\": epoch + 1, \"train_loss_best_attn\": train_loss,\n",
        "                       \"val_loss_best_attn\": val_loss, \"val_acc_best_attn\": val_acc, \"lr_best_attn\": optimizer.param_groups[0]['lr']})\n",
        "\n",
        "            # Early stopping logic\n",
        "            if val_acc > best_val_acc_this_training:\n",
        "                best_val_acc_this_training = val_acc\n",
        "                epochs_no_improve = 0\n",
        "                torch.save(model.state_dict(), model_save_path)\n",
        "                print(f\"Saved new best attention model to {model_save_path} (Val Acc: {best_val_acc_this_training:.4f})\")\n",
        "            else:\n",
        "                epochs_no_improve += 1\n",
        "\n",
        "            if epochs_no_improve >= max_epochs_no_improve_val:\n",
        "                print(f\"Early stopping for best attention model training at epoch {epoch+1}.\")\n",
        "                break\n",
        "\n",
        "        # Save the final model state if no improvement happened over the initial checkpoint\n",
        "        if not os.path.exists(model_save_path):\n",
        "            torch.save(model.state_dict(), model_save_path)\n",
        "            print(f\"Saved final attention model state to {model_save_path}\")\n",
        "\n",
        "        wandb.summary[\"final_best_val_accuracy_during_attn_training\"] = best_val_acc_this_training\n",
        "        print(f\"Finished training best attention model. Best Val Acc during this training: {best_val_acc_this_training:.4f}\")\n",
        "    return True\n",
        "\n",
        "\n",
        "def plot_attention_heatmap(source_chars, predicted_chars, attention_matrix, file_path=\"attention_heatmap.png\", title=\"Attention Heatmap\"):\n",
        "    \"\"\"\n",
        "    Plots an attention heatmap and saves it to a file.\n",
        "\n",
        "    Args:\n",
        "        source_chars (list): List of characters from the source sequence.\n",
        "        predicted_chars (list): List of characters from the predicted target sequence.\n",
        "        attention_matrix (np.ndarray): 2D NumPy array of attention weights (rows=target, cols=source).\n",
        "        file_path (str): Path to save the generated heatmap image.\n",
        "        title (str): Title for the heatmap plot.\n",
        "    \"\"\"\n",
        "    if attention_matrix is None or not isinstance(attention_matrix, np.ndarray) or attention_matrix.ndim != 2 or attention_matrix.shape[0] == 0 or attention_matrix.shape[1] == 0:\n",
        "        print(f\"Warning: Invalid attention matrix for '{''.join(source_chars)}' -> '{''.join(predicted_chars)}'. Skipping plot.\")\n",
        "        return\n",
        "\n",
        "    plot_pred_len = len(predicted_chars)\n",
        "    plot_src_len = len(source_chars)\n",
        "\n",
        "    if plot_pred_len == 0 or plot_src_len == 0:\n",
        "        print(f\"Warning: Empty source or predicted characters for heatmap. Source: {plot_src_len}, Pred: {plot_pred_len}. Skipping.\")\n",
        "        return\n",
        "\n",
        "    current_attention_matrix = attention_matrix[:plot_pred_len, :plot_src_len]\n",
        "\n",
        "    if current_attention_matrix.shape[0] != plot_pred_len or current_attention_matrix.shape[1] != plot_src_len :\n",
        "        print(f\"Warning: Attention matrix shape ({attention_matrix.shape}) \"\n",
        "              f\"could not be perfectly aligned with char lists (Pred:{plot_pred_len}, Src:{plot_src_len}). \"\n",
        "              f\"Using shape {current_attention_matrix.shape} for plot. Input: '{''.join(source_chars)}'\")\n",
        "        if not (current_attention_matrix.shape[0] > 0 and current_attention_matrix.shape[1] > 0):\n",
        "             print(\"  Skipping plot due to zero dimension after alignment.\")\n",
        "             return\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(max(6, plot_src_len*0.7), max(4, plot_pred_len*0.7)))\n",
        "    cax = ax.matshow(current_attention_matrix, cmap='viridis')\n",
        "    fig.colorbar(cax)\n",
        "\n",
        "    try: # Attempt to set Devanagari font\n",
        "        ax.set_yticklabels([''] + predicted_chars, fontfamily='Arial Unicode MS', fontsize=10)\n",
        "    except:\n",
        "        print(\"Warning: Arial Unicode MS font not found. Using default sans-serif for Devanagari labels.\")\n",
        "        ax.set_yticklabels([''] + predicted_chars, fontfamily='sans-serif', fontsize=10)\n",
        "    ax.set_xticklabels([''] + source_chars, rotation=90, fontfamily='sans-serif', fontsize=10)\n",
        "\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.xlabel(\"Source (Latin)\")\n",
        "    plt.ylabel(\"Prediction (Devanagari)\")\n",
        "    plt.title(title, fontsize=12)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    try:\n",
        "        plt.savefig(file_path)\n",
        "        print(f\"Saved attention heatmap to {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving heatmap: {e}\")\n",
        "    plt.close(fig)\n",
        "\n",
        "# --- Main Execution Block for Heatmap Generation ---\n",
        "if __name__ == '__main__':\n",
        "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "    # --- W&B Login ---\n",
        "    try:\n",
        "        if 'KAGGLE_KERNEL_RUN_TYPE' in os.environ:\n",
        "            print(\"Detected Kaggle environment. Ensuring WANDB_API_KEY is set.\")\n",
        "            if \"WANDB_API_KEY\" in os.environ: wandb.login()\n",
        "            else:\n",
        "                from kaggle_secrets import UserSecretsClient\n",
        "                user_secrets = UserSecretsClient()\n",
        "                wandb_api_key = user_secrets.get_secret(\"WANDB_API_KEY\")\n",
        "                wandb.login(key=wandb_api_key)\n",
        "            print(\"W&B login for Kaggle successful.\")\n",
        "        else:\n",
        "            wandb.login()\n",
        "        print(\"W&B login process attempted/completed.\")\n",
        "    except Exception as e:\n",
        "        print(f\"W&B login failed: {e}. Ensure API key is configured.\")\n",
        "        wandb = None # Disable wandb if login fails\n",
        "\n",
        "    # --- Best Attention Model Hyperparameters (UPDATE THIS SECTION) ---\n",
        "    # These hyperparameters should come from your W&B sweep for the Attention Model.\n",
        "    BEST_ATTN_HYPERPARAMETERS = {\n",
        "        'embedding_dim': 256,\n",
        "        'hidden_dim': 512,\n",
        "        'encoder_layers': 1,\n",
        "        'decoder_layers': 1,\n",
        "        'cell_type': 'LSTM',\n",
        "        'dropout_p': 0.3,\n",
        "        'encoder_bidirectional': True,\n",
        "        'learning_rate_train': 0.0008,\n",
        "        'batch_size_train': 64,\n",
        "        'epochs_train': 20,\n",
        "        'clip_value_train': 1.0,\n",
        "        'teacher_forcing_train': 0.5,\n",
        "        'max_epochs_no_improve_train': 7,\n",
        "        'lr_scheduler_patience_train': 3,\n",
        "        'vocab_min_freq': 1,\n",
        "        'max_seq_len': 50,\n",
        "        'eval_batch_size': 64,\n",
        "        # beam_width_eval is not directly used by predict_with_attention, which is greedy for this heatmap purpose\n",
        "    }\n",
        "    ATTN_MODEL_SAVE_PATH = \"/kaggle/working/best_attention_model_q5.pt\"\n",
        "    HEATMAP_OUTPUT_DIR = \"/kaggle/working/attention_heatmaps_q5d\"\n",
        "\n",
        "    print(f\"Target hyperparameters for best attention model: {BEST_ATTN_HYPERPARAMETERS}\")\n",
        "\n",
        "    # --- Phase 1: Train and Save the Best Attention Model (if checkpoint doesn't exist) ---\n",
        "    if not os.path.exists(ATTN_MODEL_SAVE_PATH):\n",
        "        print(f\"\\n--- Attention Model Checkpoint NOT FOUND at {ATTN_MODEL_SAVE_PATH} ---\")\n",
        "        print(\"--- Attempting to TRAIN AND SAVE the best attention model using BEST_ATTN_HYPERPARAMETERS ---\")\n",
        "        training_successful = train_and_save_attention_model(BEST_ATTN_HYPERPARAMETERS, ATTN_MODEL_SAVE_PATH, DEVICE)\n",
        "        if not training_successful or not os.path.exists(ATTN_MODEL_SAVE_PATH):\n",
        "            print(\"ERROR: Failed to train and save the best attention model. Exiting.\")\n",
        "            exit()\n",
        "        print(f\"Best attention model trained and saved to {ATTN_MODEL_SAVE_PATH}\")\n",
        "    else:\n",
        "        print(f\"Found existing attention model checkpoint at {ATTN_MODEL_SAVE_PATH}. Will use this for heatmaps.\")\n",
        "\n",
        "    # --- Phase 2: Load Model and Prepare for Heatmap Generation ---\n",
        "    print(\"\\n--- Preparing for Heatmap Generation ---\")\n",
        "\n",
        "    BASE_DATA_DIR = \"/kaggle/input/dakshina-dl-a3/dakshina_dataset_v1.0/hi/\"\n",
        "    DATA_DIR = os.path.join(BASE_DATA_DIR, \"lexicons/\")\n",
        "    train_file_for_vocab = os.path.join(DATA_DIR, \"hi.translit.sampled.train.tsv\")\n",
        "    test_file = os.path.join(DATA_DIR, \"hi.translit.sampled.test.tsv\")\n",
        "\n",
        "    source_vocab = Vocabulary(\"latin_attn_heatmap\")\n",
        "    target_vocab = Vocabulary(\"devanagari_attn_heatmap\")\n",
        "\n",
        "    temp_train_ds_attn_vocab = TransliterationDataset(train_file_for_vocab, source_vocab, target_vocab,\n",
        "                                                max_len=BEST_ATTN_HYPERPARAMETERS['max_seq_len'])\n",
        "    if not temp_train_ds_attn_vocab.pairs: exit()\n",
        "    for src, tgt in temp_train_ds_attn_vocab.pairs:\n",
        "        source_vocab.add_sequence(src)\n",
        "        target_vocab.add_sequence(tgt)\n",
        "    source_vocab.build_vocab(min_freq=BEST_ATTN_HYPERPARAMETERS['vocab_min_freq'])\n",
        "    target_vocab.build_vocab(min_freq=BEST_ATTN_HYPERPARAMETERS['vocab_min_freq'])\n",
        "\n",
        "    test_dataset_for_heatmaps = TransliterationDataset(test_file, source_vocab, target_vocab,\n",
        "                                                  max_len=BEST_ATTN_HYPERPARAMETERS['max_seq_len'])\n",
        "    if not test_dataset_for_heatmaps.pairs:\n",
        "        print(\"Test dataset for heatmaps is empty. Exiting.\")\n",
        "        exit()\n",
        "\n",
        "    encoder_hidden_dim_eff_test = BEST_ATTN_HYPERPARAMETERS['hidden_dim'] * (2 if BEST_ATTN_HYPERPARAMETERS['encoder_bidirectional'] else 1)\n",
        "    encoder = Encoder(source_vocab.n_chars, BEST_ATTN_HYPERPARAMETERS['embedding_dim'], BEST_ATTN_HYPERPARAMETERS['hidden_dim'],\n",
        "                      BEST_ATTN_HYPERPARAMETERS['encoder_layers'], BEST_ATTN_HYPERPARAMETERS['cell_type'], BEST_ATTN_HYPERPARAMETERS['dropout_p'],\n",
        "                      BEST_ATTN_HYPERPARAMETERS['encoder_bidirectional'], pad_idx=source_vocab.pad_idx).to(DEVICE)\n",
        "    decoder = DecoderWithAttention(target_vocab.n_chars, BEST_ATTN_HYPERPARAMETERS['embedding_dim'],\n",
        "                                   encoder_hidden_dim_eff_test, BEST_ATTN_HYPERPARAMETERS['hidden_dim'],\n",
        "                                   BEST_ATTN_HYPERPARAMETERS['decoder_layers'], BEST_ATTN_HYPERPARAMETERS['cell_type'], BEST_ATTN_HYPERPARAMETERS['dropout_p'],\n",
        "                                   pad_idx=target_vocab.pad_idx).to(DEVICE)\n",
        "    model = Seq2SeqWithAttention(encoder, decoder, DEVICE, target_vocab.sos_idx).to(DEVICE)\n",
        "\n",
        "    print(f\"Loading weights into attention model from: {ATTN_MODEL_SAVE_PATH}\")\n",
        "    model.load_state_dict(torch.load(ATTN_MODEL_SAVE_PATH, map_location=DEVICE))\n",
        "    print(\"Attention model weights loaded successfully.\")\n",
        "    model.eval() # Ensure model is in evaluation mode\n",
        "\n",
        "    # --- Generate and Plot Attention Heatmaps ---\n",
        "    if not os.path.exists(HEATMAP_OUTPUT_DIR):\n",
        "        os.makedirs(HEATMAP_OUTPUT_DIR)\n",
        "\n",
        "    num_heatmap_samples = 10\n",
        "    actual_num_samples_heatmap = min(num_heatmap_samples, len(test_dataset_for_heatmaps.pairs))\n",
        "    sample_indices_heatmap = []\n",
        "    if len(test_dataset_for_heatmaps.pairs) > 0:\n",
        "        sample_indices_heatmap = random.sample(range(len(test_dataset_for_heatmaps.pairs)), actual_num_samples_heatmap)\n",
        "\n",
        "    generated_heatmap_files = []\n",
        "\n",
        "    print(f\"\\n--- Generating {len(sample_indices_heatmap)} Attention Heatmaps ---\")\n",
        "    for i, data_idx in enumerate(sample_indices_heatmap):\n",
        "        source_str, true_target_str = test_dataset_for_heatmaps.pairs[data_idx]\n",
        "\n",
        "        # Prepare input for predict_with_attention\n",
        "        source_indices_for_pred = source_vocab.sequence_to_indices(source_str, add_eos=True, max_len=BEST_ATTN_HYPERPARAMETERS['max_seq_len'])\n",
        "        source_tensor_hm = torch.tensor(source_indices_for_pred, dtype=torch.long).unsqueeze(0).to(DEVICE)\n",
        "        source_length_hm = torch.tensor([len(source_indices_for_pred)], dtype=torch.long).to(DEVICE)\n",
        "\n",
        "        predicted_indices_hm, attention_matrix = model.predict_with_attention(\n",
        "            source_tensor_hm,\n",
        "            source_length_hm,\n",
        "            max_output_len=len(true_target_str) + 10,\n",
        "            target_eos_idx=target_vocab.eos_idx\n",
        "        )\n",
        "\n",
        "        # Convert indices to characters for plotting\n",
        "        source_chars_for_plot = list(source_str) # Use original source string characters\n",
        "        predicted_chars_for_plot = list(target_vocab.indices_to_sequence(predicted_indices_hm, strip_special=True))\n",
        "\n",
        "        if attention_matrix is not None and len(predicted_chars_for_plot) > 0 and len(source_chars_for_plot) > 0:\n",
        "            # Slice attention_matrix to match the lengths of displayed characters\n",
        "            valid_pred_len = min(len(predicted_chars_for_plot), attention_matrix.shape[0])\n",
        "            valid_src_len = min(len(source_chars_for_plot), attention_matrix.shape[1])\n",
        "\n",
        "            if valid_pred_len > 0 and valid_src_len > 0:\n",
        "                plot_attn_matrix = attention_matrix[:valid_pred_len, :valid_src_len]\n",
        "\n",
        "                heatmap_file_path = os.path.join(HEATMAP_OUTPUT_DIR, f\"attn_heatmap_{i+1}_{source_str[:15].replace(' ','_').replace('/','')}.png\")\n",
        "                plot_attention_heatmap(source_chars_for_plot[:valid_src_len],\n",
        "                                       predicted_chars_for_plot[:valid_pred_len],\n",
        "                                       plot_attn_matrix,\n",
        "                                       heatmap_file_path,\n",
        "                                       title=f\"Input: {source_str} -> Pred: {''.join(predicted_chars_for_plot)}\")\n",
        "                generated_heatmap_files.append(heatmap_file_path)\n",
        "            else:\n",
        "                print(f\"Warning: Not enough data in attention matrix or char lists for '{source_str}'. Skipping heatmap.\")\n",
        "        else:\n",
        "            print(f\"Warning: Could not generate attention matrix or empty prediction/source for '{source_str}'. Skipping heatmap.\")\n",
        "\n",
        "    print(f\"\\n--- Markdown for Displaying Attention Heatmaps (Saved in '{HEATMAP_OUTPUT_DIR}') ---\")\n",
        "    if generated_heatmap_files:\n",
        "        print(\"You can use the following Markdown in your report (adjust paths if needed):\")\n",
        "        md_grid_rows = []\n",
        "        num_full_rows = len(generated_heatmap_files) // 3\n",
        "        for r_idx in range(num_full_rows):\n",
        "            md_row_headers_str = \" | \".join([f\"Heatmap {r_idx*3+j+1}\" for j in range(3)])\n",
        "            md_row_images_str = \" | \".join([f\"![Attention {r_idx*3+j+1}]({os.path.relpath(generated_heatmap_files[r_idx*3+j], '/kaggle/working/')})\" for j in range(3)])\n",
        "            if r_idx == 0:\n",
        "                md_grid_rows.append(f\"| {md_row_headers_str} |\")\n",
        "                md_grid_rows.append(f\"|---|---|---|\")\n",
        "            md_grid_rows.append(f\"| {md_row_images_str} |\")\n",
        "\n",
        "        remaining_idx_start = num_full_rows * 3\n",
        "        if remaining_idx_start < len(generated_heatmap_files):\n",
        "            md_row_headers_list = []\n",
        "            md_row_images_list = []\n",
        "            for j, file_idx in enumerate(range(remaining_idx_start, len(generated_heatmap_files))):\n",
        "                md_row_headers_list.append(f\"Heatmap {file_idx+1}\")\n",
        "                md_row_images_list.append(f\"![Attention {file_idx+1}]({os.path.relpath(generated_heatmap_files[file_idx], '/kaggle/working/')})\")\n",
        "\n",
        "            # Pad to 3 columns if necessary\n",
        "            while len(md_row_headers_list) < 3: md_row_headers_list.append(\" \")\n",
        "            while len(md_row_images_list) < 3: md_row_images_list.append(\" \")\n",
        "\n",
        "            if num_full_rows == 0:\n",
        "                md_grid_rows.append(f\"| {md_row_headers_list[0]} | {md_row_headers_list[1]} | {md_row_headers_list[2]} |\")\n",
        "                md_grid_rows.append(f\"|---|---|---|\")\n",
        "            md_grid_rows.append(f\"| {md_row_images_list[0]} | {md_row_images_list[1]} | {md_row_images_list[2]} |\")\n",
        "\n",
        "        print(\"\\n\".join(md_grid_rows))\n",
        "        print(\"\\n(Note: Paths are relative to '/kaggle/working/'. Adjust if your report is viewed elsewhere.)\")\n",
        "    else:\n",
        "        print(\"No heatmaps were generated successfully.\")\n",
        "\n",
        "    # --- Log Heatmaps to W&B ---\n",
        "    if wandb and wandb.run is None:\n",
        "        wandb.init(project=\"DL_A3\", name=f\"Q5d_Attention_Heatmaps\", config=BEST_ATTN_HYPERPARAMETERS, job_type=\"q5d_heatmap_generation\", reinit=True)\n",
        "\n",
        "    if wandb and wandb.run and generated_heatmap_files:\n",
        "        for i, f_path in enumerate(generated_heatmap_files):\n",
        "            if os.path.exists(f_path):\n",
        "                wandb.log({f\"q5d_attention_heatmap_sample_{i+1}\": wandb.Image(f_path, caption=f\"Heatmap for sample {i+1}\")})\n",
        "        print(\"Attention heatmaps logged to W&B.\")\n",
        "        if wandb.run: wandb.finish()\n",
        "\n",
        "    print(\"Script finished.\")"
      ],
      "metadata": {
        "id": "fMNy1hu0T4ID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q6"
      ],
      "metadata": {
        "id": "x-ACWkleTqY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import numpy as np\n",
        "import wandb\n",
        "import os\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "import heapq\n",
        "import json\n",
        "import html\n",
        "\n",
        "# --- Constants for Special Tokens ---\n",
        "SOS_TOKEN = \"<sos>\"  # Start-of-sequence token\n",
        "EOS_TOKEN = \"<eos>\"  # End-of-sequence token\n",
        "PAD_TOKEN = \"<pad>\"  # Padding token\n",
        "UNK_TOKEN = \"<unk>\"  # Unknown token\n",
        "\n",
        "# --- Attention Mechanism ---\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements a general (Luong-style) attention mechanism to compute alignment scores\n",
        "    between the decoder's current hidden state and all encoder's output states.\n",
        "    \"\"\"\n",
        "    def __init__(self, encoder_hidden_dim_eff, decoder_hidden_dim):\n",
        "        super(Attention, self).__init__()\n",
        "        self.attn_W = nn.Linear(encoder_hidden_dim_eff + decoder_hidden_dim, decoder_hidden_dim)\n",
        "        self.attn_v = nn.Linear(decoder_hidden_dim, 1, bias=False)\n",
        "\n",
        "    def forward(self, decoder_hidden_top_layer, encoder_outputs):\n",
        "        \"\"\"\n",
        "        Calculates attention weights.\n",
        "\n",
        "        Args:\n",
        "            decoder_hidden_top_layer (torch.Tensor): The top layer's hidden state of the decoder RNN (batch_size, dec_hidden_dim).\n",
        "            encoder_outputs (torch.Tensor): All hidden states from the encoder (batch_size, src_len, enc_hidden_dim).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Attention weights (batch_size, src_len) representing alignment probabilities.\n",
        "        \"\"\"\n",
        "        src_len = encoder_outputs.size(1)\n",
        "        repeated_decoder_hidden = decoder_hidden_top_layer.unsqueeze(1).repeat(1, src_len, 1)\n",
        "\n",
        "        energy_input = torch.cat((repeated_decoder_hidden, encoder_outputs), dim=2)\n",
        "        energy = torch.tanh(self.attn_W(energy_input))\n",
        "\n",
        "        attention_scores = self.attn_v(energy).squeeze(2)\n",
        "        return F.softmax(attention_scores, dim=1)\n",
        "\n",
        "# --- Model Definition (Encoder, DecoderWithAttention, Seq2SeqWithAttention) ---\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    The Encoder processes the input sequence, producing a context vector (final hidden state)\n",
        "    and a sequence of output states for the attention mechanism.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_vocab_size, embedding_dim, hidden_dim, n_layers,\n",
        "                 cell_type='LSTM', dropout_p=0.1, bidirectional=False, pad_idx=0):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.input_vocab_size = input_vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.cell_type = cell_type.upper()\n",
        "        self.bidirectional = bidirectional\n",
        "        self.num_directions = 2 if bidirectional else 1\n",
        "\n",
        "        self.embedding = nn.Embedding(input_vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "        rnn_dropout = dropout_p if n_layers > 1 else 0\n",
        "        if self.cell_type == 'RNN':\n",
        "            self.rnn = nn.RNN(embedding_dim, hidden_dim, n_layers,\n",
        "                              dropout=rnn_dropout, batch_first=True, bidirectional=self.bidirectional)\n",
        "        elif self.cell_type == 'GRU':\n",
        "            self.rnn = nn.GRU(embedding_dim, hidden_dim, n_layers,\n",
        "                              dropout=rnn_dropout, batch_first=True, bidirectional=self.bidirectional)\n",
        "        elif self.cell_type == 'LSTM':\n",
        "            self.rnn = nn.LSTM(embedding_dim, hidden_dim, n_layers,\n",
        "                               dropout=rnn_dropout, batch_first=True, bidirectional=self.bidirectional)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported cell type. Choose from 'RNN', 'GRU', 'LSTM'.\")\n",
        "\n",
        "    def forward(self, input_seq, input_lengths):\n",
        "        \"\"\"\n",
        "        Forward pass for the encoder.\n",
        "\n",
        "        Args:\n",
        "            input_seq (torch.Tensor): Padded input sequences (batch_size, seq_len).\n",
        "            input_lengths (torch.Tensor): Lengths of the original sequences (batch_size,).\n",
        "\n",
        "        Returns:\n",
        "            tuple: (`encoder_outputs`, `hidden_state`).\n",
        "                   `encoder_outputs`: All hidden states from the last layer (batch_size, src_len, hidden_dim * num_directions).\n",
        "                   `hidden_state`: The final hidden state (and cell state for LSTM) from all layers.\n",
        "        \"\"\"\n",
        "        embedded = self.embedding(input_seq)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "        packed_outputs, hidden = self.rnn(packed_embedded)\n",
        "        encoder_outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs, batch_first=True)\n",
        "        return encoder_outputs, hidden\n",
        "\n",
        "class DecoderWithAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    The Decoder generates the output sequence one token at a time,\n",
        "    incorporating an attention mechanism over the encoder's output states.\n",
        "    \"\"\"\n",
        "    def __init__(self, output_vocab_size, embedding_dim, encoder_hidden_dim_eff,\n",
        "                 decoder_hidden_dim, n_layers, cell_type='LSTM', dropout_p=0.1, pad_idx=0):\n",
        "        super(DecoderWithAttention, self).__init__()\n",
        "        self.output_vocab_size = output_vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.encoder_hidden_dim_eff = encoder_hidden_dim_eff\n",
        "        self.decoder_hidden_dim = decoder_hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.cell_type = cell_type.upper()\n",
        "\n",
        "        self.attention = Attention(encoder_hidden_dim_eff, decoder_hidden_dim)\n",
        "        self.embedding = nn.Embedding(output_vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "        # RNN input is concatenation of token embedding and context vector\n",
        "        rnn_input_dim = embedding_dim + encoder_hidden_dim_eff\n",
        "\n",
        "        rnn_dropout = dropout_p if n_layers > 1 else 0\n",
        "        if self.cell_type == 'RNN':\n",
        "            self.rnn = nn.RNN(rnn_input_dim, decoder_hidden_dim, n_layers,\n",
        "                              dropout=rnn_dropout, batch_first=True)\n",
        "        elif self.cell_type == 'GRU':\n",
        "            self.rnn = nn.GRU(rnn_input_dim, decoder_hidden_dim, n_layers,\n",
        "                              dropout=rnn_dropout, batch_first=True)\n",
        "        elif self.cell_type == 'LSTM':\n",
        "            self.rnn = nn.LSTM(rnn_input_dim, decoder_hidden_dim, n_layers,\n",
        "                               dropout=rnn_dropout, batch_first=True)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported cell type. Choose from 'RNN', 'GRU', 'LSTM'.\")\n",
        "\n",
        "        self.fc_out = nn.Linear(decoder_hidden_dim, output_vocab_size)\n",
        "\n",
        "    def forward(self, input_char, prev_decoder_hidden, encoder_outputs):\n",
        "        \"\"\"\n",
        "        Forward pass for the attention-based decoder.\n",
        "\n",
        "        Args:\n",
        "            input_char (torch.Tensor): Current target token (batch_size,).\n",
        "            prev_decoder_hidden (torch.Tensor or tuple): Previous hidden state(s) of the decoder RNN.\n",
        "            encoder_outputs (torch.Tensor): All hidden states from the encoder (batch_size, src_len, enc_hidden_dim_eff).\n",
        "\n",
        "        Returns:\n",
        "            tuple: (`prediction_logits`, `current_decoder_hidden`, `attention_weights`).\n",
        "                   `prediction_logits`: Raw scores for each token in the vocabulary.\n",
        "                   `current_decoder_hidden`: Updated hidden state of the decoder RNN.\n",
        "                   `attention_weights`: Attention probabilities (batch_size, src_len).\n",
        "        \"\"\"\n",
        "        input_char = input_char.unsqueeze(1)\n",
        "        embedded = self.embedding(input_char)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        # Get the hidden state for attention query (top layer's hidden state)\n",
        "        if self.cell_type == 'LSTM':\n",
        "            attention_query_hidden = prev_decoder_hidden[0][-1, :, :]\n",
        "        else:\n",
        "            attention_query_hidden = prev_decoder_hidden[-1, :, :]\n",
        "\n",
        "        # Calculate attention weights and context vector\n",
        "        attention_weights = self.attention(attention_query_hidden, encoder_outputs)\n",
        "        context_vector = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)\n",
        "\n",
        "        # Concatenate embedded input and context vector to form RNN input\n",
        "        rnn_input = torch.cat((embedded, context_vector), dim=2)\n",
        "\n",
        "        # Pass through decoder RNN\n",
        "        rnn_output, current_decoder_hidden = self.rnn(rnn_input, prev_decoder_hidden)\n",
        "\n",
        "        # Project RNN output to vocabulary size\n",
        "        rnn_output_squeezed = rnn_output.squeeze(1)\n",
        "        prediction_logits = self.fc_out(rnn_output_squeezed)\n",
        "\n",
        "        return prediction_logits, current_decoder_hidden, attention_weights\n",
        "\n",
        "\n",
        "class Seq2SeqWithAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    The main Sequence-to-Sequence model integrating the Encoder and Attention-based Decoder.\n",
        "    \"\"\"\n",
        "    def __init__(self, encoder, decoder, device, target_sos_idx):\n",
        "        super(Seq2SeqWithAttention, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "        self.target_sos_idx = target_sos_idx\n",
        "\n",
        "        # Layers to adapt encoder's final hidden state to decoder's initial hidden state\n",
        "        encoder_effective_final_hidden_dim = self.encoder.hidden_dim * self.encoder.num_directions\n",
        "        decoder_rnn_hidden_dim = self.decoder.decoder_hidden_dim\n",
        "\n",
        "        self.fc_adapt_hidden = None\n",
        "        self.fc_adapt_cell = None\n",
        "        if encoder_effective_final_hidden_dim != decoder_rnn_hidden_dim:\n",
        "            self.fc_adapt_hidden = nn.Linear(encoder_effective_final_hidden_dim, decoder_rnn_hidden_dim)\n",
        "            if self.encoder.cell_type == 'LSTM':\n",
        "                self.fc_adapt_cell = nn.Linear(encoder_effective_final_hidden_dim, decoder_rnn_hidden_dim)\n",
        "\n",
        "    def _adapt_encoder_hidden_for_decoder(self, encoder_final_hidden_state):\n",
        "        \"\"\"\n",
        "        Adapts the encoder's final hidden state(s) to match the decoder's expected dimensions and layer count.\n",
        "        Handles bidirectionality and differing layer counts.\n",
        "        \"\"\"\n",
        "        is_lstm = self.encoder.cell_type == 'LSTM'\n",
        "        if is_lstm:\n",
        "            h_from_enc, c_from_enc = encoder_final_hidden_state\n",
        "        else:\n",
        "            h_from_enc = encoder_final_hidden_state\n",
        "            c_from_enc = None\n",
        "\n",
        "        batch_size = h_from_enc.size(1)\n",
        "\n",
        "        # Reshape and concatenate bidirectional states if applicable\n",
        "        h_processed = h_from_enc.view(self.encoder.n_layers, self.encoder.num_directions,\n",
        "                                      batch_size, self.encoder.hidden_dim)\n",
        "        if self.encoder.bidirectional:\n",
        "            h_processed = torch.cat([h_processed[:, 0, :, :], h_processed[:, 1, :, :]], dim=2)\n",
        "        else:\n",
        "            h_processed = h_processed.squeeze(1)\n",
        "\n",
        "        c_processed = None\n",
        "        if is_lstm and c_from_enc is not None:\n",
        "            c_processed = c_from_enc.view(self.encoder.n_layers, self.encoder.num_directions,\n",
        "                                          batch_size, self.encoder.hidden_dim)\n",
        "            if self.encoder.bidirectional:\n",
        "                c_processed = torch.cat([c_processed[:, 0, :, :], c_processed[:, 1, :, :]], dim=2)\n",
        "            else:\n",
        "                c_processed = c_processed.squeeze(1)\n",
        "\n",
        "        # Apply linear transformation for dimension adaptation if needed\n",
        "        if self.fc_adapt_hidden:\n",
        "            h_processed = self.fc_adapt_hidden(h_processed)\n",
        "            if is_lstm and c_processed is not None and self.fc_adapt_cell:\n",
        "                c_processed = self.fc_adapt_cell(c_processed)\n",
        "\n",
        "        # Adapt number of layers for the decoder's RNN\n",
        "        final_h = torch.zeros(self.decoder.n_layers, batch_size, self.decoder.decoder_hidden_dim, device=self.device)\n",
        "        final_c = torch.zeros(self.decoder.n_layers, batch_size, self.decoder.decoder_hidden_dim, device=self.device) if is_lstm else None\n",
        "\n",
        "        num_layers_to_copy = min(self.encoder.n_layers, self.decoder.n_layers)\n",
        "\n",
        "        final_h[:num_layers_to_copy, :, :] = h_processed[:num_layers_to_copy, :, :]\n",
        "        if is_lstm and c_processed is not None:\n",
        "            final_c[:num_layers_to_copy, :, :] = c_processed[:num_layers_to_copy, :, :]\n",
        "\n",
        "        # If decoder has more layers than encoder, repeat the last encoder layer's state\n",
        "        if self.decoder.n_layers > self.encoder.n_layers and self.encoder.n_layers > 0:\n",
        "            last_h_layer_to_repeat = h_processed[self.encoder.n_layers-1, :, :]\n",
        "            for i in range(self.encoder.n_layers, self.decoder.n_layers):\n",
        "                final_h[i, :, :] = last_h_layer_to_repeat\n",
        "                if is_lstm and c_processed is not None:\n",
        "                    last_c_layer_to_repeat = c_processed[self.encoder.n_layers-1, :, :]\n",
        "                    final_c[i, :, :] = last_c_layer_to_repeat\n",
        "\n",
        "        return (final_h, final_c) if is_lstm else final_h\n",
        "\n",
        "    def forward(self, source_seq, source_lengths, target_seq, teacher_forcing_ratio=0.5):\n",
        "        \"\"\"\n",
        "        Forward pass for the Seq2SeqWithAttention model during training.\n",
        "\n",
        "        Args:\n",
        "            source_seq (torch.Tensor): Padded source sequences.\n",
        "            source_lengths (torch.Tensor): Lengths of source sequences.\n",
        "            target_seq (torch.Tensor): Padded target sequences (including SOS token).\n",
        "            teacher_forcing_ratio (float): Probability of using actual target token as next input.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Logits for the predicted target sequence.\n",
        "        \"\"\"\n",
        "        batch_size = source_seq.shape[0]\n",
        "        target_len = target_seq.shape[1]\n",
        "        target_vocab_size = self.decoder.output_vocab_size\n",
        "        outputs_logits = torch.zeros(batch_size, target_len, target_vocab_size).to(self.device)\n",
        "\n",
        "        # Encode the source sequence to get all encoder outputs and final hidden state\n",
        "        encoder_outputs, encoder_final_hidden = self.encoder(source_seq, source_lengths)\n",
        "        # Adapt encoder's final hidden state to initialize the decoder's hidden state\n",
        "        decoder_hidden = self._adapt_encoder_hidden_for_decoder(encoder_final_hidden)\n",
        "        decoder_input = target_seq[:, 0]\n",
        "\n",
        "        # Iterate through the target sequence, predicting one token at a time\n",
        "        for t in range(target_len - 1):\n",
        "            decoder_output_logits, decoder_hidden, _ = \\\n",
        "                self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            outputs_logits[:, t+1] = decoder_output_logits\n",
        "\n",
        "            # Apply teacher forcing: use true target or predicted token for next input\n",
        "            teacher_force_this_step = random.random() < teacher_forcing_ratio\n",
        "            top1_predicted_token = decoder_output_logits.argmax(1)\n",
        "\n",
        "            decoder_input = target_seq[:, t+1] if teacher_force_this_step else top1_predicted_token\n",
        "        return outputs_logits\n",
        "\n",
        "    def predict_with_attention(self, source_seq, source_lengths, max_output_len=50, target_eos_idx=None):\n",
        "        \"\"\"\n",
        "        Generates a sequence using greedy decoding and records attention weights.\n",
        "\n",
        "        Args:\n",
        "            source_seq (torch.Tensor): Input source sequence (batch_size=1).\n",
        "            source_lengths (torch.Tensor): Length of the source sequence.\n",
        "            max_output_len (int): Maximum length of the sequence to generate.\n",
        "            target_eos_idx (int, optional): Index of the EOS token.\n",
        "\n",
        "        Returns:\n",
        "            tuple: Predicted token indices and attention matrices (numpy array).\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        if source_seq.dim() == 1:\n",
        "            source_seq = source_seq.unsqueeze(0)\n",
        "            source_lengths = torch.tensor([source_lengths if isinstance(source_lengths, int) else len(source_lengths)], device=self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            encoder_outputs, encoder_final_hidden = self.encoder(source_seq, source_lengths)\n",
        "            decoder_hidden = self._adapt_encoder_hidden_for_decoder(encoder_final_hidden)\n",
        "            decoder_input = torch.tensor([self.target_sos_idx], device=self.device)\n",
        "\n",
        "            predicted_indices = []\n",
        "            attention_matrices = []\n",
        "\n",
        "            for _ in range(max_output_len):\n",
        "                decoder_output_logits, decoder_hidden, attention_weights = \\\n",
        "                    self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "\n",
        "                attention_matrices.append(attention_weights.squeeze(0).cpu().numpy())\n",
        "\n",
        "                top1_predicted_token = decoder_output_logits.argmax(1)\n",
        "                predicted_idx = top1_predicted_token.item()\n",
        "\n",
        "                predicted_indices.append(predicted_idx)\n",
        "                if target_eos_idx is not None and predicted_idx == target_eos_idx:\n",
        "                    break\n",
        "\n",
        "                if len(predicted_indices) >= max_output_len: break\n",
        "                decoder_input = top1_predicted_token\n",
        "        return predicted_indices, np.array(attention_matrices) if attention_matrices else None\n",
        "\n",
        "# --- Data Loading and Preprocessing ---\n",
        "\n",
        "class Vocabulary:\n",
        "    \"\"\"\n",
        "    Manages the mapping between characters and their numerical indices.\n",
        "    Includes special tokens for padding, start-of-sequence, end-of-sequence, and unknown characters.\n",
        "    \"\"\"\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.char2index = {PAD_TOKEN: 0, SOS_TOKEN: 1, EOS_TOKEN: 2, UNK_TOKEN: 3}\n",
        "        self.index2char = {0: PAD_TOKEN, 1: SOS_TOKEN, 2: EOS_TOKEN, 3: UNK_TOKEN}\n",
        "        self.char_counts = Counter()\n",
        "        self.n_chars = 4\n",
        "        self.pad_idx = self.char2index[PAD_TOKEN]\n",
        "        self.sos_idx = self.char2index[SOS_TOKEN]\n",
        "        self.eos_idx = self.char2index[EOS_TOKEN]\n",
        "\n",
        "    def add_sequence(self, sequence):\n",
        "        \"\"\"Adds characters from a sequence to the character counts.\"\"\"\n",
        "        for char in list(sequence):\n",
        "            self.char_counts[char] += 1\n",
        "\n",
        "    def build_vocab(self, min_freq=1):\n",
        "        \"\"\"\n",
        "        Builds the vocabulary mapping based on character counts and a minimum frequency.\n",
        "        Characters appearing less than `min_freq` will be treated as UNK_TOKEN.\n",
        "        \"\"\"\n",
        "        sorted_chars = sorted(self.char_counts.keys(), key=lambda char: (-self.char_counts[char], char))\n",
        "        for char in sorted_chars:\n",
        "            if self.char_counts[char] >= min_freq and char not in self.char2index:\n",
        "                self.char2index[char] = self.n_chars\n",
        "                self.index2char[self.n_chars] = char\n",
        "                self.n_chars += 1\n",
        "\n",
        "    def sequence_to_indices(self, sequence, add_eos=False, add_sos=False, max_len=None):\n",
        "        \"\"\"Converts a character sequence into a list of numerical indices.\"\"\"\n",
        "        indices = []\n",
        "        if add_sos:\n",
        "            indices.append(self.sos_idx)\n",
        "\n",
        "        seq_to_process = list(sequence)\n",
        "        if max_len:\n",
        "            effective_max_len = max_len\n",
        "            if add_sos: effective_max_len -=1\n",
        "            if add_eos: effective_max_len -=1\n",
        "            seq_to_process = seq_to_process[:max(0, effective_max_len)] # Ensure non-negative slice\n",
        "\n",
        "        for char in seq_to_process:\n",
        "            indices.append(self.char2index.get(char, self.char2index[UNK_TOKEN]))\n",
        "\n",
        "        if add_eos: indices.append(self.eos_idx)\n",
        "        return indices\n",
        "\n",
        "    def indices_to_sequence(self, indices, strip_special=True):\n",
        "        \"\"\"Converts a list of numerical indices back into a character sequence.\"\"\"\n",
        "        chars = []\n",
        "        for index_val in indices:\n",
        "            if strip_special and index_val == self.eos_idx: break\n",
        "            if strip_special and index_val in [self.sos_idx, self.pad_idx]: continue\n",
        "            chars.append(self.index2char.get(index_val, UNK_TOKEN))\n",
        "        return \"\".join(chars)\n",
        "\n",
        "class TransliterationDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A PyTorch Dataset for loading and preparing transliteration pairs.\n",
        "    Reads data from a TSV file and converts text sequences to token indices.\n",
        "    \"\"\"\n",
        "    def __init__(self, file_path, source_vocab, target_vocab, max_len=50):\n",
        "        self.pairs = []\n",
        "        self.source_vocab = source_vocab\n",
        "        self.target_vocab = target_vocab\n",
        "        self.max_len = max_len\n",
        "\n",
        "        if not os.path.exists(file_path):\n",
        "            print(f\"ERROR: Data file not found: {file_path}\")\n",
        "            return\n",
        "\n",
        "        print(f\"Loading data from: {file_path}\")\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                for line_num, line in enumerate(f):\n",
        "                    parts = line.strip().split('\\t')\n",
        "                    if len(parts) >= 2:\n",
        "                        target, source = parts[0], parts[1]\n",
        "                        if not source or not target or \\\n",
        "                           (self.max_len and (len(source) > self.max_len or len(target) > self.max_len)):\n",
        "                            continue\n",
        "                        self.pairs.append((source, target))\n",
        "            print(f\"Loaded {len(self.pairs)} pairs from {file_path}.\")\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR: Could not read or process file {file_path}. Error: {e}\")\n",
        "\n",
        "    def __len__(self): return len(self.pairs)\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Returns a source-target pair as Tensors of indices.\"\"\"\n",
        "        source_str, target_str = self.pairs[idx]\n",
        "        source_indices = self.source_vocab.sequence_to_indices(source_str, add_eos=True, max_len=self.max_len)\n",
        "        target_indices = self.target_vocab.sequence_to_indices(target_str, add_sos=True, add_eos=True, max_len=self.max_len)\n",
        "        return torch.tensor(source_indices, dtype=torch.long), torch.tensor(target_indices, dtype=torch.long)\n",
        "\n",
        "def collate_fn(batch, pad_idx_source, pad_idx_target):\n",
        "    \"\"\"\n",
        "    Custom collate function for DataLoader to handle variable-length sequences.\n",
        "    Pads sequences within a batch to the maximum length of that batch.\n",
        "    \"\"\"\n",
        "    batch = [item for item in batch if item is not None and len(item[0]) > 0 and len(item[1]) > 0]\n",
        "    if not batch: return None, None, None\n",
        "    source_seqs, target_seqs = zip(*batch)\n",
        "\n",
        "    source_lengths = torch.tensor([len(s) for s in source_seqs], dtype=torch.long)\n",
        "    padded_sources = pad_sequence(source_seqs, batch_first=True, padding_value=pad_idx_source)\n",
        "    padded_targets = pad_sequence(target_seqs, batch_first=True, padding_value=pad_idx_target)\n",
        "    return padded_sources, source_lengths, padded_targets\n",
        "\n",
        "# --- Training and Evaluation Functions ---\n",
        "\n",
        "def _train_one_epoch(model, dataloader, optimizer, criterion, device, clip_value, teacher_forcing_ratio, target_vocab):\n",
        "    \"\"\"\n",
        "    Trains the model for a single epoch.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The Seq2Seq model.\n",
        "        dataloader (DataLoader): DataLoader for the training set.\n",
        "        optimizer (optim.Optimizer): Optimizer for model parameters.\n",
        "        criterion (nn.Module): Loss function (e.g., CrossEntropyLoss).\n",
        "        device (torch.device): Device to run the model on.\n",
        "        clip_value (float): Gradient clipping value.\n",
        "        teacher_forcing_ratio (float): Probability of using teacher forcing.\n",
        "        target_vocab (Vocabulary): Vocabulary for the target language.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Average epoch loss and training accuracy (0.0 for this simplified version).\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    if len(dataloader) == 0: return 0.0, 0.0\n",
        "\n",
        "    for batch_data in tqdm(dataloader, desc=\"Training\", leave=False):\n",
        "        if batch_data[0] is None: continue\n",
        "        sources, source_lengths, targets = batch_data\n",
        "        if sources is None or sources.shape[0] == 0: continue\n",
        "\n",
        "        sources, targets, source_lengths = sources.to(device), targets.to(device), source_lengths.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs_logits = model(sources, source_lengths, targets, teacher_forcing_ratio)\n",
        "\n",
        "        output_dim = outputs_logits.shape[-1]\n",
        "        flat_outputs = outputs_logits[:, 1:].reshape(-1, output_dim)\n",
        "        flat_targets = targets[:, 1:].reshape(-1)\n",
        "\n",
        "        loss = criterion(flat_outputs, flat_targets)\n",
        "        if not (torch.isnan(loss) or torch.isinf(loss)):\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "    return epoch_loss / len(dataloader) if len(dataloader) > 0 else 0.0, 0.0 # Train accuracy is not computed here\n",
        "\n",
        "\n",
        "def _evaluate_one_epoch(model, dataloader, criterion, device, source_vocab, target_vocab, beam_width=1, is_test_set=False):\n",
        "    \"\"\"\n",
        "    Evaluates the model's performance on a given dataset (validation or test).\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The Seq2Seq model.\n",
        "        dataloader (DataLoader): DataLoader for the evaluation set.\n",
        "        criterion (nn.Module): Loss function.\n",
        "        device (torch.device): Device to run the model on.\n",
        "        source_vocab (Vocabulary): Vocabulary for the source language (used for debug prints).\n",
        "        target_vocab (Vocabulary): Vocabulary for the target language.\n",
        "        beam_width (int): Beam width for decoding (1 for greedy, >1 for beam search).\n",
        "        is_test_set (bool): Flag to indicate if this is the final test evaluation (for debug prints).\n",
        "\n",
        "    Returns:\n",
        "        tuple: Average epoch loss and evaluation accuracy.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "    if len(dataloader) == 0: return 0.0, 0.0\n",
        "    desc_prefix = \"Testing\" if is_test_set else \"Validating\"\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_data in tqdm(dataloader, desc=desc_prefix, leave=False):\n",
        "            if batch_data[0] is None: continue\n",
        "            sources, source_lengths, targets = batch_data\n",
        "            if sources is None or sources.shape[0] == 0: continue\n",
        "\n",
        "            sources, targets, source_lengths = sources.to(device), targets.to(device), source_lengths.to(device)\n",
        "            outputs = model(sources, source_lengths, targets, teacher_forcing_ratio=0.0)\n",
        "            output_dim = outputs.shape[-1]\n",
        "            flat_outputs = outputs[:, 1:].reshape(-1, output_dim)\n",
        "            flat_targets = targets[:, 1:].reshape(-1)\n",
        "            loss = criterion(flat_outputs, flat_targets)\n",
        "            epoch_loss += loss.item() if not (torch.isnan(loss) or torch.isinf(loss)) else 0\n",
        "\n",
        "            # For accuracy, we need to generate predictions (greedy or beam search)\n",
        "            for i in range(targets.shape[0]):\n",
        "                src_single, src_len_single = sources[i:i+1], source_lengths[i:i+1]\n",
        "\n",
        "                # Check if predict_with_attention exists for attention models, otherwise use predict_greedy\n",
        "                if hasattr(model, 'predict_with_attention') and beam_width == 1: # Use greedy for attention visualization\n",
        "                    predicted_indices, _ = model.predict_with_attention(\n",
        "                        src_single, src_len_single,\n",
        "                        max_output_len=targets.size(1) + 5,\n",
        "                        target_eos_idx=target_vocab.eos_idx\n",
        "                    )\n",
        "                elif hasattr(model, 'predict_beam_search') and beam_width > 1:\n",
        "                    predicted_indices = model.predict_beam_search(\n",
        "                        src_single, src_len_single,\n",
        "                        max_output_len=targets.size(1) + 5,\n",
        "                        target_eos_idx=target_vocab.eos_idx,\n",
        "                        beam_width=beam_width\n",
        "                    )\n",
        "                else: # Fallback for non-attention or unexpected cases\n",
        "                    # This branch should not be reached if Seq2Seq or Seq2SeqWithAttention is used correctly.\n",
        "                    # This part could be model.predict_greedy if implemented directly in Seq2Seq base class\n",
        "                    # For current `Seq2SeqWithAttention`, this is not directly available, but for `Seq2Seq` it is.\n",
        "                    # To be safe, we'll assume the model has *some* prediction method for accuracy calculation.\n",
        "                    # Or, as a last resort, extract from `outputs` (which is teacher-forced for loss)\n",
        "                    predicted_indices = outputs[i:i+1].argmax(dim=2)[0, 1:].tolist()\n",
        "\n",
        "                pred_str = target_vocab.indices_to_sequence(predicted_indices, strip_special=True)\n",
        "                true_str = target_vocab.indices_to_sequence(targets[i, 1:].tolist(), strip_special=True)\n",
        "\n",
        "                if pred_str == true_str: total_correct += 1\n",
        "                total_samples += 1\n",
        "\n",
        "                if is_test_set and batch_idx == 0 and i < 3:\n",
        "                    print(f\"  Test Example {i} - Source: '{source_vocab.indices_to_sequence(src_single[0].tolist(), strip_special=True)}'\")\n",
        "                    print(f\"    Pred: '{pred_str}', True: '{true_str}', Match: {pred_str == true_str}\")\n",
        "\n",
        "    accuracy = total_correct / total_samples if total_samples > 0 else 0.0\n",
        "    return epoch_loss / len(dataloader) if len(dataloader) > 0 else 0.0, accuracy\n",
        "\n",
        "# --- Function to Train and Save the Best Attention Model ---\n",
        "\n",
        "def train_and_save_attention_model(config_params, model_save_path, device):\n",
        "    \"\"\"\n",
        "    Performs a dedicated training run using the best hyperparameters for the attention model.\n",
        "    Saves the model checkpoint with the best validation accuracy.\n",
        "\n",
        "    Args:\n",
        "        config_params (dict): Dictionary of hyperparameters for this specific training run.\n",
        "        model_save_path (str): Path to save the best model's state_dict.\n",
        "        device (torch.device): Device to run the training on.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if training was successful and a model was saved, False otherwise.\n",
        "    \"\"\"\n",
        "    run_name_train_best_attn = f\"TRAIN_BEST_ATTN_{config_params['cell_type']}_emb{config_params['embedding_dim']}_hid{config_params['hidden_dim']}\"\n",
        "\n",
        "    with wandb.init(project=\"DL_A3_Attention_Training\", name=run_name_train_best_attn, config=config_params, job_type=\"training_best_attention_model\", reinit=True) as run:\n",
        "        cfg = wandb.config\n",
        "        print(f\"Starting dedicated training for BEST ATTENTION model with config: {cfg}\")\n",
        "\n",
        "        # --- Data Loading and Vocabulary Building ---\n",
        "        BASE_DATA_DIR = \"/kaggle/input/dakshina-dl-a3/dakshina_dataset_v1.0/hi/\"\n",
        "        DATA_DIR = os.path.join(BASE_DATA_DIR, \"lexicons/\")\n",
        "        train_file = os.path.join(DATA_DIR, \"hi.translit.sampled.train.tsv\")\n",
        "        dev_file = os.path.join(DATA_DIR, \"hi.translit.sampled.dev.tsv\")\n",
        "\n",
        "        source_vocab = Vocabulary(\"latin\")\n",
        "        target_vocab = Vocabulary(\"devanagari\")\n",
        "        temp_train_ds_vocab = TransliterationDataset(train_file, source_vocab, target_vocab, max_len=cfg.max_seq_len)\n",
        "        if not temp_train_ds_vocab.pairs:\n",
        "            print(f\"ERROR: No data for vocab from {train_file}\")\n",
        "            return False\n",
        "        for src, tgt in temp_train_ds_vocab.pairs:\n",
        "            source_vocab.add_sequence(src)\n",
        "            target_vocab.add_sequence(tgt)\n",
        "        source_vocab.build_vocab(min_freq=cfg.vocab_min_freq)\n",
        "        target_vocab.build_vocab(min_freq=cfg.vocab_min_freq)\n",
        "\n",
        "        train_dataset = TransliterationDataset(train_file, source_vocab, target_vocab, max_len=cfg.max_seq_len)\n",
        "        dev_dataset = TransliterationDataset(dev_file, source_vocab, target_vocab, max_len=cfg.max_seq_len)\n",
        "        if not train_dataset.pairs or not dev_dataset.pairs:\n",
        "            print(f\"ERROR: Train or Dev dataset empty. Train: {len(train_dataset)}, Dev: {len(dev_dataset)}\")\n",
        "            return False\n",
        "\n",
        "        # --- DataLoader Setup ---\n",
        "        num_w = 0 if device.type == 'cpu' else min(4, os.cpu_count()//2 if os.cpu_count() else 0)\n",
        "        train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size_train, shuffle=True,\n",
        "                                  collate_fn=lambda b: collate_fn(b, source_vocab.pad_idx, target_vocab.pad_idx),\n",
        "                                  num_workers=num_w, pin_memory=True if device.type == 'cuda' else False, drop_last=True)\n",
        "        dev_loader = DataLoader(dev_dataset, batch_size=cfg.batch_size_train, shuffle=False,\n",
        "                                collate_fn=lambda b: collate_fn(b, source_vocab.pad_idx, target_vocab.pad_idx),\n",
        "                                num_workers=num_w, pin_memory=True if device.type == 'cuda' else False)\n",
        "        if not train_loader or len(train_loader)==0 or not dev_loader or len(dev_loader)==0:\n",
        "            print(f\"ERROR: Train/Dev Dataloader empty. Train: {len(train_loader)}, Dev: {len(dev_loader)}\")\n",
        "            return False\n",
        "\n",
        "        # --- Model, Optimizer, Loss Function Setup ---\n",
        "        encoder_hidden_dim_eff = cfg.hidden_dim * (2 if cfg.encoder_bidirectional else 1)\n",
        "        encoder = Encoder(source_vocab.n_chars, cfg.embedding_dim, cfg.hidden_dim,\n",
        "                          cfg.encoder_layers, cfg.cell_type, cfg.dropout_p,\n",
        "                          cfg.encoder_bidirectional, pad_idx=source_vocab.pad_idx).to(device)\n",
        "        decoder = DecoderWithAttention(target_vocab.n_chars, cfg.embedding_dim, encoder_hidden_dim_eff,\n",
        "                                       cfg.hidden_dim, cfg.decoder_layers, cfg.cell_type,\n",
        "                                       cfg.dropout_p, pad_idx=target_vocab.pad_idx).to(device)\n",
        "        model = Seq2SeqWithAttention(encoder, decoder, device, target_vocab.sos_idx).to(device)\n",
        "        print(f\"Best Attention Model parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
        "        wandb.watch(model, log=\"all\", log_freq=100)\n",
        "\n",
        "        optimizer = optim.Adam(model.parameters(), lr=cfg.learning_rate_train)\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=cfg.lr_scheduler_patience_train, factor=0.3, verbose=True)\n",
        "        criterion = nn.CrossEntropyLoss(ignore_index=target_vocab.pad_idx)\n",
        "\n",
        "        best_val_acc_this_training = -1.0\n",
        "        epochs_no_improve = 0\n",
        "        max_epochs_no_improve_val = cfg.get('max_epochs_no_improve_train', 7)\n",
        "\n",
        "        for epoch in range(cfg.epochs_train):\n",
        "            train_loss, _ = _train_one_epoch(model, train_loader, optimizer, criterion, device,\n",
        "                                             cfg.clip_value_train, cfg.teacher_forcing_train, target_vocab)\n",
        "            val_loss, val_acc = _evaluate_one_epoch(model, dev_loader, criterion, device,\n",
        "                                                    source_vocab, target_vocab, beam_width=1, is_test_set=False)\n",
        "\n",
        "            scheduler.step(val_acc)\n",
        "            print(f\"Epoch {epoch+1}/{cfg.epochs_train} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "            wandb.log({\"epoch_train_best_attn\": epoch + 1, \"train_loss_best_attn\": train_loss,\n",
        "                       \"val_loss_best_attn\": val_loss, \"val_acc_best_attn\": val_acc, \"lr_best_attn\": optimizer.param_groups[0]['lr']})\n",
        "\n",
        "            # Early stopping logic\n",
        "            if val_acc > best_val_acc_this_training:\n",
        "                best_val_acc_this_training = val_acc\n",
        "                epochs_no_improve = 0\n",
        "                torch.save(model.state_dict(), model_save_path)\n",
        "                print(f\"Saved new best attention model to {model_save_path} (Val Acc: {best_val_acc_this_training:.4f})\")\n",
        "            else:\n",
        "                epochs_no_improve += 1\n",
        "\n",
        "            if epochs_no_improve >= max_epochs_no_improve_val:\n",
        "                print(f\"Early stopping for best attention model training at epoch {epoch+1}.\")\n",
        "                break\n",
        "\n",
        "        # Save the final model state if no improvement happened over the initial checkpoint\n",
        "        if not os.path.exists(model_save_path):\n",
        "            torch.save(model.state_dict(), model_save_path)\n",
        "            print(f\"Saved final attention model state to {model_save_path}\")\n",
        "\n",
        "        wandb.summary[\"final_best_val_accuracy_during_attn_training\"] = best_val_acc_this_training\n",
        "        print(f\"Finished training best attention model. Best Val Acc during this training: {best_val_acc_this_training:.4f}\")\n",
        "    return True\n",
        "\n",
        "\n",
        "# --- Function to Generate Interactive HTML for W&B ---\n",
        "\n",
        "def generate_interactive_attention_html(examples_data):\n",
        "    \"\"\"\n",
        "    Generates an HTML string for interactive attention visualization.\n",
        "    The HTML includes JavaScript to display source and target characters,\n",
        "    and highlight source characters based on attention weights when hovering over target characters.\n",
        "\n",
        "    Args:\n",
        "        examples_data (list): A list of dictionaries, each containing:\n",
        "                              - 'name': Name of the example (e.g., \"Source -> Target\").\n",
        "                              - 'sourceChars': List of source characters.\n",
        "                              - 'targetChars': List of predicted target characters.\n",
        "                              - 'attentionMatrix': 2D list of attention weights (rows=target, cols=source).\n",
        "\n",
        "    Returns:\n",
        "        str: The complete HTML content as a string.\n",
        "    \"\"\"\n",
        "    # Embed Python data as JSON string directly into JavaScript\n",
        "    escaped_examples_data_str = json.dumps(examples_data)\n",
        "\n",
        "    html_content = f\"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html lang=\"en\">\n",
        "<head>\n",
        "    <meta charset=\"UTF-8\">\n",
        "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
        "    <title>Interactive Attention Visualization</title>\n",
        "    <script src=\"https://cdn.tailwindcss.com\"></script>\n",
        "    <style>\n",
        "        body {{ font-family: 'Inter', sans-serif; display: flex; flex-direction: column; align-items: center; padding: 20px; background-color: #f9fafb; }}\n",
        "        .container {{ background-color: white; padding: 2rem; border-radius: 0.75rem; box-shadow: 0 10px 15px -3px rgba(0,0,0,0.1), 0 4px 6px -2px rgba(0,0,0,0.05); width: 100%; max-width: 900px; }}\n",
        "        .word-display {{ display: flex; flex-wrap: wrap; margin-bottom: 1rem; padding: 0.5rem; border-radius: 0.5rem; background-color: #f3f4f6; min-height: 40px; align-items: center; justify-content: flex-start; }}\n",
        "        .char-box {{ display: inline-flex; align-items: center; justify-content: center; padding: 0.3rem 0.6rem; margin: 0.15rem; border: 1px solid #d1d5db; border-radius: 0.375rem; font-size: 1.25rem; min-width: 30px; height: 40px; text-align: center; transition: background-color 0.1s ease-in-out; }}\n",
        "        .input-char {{ background-color: #ffffff; }}\n",
        "        .output-char {{ background-color: #eff6ff; cursor: pointer; }}\n",
        "        .output-char:hover {{ background-color: #dbeafe; }}\n",
        "        .devanagari {{ font-family: 'Arial Unicode MS', 'Noto Sans Devanagari', sans-serif; }}\n",
        "        .highlight-base {{ background-color: #6ee7b7; }} /* Tailwind green-300 base */\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "    <div class=\"container\">\n",
        "        <h1 class=\"text-2xl font-semibold text-gray-800 mb-4\">Interactive Attention Connectivity</h1>\n",
        "        <div class=\"controls\">\n",
        "            <label for=\"exampleSelectViz\" class=\"mr-2 text-gray-700\">Select Example:</label>\n",
        "            <select id=\"exampleSelectViz\"></select>\n",
        "        </div>\n",
        "        <div class=\"text-lg font-medium text-gray-700 mb-1\">Input (Latin):</div>\n",
        "        <div id=\"inputSequenceViz\" class=\"word-display\"></div>\n",
        "        <div class=\"text-lg font-medium text-gray-700 mt-4 mb-1\">Predicted Output (Devanagari):</div>\n",
        "        <div id=\"outputSequenceViz\" class=\"word-display devanagari\"></div>\n",
        "        <p class=\"text-sm text-gray-600 mt-2\">Hover over a Devanagari character to see its attention to Latin characters (highlighted green).</p>\n",
        "    </div>\n",
        "    <script>\n",
        "        const examplesData = {escaped_examples_data_str};\n",
        "\n",
        "        const inputDiv = document.getElementById('inputSequenceViz');\n",
        "        const outputDiv = document.getElementById('outputSequenceViz');\n",
        "        const selectEl = document.getElementById('exampleSelectViz');\n",
        "\n",
        "        function displayExample(index) {{\n",
        "            const example = examplesData[index];\n",
        "            inputDiv.innerHTML = '';\n",
        "            outputDiv.innerHTML = '';\n",
        "            const inputSpans = [];\n",
        "\n",
        "            example.sourceChars.forEach(char => {{\n",
        "                const span = document.createElement('span');\n",
        "                span.className = 'char-box input-char';\n",
        "                span.textContent = char;\n",
        "                inputDiv.appendChild(span);\n",
        "                inputSpans.push(span);\n",
        "            }});\n",
        "\n",
        "            example.targetChars.forEach((char, targetIdx) => {{\n",
        "                const span = document.createElement('span');\n",
        "                span.className = 'char-box output-char devanagari';\n",
        "                span.textContent = char;\n",
        "                span.addEventListener('mouseover', () => {{\n",
        "                    if (targetIdx < example.attentionMatrix.length) {{\n",
        "                        const weights = example.attentionMatrix[targetIdx];\n",
        "                        inputSpans.forEach((inputSpan, inputIdx) => {{\n",
        "                            if (inputIdx < weights.length) {{\n",
        "                                const weight = weights[inputIdx];\n",
        "                                inputSpan.style.backgroundColor = `rgba(52, 211, 153, ${{weight * 0.8 + 0.2}})`; // Tailwind green-400 with opacity\n",
        "                            }}\n",
        "                        }});\n",
        "                    }}\n",
        "                }});\n",
        "                span.addEventListener('mouseout', () => {{\n",
        "                    inputSpans.forEach(s => s.style.backgroundColor = '#ffffff');\n",
        "                }});\n",
        "                outputDiv.appendChild(span);\n",
        "            }});\n",
        "        }}\n",
        "\n",
        "        examplesData.forEach((ex, i) => {{\n",
        "            const option = document.createElement('option');\n",
        "            option.value = i;\n",
        "            option.textContent = ex.name || `Example ${{i + 1}}`;\n",
        "            selectEl.appendChild(option);\n",
        "        }});\n",
        "        selectEl.addEventListener('change', (e) => displayExample(e.target.value));\n",
        "        if (examplesData.length > 0) {{\n",
        "            displayExample(0); // Load first example\n",
        "        }}\n",
        "    </script>\n",
        "</body>\n",
        "</html>\n",
        "    \"\"\"\n",
        "    return html_content\n",
        "\n",
        "# --- Main Execution Block for Attention Model Training & Visualization ---\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "    # --- W&B Login ---\n",
        "    try:\n",
        "        if 'KAGGLE_KERNEL_RUN_TYPE' in os.environ:\n",
        "            print(\"Detected Kaggle environment. Ensuring WANDB_API_KEY is set.\")\n",
        "            if \"WANDB_API_KEY\" in os.environ:\n",
        "                wandb.login()\n",
        "                print(\"W&B login using environment variable.\")\n",
        "            else:\n",
        "                from kaggle_secrets import UserSecretsClient\n",
        "                user_secrets = UserSecretsClient()\n",
        "                wandb_api_key = user_secrets.get_secret(\"WANDB_API_KEY\")\n",
        "                wandb.login(key=wandb_api_key)\n",
        "                print(\"W&B login for Kaggle successful.\")\n",
        "        else:\n",
        "            wandb.login()\n",
        "        print(\"W&B login process attempted/completed.\")\n",
        "    except Exception as e:\n",
        "        print(f\"W&B login failed: {e}. Ensure API key is configured.\")\n",
        "        wandb = None # Disable wandb if login fails\n",
        "\n",
        "    # --- Best Attention Model Hyperparameters (UPDATE THIS SECTION) ---\n",
        "    # These hyperparameters should come from your W&B sweep for the Attention Model.\n",
        "    BEST_ATTN_HYPERPARAMETERS = {\n",
        "        'embedding_dim': 256,\n",
        "        'hidden_dim': 512,\n",
        "        'encoder_layers': 1,\n",
        "        'decoder_layers': 1,\n",
        "        'cell_type': 'LSTM',\n",
        "        'dropout_p': 0.3,\n",
        "        'encoder_bidirectional': True,\n",
        "        'learning_rate_train': 0.0008,\n",
        "        'batch_size_train': 64,\n",
        "        'epochs_train': 20,\n",
        "        'clip_value_train': 1.0,\n",
        "        'teacher_forcing_train': 0.5,\n",
        "        'max_epochs_no_improve_train': 7,\n",
        "        'lr_scheduler_patience_train': 3,\n",
        "        'vocab_min_freq': 1,\n",
        "        'max_seq_len': 50,\n",
        "        'eval_batch_size': 64,\n",
        "    }\n",
        "    ATTN_MODEL_SAVE_PATH = \"/kaggle/working/best_attention_model_q5.pt\"\n",
        "\n",
        "    print(f\"Target hyperparameters for best attention model: {BEST_ATTN_HYPERPARAMETERS}\")\n",
        "\n",
        "    # --- Phase 1: Train and Save the Best Attention Model (if checkpoint doesn't exist) ---\n",
        "    if not os.path.exists(ATTN_MODEL_SAVE_PATH):\n",
        "        print(f\"\\n--- Attention Model Checkpoint NOT FOUND at {ATTN_MODEL_SAVE_PATH} ---\")\n",
        "        print(\"--- Attempting to TRAIN AND SAVE the best attention model using BEST_ATTN_HYPERPARAMETERS ---\")\n",
        "        training_successful = train_and_save_attention_model(BEST_ATTN_HYPERPARAMETERS, ATTN_MODEL_SAVE_PATH, DEVICE)\n",
        "        if not training_successful or not os.path.exists(ATTN_MODEL_SAVE_PATH):\n",
        "            print(\"ERROR: Failed to train and save the best attention model. Exiting.\")\n",
        "            exit()\n",
        "        print(f\"Best attention model trained and saved to {ATTN_MODEL_SAVE_PATH}\")\n",
        "    else:\n",
        "        print(f\"Found existing attention model checkpoint at {ATTN_MODEL_SAVE_PATH}. Will use this for visualization.\")\n",
        "\n",
        "    # --- Phase 2: Load Model and Prepare for Interactive Visualization ---\n",
        "    print(\"\\n--- Preparing for Interactive Attention Visualization ---\")\n",
        "\n",
        "    BASE_DATA_DIR = \"/kaggle/input/dakshina-dl-a3/dakshina_dataset_v1.0/hi/\"\n",
        "    DATA_DIR = os.path.join(BASE_DATA_DIR, \"lexicons/\")\n",
        "    train_file_for_vocab = os.path.join(DATA_DIR, \"hi.translit.sampled.train.tsv\")\n",
        "    test_file = os.path.join(DATA_DIR, \"hi.translit.sampled.test.tsv\")\n",
        "\n",
        "    source_vocab = Vocabulary(\"latin_attn_viz\")\n",
        "    target_vocab = Vocabulary(\"devanagari_attn_viz\")\n",
        "\n",
        "    temp_train_ds_attn_vocab = TransliterationDataset(train_file_for_vocab, source_vocab, target_vocab,\n",
        "                                                max_len=BEST_ATTN_HYPERPARAMETERS['max_seq_len'])\n",
        "    if not temp_train_ds_attn_vocab.pairs: exit()\n",
        "    for src, tgt in temp_train_ds_attn_vocab.pairs:\n",
        "        source_vocab.add_sequence(src)\n",
        "        target_vocab.add_sequence(tgt)\n",
        "    source_vocab.build_vocab(min_freq=BEST_ATTN_HYPERPARAMETERS['vocab_min_freq'])\n",
        "    target_vocab.build_vocab(min_freq=BEST_ATTN_HYPERPARAMETERS['vocab_min_freq'])\n",
        "\n",
        "    test_dataset_for_viz = TransliterationDataset(test_file, source_vocab, target_vocab,\n",
        "                                                  max_len=BEST_ATTN_HYPERPARAMETERS['max_seq_len'])\n",
        "    if not test_dataset_for_viz.pairs:\n",
        "        print(\"Test dataset for visualization is empty. Exiting.\")\n",
        "        exit()\n",
        "\n",
        "    encoder_hidden_dim_eff_test = BEST_ATTN_HYPERPARAMETERS['hidden_dim'] * (2 if BEST_ATTN_HYPERPARAMETERS['encoder_bidirectional'] else 1)\n",
        "    encoder = Encoder(source_vocab.n_chars, BEST_ATTN_HYPERPARAMETERS['embedding_dim'], BEST_ATTN_HYPERPARAMETERS['hidden_dim'],\n",
        "                      BEST_ATTN_HYPERPARAMETERS['encoder_layers'], BEST_ATTN_HYPERPARAMETERS['cell_type'], BEST_ATTN_HYPERPARAMETERS['dropout_p'],\n",
        "                      BEST_ATTN_HYPERPARAMETERS['encoder_bidirectional'], pad_idx=source_vocab.pad_idx).to(DEVICE)\n",
        "    decoder = DecoderWithAttention(target_vocab.n_chars, BEST_ATTN_HYPERPARAMETERS['embedding_dim'],\n",
        "                                   encoder_hidden_dim_eff_test, BEST_ATTN_HYPERPARAMETERS['hidden_dim'],\n",
        "                                   BEST_ATTN_HYPERPARAMETERS['decoder_layers'], BEST_ATTN_HYPERPARAMETERS['cell_type'], BEST_ATTN_HYPERPARAMETERS['dropout_p'],\n",
        "                                   pad_idx=target_vocab.pad_idx).to(DEVICE)\n",
        "    model = Seq2SeqWithAttention(encoder, decoder, DEVICE, target_vocab.sos_idx).to(DEVICE)\n",
        "\n",
        "    print(f\"Loading weights into attention model from: {ATTN_MODEL_SAVE_PATH}\")\n",
        "    model.load_state_dict(torch.load(ATTN_MODEL_SAVE_PATH, map_location=DEVICE))\n",
        "    print(\"Attention model weights loaded successfully.\")\n",
        "    model.eval() # Set model to evaluation mode\n",
        "\n",
        "    # --- Generate Data for Interactive Visualization ---\n",
        "    num_viz_samples = 10\n",
        "    actual_num_viz_samples = min(num_viz_samples, len(test_dataset_for_viz.pairs))\n",
        "    sample_indices_viz = []\n",
        "    if len(test_dataset_for_viz.pairs) > 0:\n",
        "        sample_indices_viz = random.sample(range(len(test_dataset_for_viz.pairs)), actual_num_viz_samples)\n",
        "\n",
        "    examples_for_html = []\n",
        "    print(f\"\\n--- Generating Data for {len(sample_indices_viz)} Interactive Visualizations ---\")\n",
        "\n",
        "    for i, data_idx in enumerate(sample_indices_viz):\n",
        "        source_str, true_target_str = test_dataset_for_viz.pairs[data_idx]\n",
        "\n",
        "        source_indices_for_pred = source_vocab.sequence_to_indices(source_str, add_eos=True, max_len=BEST_ATTN_HYPERPARAMETERS['max_seq_len'])\n",
        "        source_tensor = torch.tensor(source_indices_for_pred, dtype=torch.long).unsqueeze(0).to(DEVICE)\n",
        "        source_length = torch.tensor([len(source_indices_for_pred)], dtype=torch.long).to(DEVICE)\n",
        "\n",
        "        # Get predicted indices and attention matrix\n",
        "        predicted_indices, attention_matrix_np = model.predict_with_attention(\n",
        "            source_tensor,\n",
        "            source_length,\n",
        "            max_output_len=len(true_target_str) + 10, # Generate slightly longer than true target\n",
        "            target_eos_idx=target_vocab.eos_idx\n",
        "        )\n",
        "\n",
        "        # Get characters for display (strip all special tokens for clean display)\n",
        "        display_source_chars = list(source_vocab.indices_to_sequence(source_indices_for_pred, strip_special=True))\n",
        "        display_target_chars = list(target_vocab.indices_to_sequence(predicted_indices, strip_special=True))\n",
        "\n",
        "        # Align attention matrix with the displayed characters\n",
        "        final_attention_matrix_for_js = []\n",
        "        if attention_matrix_np is not None and \\\n",
        "           len(display_target_chars) > 0 and \\\n",
        "           len(display_source_chars) > 0 and \\\n",
        "           attention_matrix_np.shape[0] >= len(display_target_chars) and \\\n",
        "           attention_matrix_np.shape[1] >= len(display_source_chars) :\n",
        "\n",
        "            # Slice attention matrix to correspond to the displayed non-special tokens\n",
        "            num_rows_to_take = len(display_target_chars)\n",
        "            num_cols_to_take = len(display_source_chars)\n",
        "            final_attention_matrix_for_js = attention_matrix_np[:num_rows_to_take, :num_cols_to_take].tolist()\n",
        "        else:\n",
        "            print(f\"  Skipping example '{source_str}' due to attention matrix dimension mismatch or empty strings.\")\n",
        "            print(f\"  Attn shape: {attention_matrix_np.shape if attention_matrix_np is not None else 'None'}, \"\n",
        "                  f\"display_target_len: {len(display_target_chars)}, display_source_len: {len(display_source_chars)}\")\n",
        "            continue # Skip this example if dimensions don't align\n",
        "\n",
        "        examples_for_html.append({\n",
        "            \"name\": f\"Ex {i+1}: {source_str} -> {true_target_str}\",\n",
        "            \"sourceChars\": display_source_chars,\n",
        "            \"targetChars\": display_target_chars,\n",
        "            \"attentionMatrix\": final_attention_matrix_for_js\n",
        "        })\n",
        "\n",
        "    # --- Generate and Log Interactive HTML to W&B ---\n",
        "    if examples_for_html:\n",
        "        interactive_html_content = generate_interactive_attention_html(examples_for_html)\n",
        "\n",
        "        # Ensure a W&B run is active for logging\n",
        "        if wandb and wandb.run is None:\n",
        "            wandb.init(project=\"DL_A3\", name=\"Q6_Interactive_Attention_Viz\", config=BEST_ATTN_HYPERPARAMETERS, job_type=\"q6_visualization\", reinit=True)\n",
        "\n",
        "        if wandb and wandb.run:\n",
        "            wandb.log({\"interactive_attention_visualization\": wandb.Html(interactive_html_content)})\n",
        "            print(\"\\nLogged interactive attention visualization to W&B.\")\n",
        "\n",
        "            # Save HTML locally for inspection\n",
        "            with open(\"/kaggle/working/interactive_attention_viz.html\", \"w\", encoding=\"utf-8\") as f_html:\n",
        "                f_html.write(interactive_html_content)\n",
        "            print(\"Saved interactive HTML to /kaggle/working/interactive_attention_viz.html\")\n",
        "\n",
        "            # Log the generated HTML file as an artifact\n",
        "            html_artifact = wandb.Artifact(\"interactive_attention_html\", type=\"visualization\")\n",
        "            html_artifact.add_file(\"/kaggle/working/interactive_attention_viz.html\")\n",
        "            wandb.log_artifact(html_artifact)\n",
        "            print(\"Logged HTML file as a W&B artifact.\")\n",
        "\n",
        "        else:\n",
        "            print(\"W&B run not active. Could not log interactive HTML.\")\n",
        "            # Save HTML locally if W&B is not active\n",
        "            with open(\"/kaggle/working/interactive_attention_viz.html\", \"w\", encoding=\"utf-8\") as f_html:\n",
        "                f_html.write(interactive_html_content)\n",
        "            print(\"W&B not active. Saved interactive HTML locally to /kaggle/working/interactive_attention_viz.html\")\n",
        "    else:\n",
        "        print(\"No valid examples with attention data were generated for the interactive HTML.\")\n",
        "\n",
        "    # Finish the W&B run if one was started\n",
        "    if wandb and wandb.run: wandb.finish()\n",
        "    print(\"Script finished.\")"
      ],
      "metadata": {
        "id": "Jj29nUreTr4U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}