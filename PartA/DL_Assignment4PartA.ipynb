{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lndoaWz5mFCH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 2"
      ],
      "metadata": {
        "id": "bLfVKCU5mFho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import random\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Encoder part of the seq2seq model.\n",
        "    It takes a sequence of input characters and produces a context vector.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_vocab_size, embedding_dim, hidden_dim, n_layers,\n",
        "                 cell_type='LSTM', dropout_p=0.1, bidirectional=False):\n",
        "        \"\"\"\n",
        "        Initializes the Encoder.\n",
        "        \"\"\"\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.input_vocab_size = input_vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.cell_type = cell_type.upper()\n",
        "        self.bidirectional = bidirectional\n",
        "        self.num_directions = 2 if bidirectional else 1\n",
        "\n",
        "        self.embedding = nn.Embedding(input_vocab_size, embedding_dim)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "        rnn_dropout = dropout_p if n_layers > 1 else 0\n",
        "        if self.cell_type == 'RNN':\n",
        "            self.rnn = nn.RNN(embedding_dim, hidden_dim, n_layers,\n",
        "                              dropout=rnn_dropout, batch_first=True, bidirectional=self.bidirectional)\n",
        "        elif self.cell_type == 'GRU':\n",
        "            self.rnn = nn.GRU(embedding_dim, hidden_dim, n_layers,\n",
        "                              dropout=rnn_dropout, batch_first=True, bidirectional=self.bidirectional)\n",
        "        elif self.cell_type == 'LSTM':\n",
        "            self.rnn = nn.LSTM(embedding_dim, hidden_dim, n_layers,\n",
        "                               dropout=rnn_dropout, batch_first=True, bidirectional=self.bidirectional)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported cell type. Choose from 'RNN', 'GRU', 'LSTM'.\")\n",
        "\n",
        "    def forward(self, input_seq):\n",
        "        \"\"\"\n",
        "        Forward pass of the encoder.\n",
        "        \"\"\"\n",
        "        embedded = self.embedding(input_seq)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        outputs, hidden = self.rnn(embedded)\n",
        "\n",
        "        return outputs, hidden\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Decoder part of the seq2seq model.\n",
        "    It takes the encoder's context vector and generates an output sequence.\n",
        "    \"\"\"\n",
        "    def __init__(self, output_vocab_size, embedding_dim, hidden_dim, n_layers,\n",
        "                 cell_type='LSTM', dropout_p=0.1, encoder_bidirectional=False):\n",
        "        \"\"\"\n",
        "        Initializes the Decoder.\n",
        "        \"\"\"\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.output_vocab_size = output_vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.cell_type = cell_type.upper()\n",
        "        self.encoder_bidirectional = encoder_bidirectional\n",
        "\n",
        "\n",
        "        self.embedding = nn.Embedding(output_vocab_size, embedding_dim)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "        rnn_dropout = dropout_p if n_layers > 1 else 0\n",
        "        if self.cell_type == 'RNN':\n",
        "            self.rnn = nn.RNN(embedding_dim, hidden_dim, n_layers,\n",
        "                              dropout=rnn_dropout, batch_first=True)\n",
        "        elif self.cell_type == 'GRU':\n",
        "            self.rnn = nn.GRU(embedding_dim, hidden_dim, n_layers,\n",
        "                              dropout=rnn_dropout, batch_first=True)\n",
        "        elif self.cell_type == 'LSTM':\n",
        "            self.rnn = nn.LSTM(embedding_dim, hidden_dim, n_layers,\n",
        "                               dropout=rnn_dropout, batch_first=True)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported cell type. Choose from 'RNN', 'GRU', 'LSTM'.\")\n",
        "\n",
        "        self.fc_out = nn.Linear(hidden_dim, output_vocab_size)\n",
        "\n",
        "    def forward(self, input_char, hidden_state):\n",
        "        \"\"\"\n",
        "        Forward pass for a single decoding step.\n",
        "        \"\"\"\n",
        "        input_char = input_char.unsqueeze(1)\n",
        "\n",
        "        embedded = self.embedding(input_char)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        rnn_output, new_hidden_state = self.rnn(embedded, hidden_state)\n",
        "\n",
        "        rnn_output_squeezed = rnn_output.squeeze(1)\n",
        "\n",
        "        prediction = self.fc_out(rnn_output_squeezed)\n",
        "\n",
        "        return prediction, new_hidden_state\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    \"\"\"\n",
        "    The main Seq2Seq model that combines Encoder and Decoder.\n",
        "    \"\"\"\n",
        "    def __init__(self, encoder, decoder, device, target_sos_idx):\n",
        "        \"\"\"\n",
        "        Initializes the Seq2Seq model.\n",
        "        \"\"\"\n",
        "        super(Seq2Seq, self).__init__()\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "        self.target_sos_idx = target_sos_idx\n",
        "\n",
        "        if self.encoder.bidirectional:\n",
        "            encoder_hidden_dim_actual = self.encoder.hidden_dim * self.encoder.num_directions\n",
        "            decoder_hidden_dim_expected = self.decoder.hidden_dim\n",
        "\n",
        "            self.fc_hidden = nn.Linear(encoder_hidden_dim_actual, decoder_hidden_dim_expected)\n",
        "            if self.encoder.cell_type == 'LSTM':\n",
        "                self.fc_cell = nn.Linear(encoder_hidden_dim_actual, decoder_hidden_dim_expected)\n",
        "\n",
        "\n",
        "    def _adapt_encoder_hidden(self, encoder_hidden):\n",
        "        \"\"\"\n",
        "        Adapts the encoder's final hidden state to be suitable as the decoder's initial hidden state.\n",
        "        Handles bidirectional encoders by combining forward and backward states.\n",
        "        \"\"\"\n",
        "        if not self.encoder.bidirectional:\n",
        "            return encoder_hidden\n",
        "\n",
        "        if self.encoder.cell_type == 'LSTM':\n",
        "            h_n, c_n = encoder_hidden\n",
        "\n",
        "            h_n = h_n.view(self.encoder.n_layers, self.encoder.num_directions, h_n.size(1), self.encoder.hidden_dim)\n",
        "            c_n = c_n.view(self.encoder.n_layers, self.encoder.num_directions, c_n.size(1), self.encoder.hidden_dim)\n",
        "\n",
        "            h_n_cat = torch.cat((h_n[:, 0, :, :], h_n[:, 1, :, :]), dim=2)\n",
        "            c_n_cat = torch.cat((c_n[:, 0, :, :], c_n[:, 1, :, :]), dim=2)\n",
        "\n",
        "            adapted_h = self.fc_hidden(h_n_cat)\n",
        "            adapted_c = self.fc_cell(c_n_cat)\n",
        "            return (adapted_h, adapted_c)\n",
        "\n",
        "        else:\n",
        "            h_n = encoder_hidden\n",
        "            h_n = h_n.view(self.encoder.n_layers, self.encoder.num_directions, h_n.size(1), self.encoder.hidden_dim)\n",
        "            h_n_cat = torch.cat((h_n[:, 0, :, :], h_n[:, 1, :, :]), dim=2)\n",
        "            adapted_h = self.fc_hidden(h_n_cat)\n",
        "            return adapted_h\n",
        "\n",
        "\n",
        "    def forward(self, source_seq, target_seq, teacher_forcing_ratio=0.5):\n",
        "        \"\"\"\n",
        "        Forward pass of the Seq2Seq model.\n",
        "        \"\"\"\n",
        "        batch_size = source_seq.shape[0]\n",
        "        target_len = target_seq.shape[1]\n",
        "        target_vocab_size = self.decoder.output_vocab_size\n",
        "\n",
        "        outputs = torch.zeros(batch_size, target_len, target_vocab_size).to(self.device)\n",
        "\n",
        "        _, encoder_final_hidden = self.encoder(source_seq)\n",
        "\n",
        "        decoder_hidden = self._adapt_encoder_hidden(encoder_final_hidden)\n",
        "\n",
        "        decoder_input = target_seq[:, 0]\n",
        "\n",
        "        for t in range(target_len -1):\n",
        "            decoder_output_logits, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
        "\n",
        "            outputs[:, t+1] = decoder_output_logits\n",
        "\n",
        "            teacher_force_this_step = random.random() < teacher_forcing_ratio\n",
        "\n",
        "            top1_predicted_token = decoder_output_logits.argmax(1)\n",
        "\n",
        "            decoder_input = target_seq[:, t+1] if teacher_force_this_step else top1_predicted_token\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def predict(self, source_seq, max_output_len=50):\n",
        "        \"\"\"\n",
        "        Generate a sequence of characters given a source sequence during inference.\n",
        "        No teacher forcing is used.\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        batch_size = source_seq.shape[0]\n",
        "        if batch_size != 1:\n",
        "            raise ValueError(\"Predict function currently supports batch_size=1 for simplicity.\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            _, encoder_final_hidden = self.encoder(source_seq)\n",
        "            decoder_hidden = self._adapt_encoder_hidden(encoder_final_hidden)\n",
        "\n",
        "            decoder_input = torch.tensor([self.target_sos_idx], device=self.device)\n",
        "\n",
        "            predicted_indices = []\n",
        "\n",
        "            for _ in range(max_output_len):\n",
        "                decoder_output_logits, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
        "\n",
        "                top1_predicted_token = decoder_output_logits.argmax(1)\n",
        "                predicted_idx = top1_predicted_token.item()\n",
        "                predicted_indices.append(predicted_idx)\n",
        "\n",
        "                decoder_input = top1_predicted_token\n",
        "\n",
        "        self.train()\n",
        "        return predicted_indices\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    INPUT_VOCAB_SIZE = 50\n",
        "    OUTPUT_VOCAB_SIZE = 60\n",
        "    TARGET_SOS_IDX = 0\n",
        "    TARGET_EOS_IDX = 1\n",
        "\n",
        "    EMBEDDING_DIM = 128\n",
        "    HIDDEN_DIM_ENCODER = 256\n",
        "    HIDDEN_DIM_DECODER = 256\n",
        "    N_LAYERS_ENCODER = 2\n",
        "    N_LAYERS_DECODER = 2\n",
        "    CELL_TYPE = 'LSTM'\n",
        "    DROPOUT = 0.3\n",
        "    ENCODER_BIDIRECTIONAL = True\n",
        "\n",
        "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    encoder = Encoder(INPUT_VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM_ENCODER, N_LAYERS_ENCODER,\n",
        "                      CELL_TYPE, DROPOUT, bidirectional=ENCODER_BIDIRECTIONAL).to(DEVICE)\n",
        "\n",
        "    decoder = Decoder(OUTPUT_VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM_DECODER, N_LAYERS_DECODER,\n",
        "                      CELL_TYPE, DROPOUT, encoder_bidirectional=ENCODER_BIDIRECTIONAL).to(DEVICE)\n",
        "\n",
        "    model = Seq2Seq(encoder, decoder, DEVICE, TARGET_SOS_IDX).to(DEVICE)\n",
        "\n",
        "    print(f\"Model initialized on {DEVICE}\")\n",
        "    print(f\"Encoder cell type: {encoder.cell_type}, Layers: {encoder.n_layers}, Hidden: {encoder.hidden_dim}, Bidirectional: {encoder.bidirectional}\")\n",
        "    print(f\"Decoder cell type: {decoder.cell_type}, Layers: {decoder.n_layers}, Hidden: {decoder.hidden_dim}\")\n",
        "\n",
        "    BATCH_SIZE = 4\n",
        "    SOURCE_SEQ_LEN = 10\n",
        "    TARGET_SEQ_LEN = 12\n",
        "\n",
        "    dummy_source_seq = torch.randint(0, INPUT_VOCAB_SIZE, (BATCH_SIZE, SOURCE_SEQ_LEN)).to(DEVICE)\n",
        "\n",
        "    dummy_target_seq = torch.randint(1, OUTPUT_VOCAB_SIZE, (BATCH_SIZE, TARGET_SEQ_LEN)).to(DEVICE)\n",
        "    dummy_target_seq[:, 0] = TARGET_SOS_IDX\n",
        "\n",
        "    print(f\"\\nDummy source shape: {dummy_source_seq.shape}\")\n",
        "    print(f\"Dummy target shape: {dummy_target_seq.shape}\")\n",
        "\n",
        "    model.train()\n",
        "    output_logits = model(dummy_source_seq, dummy_target_seq, teacher_forcing_ratio=0.5)\n",
        "    print(f\"\\nOutput logits shape from forward pass: {output_logits.shape}\")\n",
        "\n",
        "    model.eval()\n",
        "    dummy_single_source_seq = torch.randint(0, INPUT_VOCAB_SIZE, (1, SOURCE_SEQ_LEN)).to(DEVICE)\n",
        "    predicted_sequence_indices = model.predict(dummy_single_source_seq, max_output_len=TARGET_SEQ_LEN)\n",
        "    print(f\"\\nPredicted sequence indices for a single source: {predicted_sequence_indices}\")\n",
        "    print(f\"Length of predicted sequence: {len(predicted_sequence_indices)}\")\n",
        "\n",
        "    def count_parameters(m):\n",
        "        return sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
        "\n",
        "    print(f'\\nThe model has {count_parameters(model):,} trainable parameters')\n",
        "\n",
        "    print(\"\\n--- Example of changing parameters ---\")\n",
        "    encoder_gru_unidir = Encoder(INPUT_VOCAB_SIZE, embedding_dim=64, hidden_dim=128, n_layers=1,\n",
        "                                 cell_type='GRU', dropout_p=0.1, bidirectional=False).to(DEVICE)\n",
        "    decoder_gru_unidir = Decoder(OUTPUT_VOCAB_SIZE, embedding_dim=64, hidden_dim=128, n_layers=1,\n",
        "                                 cell_type='GRU', dropout_p=0.1, encoder_bidirectional=False).to(DEVICE)\n",
        "    model_gru_unidir = Seq2Seq(encoder_gru_unidir, decoder_gru_unidir, DEVICE, TARGET_SOS_IDX).to(DEVICE)\n",
        "    print(f\"GRU Unidirectional Model initialized on {DEVICE}\")\n",
        "    print(f\"Encoder cell type: {encoder_gru_unidir.cell_type}, Layers: {encoder_gru_unidir.n_layers}, Hidden: {encoder_gru_unidir.hidden_dim}, Bidirectional: {encoder_gru_unidir.bidirectional}\")\n",
        "    print(f\"Decoder cell type: {decoder_gru_unidir.cell_type}, Layers: {decoder_gru_unidir.n_layers}, Hidden: {decoder_gru_unidir.hidden_dim}\")\n",
        "    print(f'The GRU Unidir model has {count_parameters(model_gru_unidir):,} trainable parameters')\n"
      ],
      "metadata": {
        "id": "RUnMFY-ZmM4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q3 - Vanilla Model"
      ],
      "metadata": {
        "id": "CEGuP3HemkC7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import numpy as np\n",
        "import wandb\n",
        "import os\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "import heapq\n",
        "\n",
        "# --- Constants ---\n",
        "SOS_TOKEN = \"<sos>\"\n",
        "EOS_TOKEN = \"<eos>\"\n",
        "PAD_TOKEN = \"<pad>\"\n",
        "UNK_TOKEN = \"<unk>\"\n",
        "\n",
        "# --- Model Definition (Encoder, Decoder, Seq2Seq) ---\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_vocab_size, embedding_dim, hidden_dim, n_layers,\n",
        "                 cell_type='LSTM', dropout_p=0.1, bidirectional=False, pad_idx=0):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.input_vocab_size = input_vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.cell_type = cell_type.upper()\n",
        "        self.bidirectional = bidirectional\n",
        "        self.num_directions = 2 if bidirectional else 1\n",
        "\n",
        "        self.embedding = nn.Embedding(input_vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "        rnn_dropout = dropout_p if n_layers > 1 else 0\n",
        "        if self.cell_type == 'RNN':\n",
        "            self.rnn = nn.RNN(embedding_dim, hidden_dim, n_layers,\n",
        "                              dropout=rnn_dropout, batch_first=True, bidirectional=self.bidirectional)\n",
        "        elif self.cell_type == 'GRU':\n",
        "            self.rnn = nn.GRU(embedding_dim, hidden_dim, n_layers,\n",
        "                              dropout=rnn_dropout, batch_first=True, bidirectional=self.bidirectional)\n",
        "        elif self.cell_type == 'LSTM':\n",
        "            self.rnn = nn.LSTM(embedding_dim, hidden_dim, n_layers,\n",
        "                               dropout=rnn_dropout, batch_first=True, bidirectional=self.bidirectional)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported cell type. Choose from 'RNN', 'GRU', 'LSTM'.\")\n",
        "\n",
        "    def forward(self, input_seq, input_lengths):\n",
        "        embedded = self.embedding(input_seq)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "        packed_outputs, hidden = self.rnn(packed_embedded)\n",
        "        return None, hidden\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_vocab_size, embedding_dim,\n",
        "                 decoder_hidden_dim, n_layers, cell_type='LSTM', dropout_p=0.1, pad_idx=0):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.output_vocab_size = output_vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.decoder_hidden_dim = decoder_hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.cell_type = cell_type.upper()\n",
        "\n",
        "        self.embedding = nn.Embedding(output_vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "        rnn_input_dim = embedding_dim\n",
        "\n",
        "        rnn_dropout = dropout_p if n_layers > 1 else 0\n",
        "        if self.cell_type == 'RNN':\n",
        "            self.rnn = nn.RNN(rnn_input_dim, decoder_hidden_dim, n_layers,\n",
        "                              dropout=rnn_dropout, batch_first=True)\n",
        "        elif self.cell_type == 'GRU':\n",
        "            self.rnn = nn.GRU(rnn_input_dim, decoder_hidden_dim, n_layers,\n",
        "                              dropout=rnn_dropout, batch_first=True)\n",
        "        elif self.cell_type == 'LSTM':\n",
        "            self.rnn = nn.LSTM(rnn_input_dim, decoder_hidden_dim, n_layers,\n",
        "                               dropout=rnn_dropout, batch_first=True)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported cell type. Choose from 'RNN', 'GRU', 'LSTM'.\")\n",
        "\n",
        "        self.fc_out = nn.Linear(decoder_hidden_dim, output_vocab_size)\n",
        "\n",
        "    def forward(self, input_char, prev_decoder_hidden):\n",
        "        input_char = input_char.unsqueeze(1)\n",
        "        embedded = self.embedding(input_char)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        rnn_output, current_decoder_hidden = self.rnn(embedded, prev_decoder_hidden)\n",
        "\n",
        "        rnn_output_squeezed = rnn_output.squeeze(1)\n",
        "        prediction_logits = self.fc_out(rnn_output_squeezed)\n",
        "        return prediction_logits, current_decoder_hidden\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device, target_sos_idx):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "        self.target_sos_idx = target_sos_idx\n",
        "\n",
        "        encoder_effective_output_dim_per_layer = self.encoder.hidden_dim * self.encoder.num_directions\n",
        "        decoder_rnn_expected_hidden_dim = self.decoder.decoder_hidden_dim\n",
        "\n",
        "        self.needs_dim_adaptation = encoder_effective_output_dim_per_layer != decoder_rnn_expected_hidden_dim\n",
        "        self.fc_adapt_hidden = None\n",
        "        self.fc_adapt_cell = None\n",
        "\n",
        "        if self.needs_dim_adaptation:\n",
        "            self.fc_adapt_hidden = nn.Linear(encoder_effective_output_dim_per_layer, decoder_rnn_expected_hidden_dim)\n",
        "            if self.encoder.cell_type == 'LSTM':\n",
        "                self.fc_adapt_cell = nn.Linear(encoder_effective_output_dim_per_layer, decoder_rnn_expected_hidden_dim)\n",
        "\n",
        "    def _adapt_encoder_hidden_for_decoder(self, encoder_final_hidden_state):\n",
        "        is_lstm = self.encoder.cell_type == 'LSTM'\n",
        "        if is_lstm:\n",
        "            h_from_enc, c_from_enc = encoder_final_hidden_state\n",
        "        else:\n",
        "            h_from_enc = encoder_final_hidden_state\n",
        "            c_from_enc = None\n",
        "\n",
        "        batch_size = h_from_enc.size(1)\n",
        "\n",
        "        h_processed = h_from_enc.view(self.encoder.n_layers, self.encoder.num_directions,\n",
        "                                     batch_size, self.encoder.hidden_dim)\n",
        "        if self.encoder.bidirectional:\n",
        "            h_processed = torch.cat([h_processed[:, 0, :, :], h_processed[:, 1, :, :]], dim=2)\n",
        "        else:\n",
        "            h_processed = h_processed.squeeze(1)\n",
        "\n",
        "        c_processed = None\n",
        "        if is_lstm and c_from_enc is not None:\n",
        "            c_processed = c_from_enc.view(self.encoder.n_layers, self.encoder.num_directions,\n",
        "                                         batch_size, self.encoder.hidden_dim)\n",
        "            if self.encoder.bidirectional:\n",
        "                c_processed = torch.cat([c_processed[:, 0, :, :], c_processed[:, 1, :, :]], dim=2)\n",
        "            else:\n",
        "                c_processed = c_processed.squeeze(1)\n",
        "\n",
        "        if self.needs_dim_adaptation:\n",
        "            h_processed = self.fc_adapt_hidden(h_processed)\n",
        "            if is_lstm and c_processed is not None and self.fc_adapt_cell:\n",
        "                c_processed = self.fc_adapt_cell(c_processed)\n",
        "\n",
        "        final_h = torch.zeros(self.decoder.n_layers, batch_size, self.decoder.decoder_hidden_dim, device=self.device)\n",
        "        final_c = torch.zeros(self.decoder.n_layers, batch_size, self.decoder.decoder_hidden_dim, device=self.device) if is_lstm else None\n",
        "\n",
        "        if self.encoder.n_layers == self.decoder.n_layers:\n",
        "            final_h = h_processed\n",
        "            if is_lstm: final_c = c_processed\n",
        "        elif self.encoder.n_layers > self.decoder.n_layers:\n",
        "            final_h = h_processed[-self.decoder.n_layers:, :, :]\n",
        "            if is_lstm and c_processed is not None:\n",
        "                final_c = c_processed[-self.decoder.n_layers:, :, :]\n",
        "        else:\n",
        "            final_h[:self.encoder.n_layers, :, :] = h_processed\n",
        "            if is_lstm and c_processed is not None:\n",
        "                final_c[:self.encoder.n_layers, :, :] = c_processed\n",
        "\n",
        "            if self.encoder.n_layers > 0:\n",
        "                last_h_layer_to_repeat = h_processed[self.encoder.n_layers-1, :, :]\n",
        "                for i in range(self.encoder.n_layers, self.decoder.n_layers):\n",
        "                    final_h[i, :, :] = last_h_layer_to_repeat\n",
        "                    if is_lstm and c_processed is not None:\n",
        "                        last_c_layer_to_repeat = c_processed[self.encoder.n_layers-1, :, :]\n",
        "                        final_c[i, :, :] = last_c_layer_to_repeat\n",
        "\n",
        "        return (final_h, final_c) if is_lstm else final_h\n",
        "\n",
        "    def forward(self, source_seq, source_lengths, target_seq, teacher_forcing_ratio=0.5):\n",
        "        batch_size = source_seq.shape[0]\n",
        "        target_len = target_seq.shape[1]\n",
        "        target_vocab_size = self.decoder.output_vocab_size\n",
        "\n",
        "        outputs_logits = torch.zeros(batch_size, target_len, target_vocab_size).to(self.device)\n",
        "\n",
        "        _, encoder_final_hidden = self.encoder(source_seq, source_lengths)\n",
        "        decoder_hidden = self._adapt_encoder_hidden_for_decoder(encoder_final_hidden)\n",
        "\n",
        "        decoder_input = target_seq[:, 0]\n",
        "\n",
        "        for t in range(target_len - 1):\n",
        "            decoder_output_logits, decoder_hidden = \\\n",
        "                self.decoder(decoder_input, decoder_hidden)\n",
        "\n",
        "            outputs_logits[:, t+1] = decoder_output_logits\n",
        "\n",
        "            teacher_force_this_step = random.random() < teacher_forcing_ratio\n",
        "            top1_predicted_token = decoder_output_logits.argmax(1)\n",
        "\n",
        "            decoder_input = target_seq[:, t+1] if teacher_force_this_step else top1_predicted_token\n",
        "\n",
        "        return outputs_logits\n",
        "\n",
        "    def predict_greedy(self, source_seq, source_lengths, max_output_len=50, target_eos_idx=None):\n",
        "        if source_seq.dim() == 1:\n",
        "            source_seq = source_seq.unsqueeze(0)\n",
        "            source_lengths = torch.tensor([source_lengths if isinstance(source_lengths, int) else len(source_lengths)], device=self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            _, encoder_final_hidden = self.encoder(source_seq, source_lengths)\n",
        "            decoder_hidden = self._adapt_encoder_hidden_for_decoder(encoder_final_hidden)\n",
        "\n",
        "            decoder_input = torch.tensor([self.target_sos_idx], device=self.device)\n",
        "            predicted_indices = []\n",
        "\n",
        "            for _ in range(max_output_len):\n",
        "                decoder_output_logits, decoder_hidden = \\\n",
        "                    self.decoder(decoder_input, decoder_hidden)\n",
        "\n",
        "                top1_predicted_token = decoder_output_logits.argmax(1)\n",
        "                predicted_idx = top1_predicted_token.item()\n",
        "\n",
        "                if target_eos_idx is not None and predicted_idx == target_eos_idx:\n",
        "                    break\n",
        "                predicted_indices.append(predicted_idx)\n",
        "                decoder_input = top1_predicted_token\n",
        "        return predicted_indices\n",
        "\n",
        "    def predict_beam_search(self, source_seq, source_lengths, max_output_len=50, target_eos_idx=None, beam_width=3):\n",
        "        if source_seq.dim() == 1:\n",
        "            source_seq = source_seq.unsqueeze(0)\n",
        "            source_lengths = torch.tensor([source_lengths if isinstance(source_lengths, int) else len(source_lengths)], device=self.device)\n",
        "\n",
        "        batch_size = source_seq.shape[0]\n",
        "        if batch_size != 1:\n",
        "            raise ValueError(\"Beam search predict function currently supports batch_size=1.\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            _, encoder_final_hidden = self.encoder(source_seq, source_lengths)\n",
        "            decoder_hidden_init = self._adapt_encoder_hidden_for_decoder(encoder_final_hidden)\n",
        "\n",
        "            beams = [(0.0, [self.target_sos_idx], decoder_hidden_init)]\n",
        "            completed_sequences = []\n",
        "\n",
        "            for _ in range(max_output_len):\n",
        "                new_beams = []\n",
        "                if len(completed_sequences) >= beam_width and all(b[1][-1] == target_eos_idx for b in beams if b[1]):\n",
        "                    break\n",
        "\n",
        "                for log_prob_beam, seq_beam, hidden_beam in beams:\n",
        "                    if not seq_beam or seq_beam[-1] == target_eos_idx:\n",
        "                        completed_sequences.append((log_prob_beam / len(seq_beam) if len(seq_beam) > 0 else -float('inf'), seq_beam))\n",
        "                        continue\n",
        "\n",
        "                    decoder_input = torch.tensor([seq_beam[-1]], device=self.device)\n",
        "\n",
        "                    decoder_output_logits, next_hidden_beam = \\\n",
        "                        self.decoder(decoder_input, hidden_beam)\n",
        "\n",
        "                    log_probs_next_token = F.log_softmax(decoder_output_logits, dim=1)\n",
        "                    topk_log_probs, topk_indices = torch.topk(log_probs_next_token, beam_width, dim=1)\n",
        "\n",
        "                    for k in range(beam_width):\n",
        "                        next_token_idx = topk_indices[0, k].item()\n",
        "                        token_log_prob = topk_log_probs[0, k].item()\n",
        "\n",
        "                        new_seq = seq_beam + [next_token_idx]\n",
        "                        new_log_prob = log_prob_beam + token_log_prob\n",
        "                        new_beams.append((new_log_prob, new_seq, next_hidden_beam))\n",
        "\n",
        "                if not new_beams: break\n",
        "\n",
        "                new_beams.sort(key=lambda x: x[0], reverse=True)\n",
        "                beams = new_beams[:beam_width]\n",
        "\n",
        "            for log_prob_beam, seq_beam, _ in beams:\n",
        "                if not seq_beam or seq_beam[-1] != target_eos_idx :\n",
        "                    completed_sequences.append((log_prob_beam / len(seq_beam) if len(seq_beam) > 0 else -float('inf'), seq_beam))\n",
        "\n",
        "            if not completed_sequences:\n",
        "                return [target_eos_idx] if target_eos_idx is not None else []\n",
        "\n",
        "            completed_sequences.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "            best_sequence_indices = completed_sequences[0][1]\n",
        "            return best_sequence_indices[1:] if best_sequence_indices and best_sequence_indices[0] == self.target_sos_idx else best_sequence_indices\n",
        "\n",
        "# --- Data Loading and Preprocessing ---\n",
        "class Vocabulary:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.char2index = {PAD_TOKEN: 0, SOS_TOKEN: 1, EOS_TOKEN: 2, UNK_TOKEN: 3}\n",
        "        self.index2char = {0: PAD_TOKEN, 1: SOS_TOKEN, 2: EOS_TOKEN, 3: UNK_TOKEN}\n",
        "        self.char_counts = Counter()\n",
        "        self.n_chars = 4\n",
        "        self.pad_idx = self.char2index[PAD_TOKEN]\n",
        "        self.sos_idx = self.char2index[SOS_TOKEN]\n",
        "        self.eos_idx = self.char2index[EOS_TOKEN]\n",
        "\n",
        "    def add_sequence(self, sequence):\n",
        "        for char in list(sequence):\n",
        "            self.char_counts[char] += 1\n",
        "\n",
        "    def build_vocab(self, min_freq=1):\n",
        "        sorted_chars = sorted(self.char_counts.keys(), key=lambda char: (-self.char_counts[char], char))\n",
        "        for char in sorted_chars:\n",
        "            if self.char_counts[char] >= min_freq:\n",
        "                if char not in self.char2index:\n",
        "                    self.char2index[char] = self.n_chars\n",
        "                    self.index2char[self.n_chars] = char\n",
        "                    self.n_chars += 1\n",
        "\n",
        "    def sequence_to_indices(self, sequence, add_eos=False, add_sos=False):\n",
        "        indices = []\n",
        "        if add_sos:\n",
        "            indices.append(self.sos_idx)\n",
        "        for char in list(sequence):\n",
        "            indices.append(self.char2index.get(char, self.char2index[UNK_TOKEN]))\n",
        "        if add_eos:\n",
        "            indices.append(self.eos_idx)\n",
        "        return indices\n",
        "\n",
        "    def indices_to_sequence(self, indices):\n",
        "        chars = []\n",
        "        for index_val in indices:\n",
        "            char = self.index2char.get(index_val, UNK_TOKEN)\n",
        "            if index_val == self.eos_idx:\n",
        "                break\n",
        "            if index_val != self.sos_idx and index_val != self.pad_idx:\n",
        "                chars.append(char)\n",
        "        return \"\".join(chars)\n",
        "\n",
        "class TransliterationDataset(Dataset):\n",
        "    def __init__(self, file_path, source_vocab, target_vocab, max_len=None):\n",
        "        self.pairs = []\n",
        "        self.source_vocab = source_vocab\n",
        "        self.target_vocab = target_vocab\n",
        "\n",
        "        if not os.path.exists(file_path):\n",
        "            print(f\"ERROR: Data file not found during Dataset init: {file_path}\")\n",
        "            return\n",
        "\n",
        "        print(f\"Loading data from: {file_path}\")\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                for line_num, line in enumerate(f):\n",
        "                    parts = line.strip().split('\\t')\n",
        "                    if len(parts) >= 2:\n",
        "                        target_sequence, source_sequence = parts[0], parts[1]\n",
        "\n",
        "                        if max_len and (len(source_sequence) > max_len or len(target_sequence) > max_len):\n",
        "                            continue\n",
        "                        if not source_sequence or not target_sequence:\n",
        "                            continue\n",
        "                        self.pairs.append((source_sequence, target_sequence))\n",
        "            print(f\"Loaded {len(self.pairs)} pairs from {file_path}.\")\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR: Could not read or process file {file_path}. Error: {e}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if idx >= len(self.pairs):\n",
        "            raise IndexError(\"Index out of bounds for dataset\")\n",
        "        source_str, target_str = self.pairs[idx]\n",
        "        source_indices = self.source_vocab.sequence_to_indices(source_str, add_eos=True)\n",
        "        target_indices = self.target_vocab.sequence_to_indices(target_str, add_sos=True, add_eos=True)\n",
        "        return torch.tensor(source_indices, dtype=torch.long), \\\n",
        "               torch.tensor(target_indices, dtype=torch.long)\n",
        "\n",
        "def collate_fn(batch, pad_idx_source, pad_idx_target):\n",
        "    batch = [item for item in batch if item is not None and item[0] is not None and item[1] is not None]\n",
        "    if not batch:\n",
        "        return None, None, None\n",
        "\n",
        "    source_seqs, target_seqs = zip(*batch)\n",
        "\n",
        "    valid_indices = [i for i, s in enumerate(source_seqs) if len(s) > 0]\n",
        "    if not valid_indices: return None, None, None\n",
        "\n",
        "    source_seqs = [source_seqs[i] for i in valid_indices]\n",
        "    target_seqs = [target_seqs[i] for i in valid_indices]\n",
        "    source_lengths = torch.tensor([len(s) for s in source_seqs], dtype=torch.long)\n",
        "\n",
        "    padded_sources = pad_sequence(source_seqs, batch_first=True, padding_value=pad_idx_source)\n",
        "    padded_targets = pad_sequence(target_seqs, batch_first=True, padding_value=pad_idx_target)\n",
        "    return padded_sources, source_lengths, padded_targets\n",
        "\n",
        "# --- Training and Evaluation Functions ---\n",
        "def train_epoch(model, dataloader, optimizer, criterion, device, clip_value, teacher_forcing_ratio, target_vocab):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    total_correct_train = 0\n",
        "    total_samples_train = 0\n",
        "\n",
        "    if len(dataloader) == 0:\n",
        "        print(\"Warning: Training dataloader is empty.\")\n",
        "        return 0.0, 0.0\n",
        "\n",
        "    for batch_data in tqdm(dataloader, desc=\"Training\", leave=False):\n",
        "        if batch_data[0] is None: continue\n",
        "        sources, source_lengths, targets = batch_data\n",
        "\n",
        "        if sources is None or source_lengths is None or targets is None or source_lengths.numel() == 0 or sources.shape[0] == 0:\n",
        "            print(\"Warning: Empty or invalid batch data after collate_fn in training. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        sources, targets, source_lengths = sources.to(device), targets.to(device), source_lengths.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs_logits = model(sources, source_lengths, targets, teacher_forcing_ratio)\n",
        "\n",
        "        output_dim = outputs_logits.shape[-1]\n",
        "        flat_outputs = outputs_logits[:, 1:].reshape(-1, output_dim)\n",
        "        flat_targets = targets[:, 1:].reshape(-1)\n",
        "\n",
        "        loss = criterion(flat_outputs, flat_targets)\n",
        "        if torch.isnan(loss) or torch.isinf(loss):\n",
        "            print(\"Warning: NaN or Inf loss detected in training. Skipping batch.\")\n",
        "            continue\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        predictions_indices_train = outputs_logits.argmax(dim=2)\n",
        "        for i in range(targets.shape[0]):\n",
        "            pred_str_train = target_vocab.indices_to_sequence(predictions_indices_train[i, 1:].tolist())\n",
        "            true_str_train = target_vocab.indices_to_sequence(targets[i, 1:].tolist())\n",
        "            if pred_str_train == true_str_train:\n",
        "                total_correct_train += 1\n",
        "            total_samples_train += 1\n",
        "\n",
        "    avg_epoch_loss = epoch_loss / len(dataloader) if len(dataloader) > 0 else 0.0\n",
        "    train_accuracy = total_correct_train / total_samples_train if total_samples_train > 0 else 0.0\n",
        "    return avg_epoch_loss, train_accuracy\n",
        "\n",
        "def evaluate(model, dataloader, criterion, device, target_vocab, beam_width=1, target_eos_idx=None):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    if len(dataloader) == 0:\n",
        "        print(\"WARNING: Validation dataloader is empty. Returning 0 loss and 0 accuracy.\")\n",
        "        return 0.0, 0.0\n",
        "\n",
        "    print_debug_once = True\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch_data in enumerate(tqdm(dataloader, desc=\"Evaluating\", leave=False)):\n",
        "            if batch_data[0] is None: continue\n",
        "            sources, source_lengths, targets = batch_data\n",
        "\n",
        "            if sources is None or source_lengths is None or targets is None or source_lengths.numel() == 0 or sources.shape[0] == 0:\n",
        "                print(\"Warning: Empty or invalid batch data in eval after collate_fn. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            sources, targets, source_lengths = sources.to(device), targets.to(device), source_lengths.to(device)\n",
        "\n",
        "            outputs_for_loss = model(sources, source_lengths, targets, teacher_forcing_ratio=0.0)\n",
        "            output_dim = outputs_for_loss.shape[-1]\n",
        "            flat_outputs_for_loss = outputs_for_loss[:, 1:].reshape(-1, output_dim)\n",
        "            flat_targets_for_loss = targets[:, 1:].reshape(-1)\n",
        "\n",
        "            loss = criterion(flat_outputs_for_loss, flat_targets_for_loss)\n",
        "            if torch.isnan(loss) or torch.isinf(loss):\n",
        "                print(\"Warning: NaN or Inf loss detected in evaluation. Skipping batch for loss accumulation.\")\n",
        "            else:\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "            for i in range(sources.shape[0]):\n",
        "                src_single = sources[i:i+1]\n",
        "                src_len_single = source_lengths[i:i+1]\n",
        "\n",
        "                if beam_width > 1 and hasattr(model, 'predict_beam_search'):\n",
        "                    predicted_indices = model.predict_beam_search(src_single, src_len_single,\n",
        "                                                                  max_output_len=targets.size(1),\n",
        "                                                                  target_eos_idx=target_eos_idx,\n",
        "                                                                  beam_width=beam_width)\n",
        "                elif hasattr(model, 'predict_greedy'):\n",
        "                    predicted_indices = model.predict_greedy(src_single, src_len_single,\n",
        "                                                             max_output_len=targets.size(1),\n",
        "                                                             target_eos_idx=target_eos_idx)\n",
        "                else:\n",
        "                    predicted_indices = outputs_for_loss[i:i+1].argmax(dim=2)[0, 1:].tolist()\n",
        "\n",
        "                pred_str = target_vocab.indices_to_sequence(predicted_indices)\n",
        "                true_seq_indices = targets[i, 1:].tolist()\n",
        "                true_str = target_vocab.indices_to_sequence(true_seq_indices)\n",
        "\n",
        "                if pred_str == true_str:\n",
        "                    total_correct += 1\n",
        "                total_samples += 1\n",
        "\n",
        "                if print_debug_once and batch_idx == 0 and i < 1 :\n",
        "                    print(f\"\\n--- Evaluation Debug Sample {i} (Batch {batch_idx}) ---\")\n",
        "                    print(f\"    Source (indices): {src_single[0, :15].tolist()}\")\n",
        "                    print(f\"    Predicted Indices: {predicted_indices[:15]}\")\n",
        "                    print(f\"    Predicted String: '{pred_str}'\")\n",
        "                    print(f\"    True Indices: {true_seq_indices[:15]}\")\n",
        "                    print(f\"    True String: '{true_str}'\")\n",
        "                    print(f\"    Match: {pred_str == true_str}\")\n",
        "\n",
        "            if print_debug_once and batch_idx == 0:\n",
        "                print_debug_once = False\n",
        "\n",
        "    avg_epoch_loss = epoch_loss / len(dataloader) if len(dataloader) > 0 else 0.0\n",
        "    accuracy = total_correct / total_samples if total_samples > 0 else 0.0\n",
        "\n",
        "    print(f\"Evaluation - Total Correct: {total_correct}, Total Samples: {total_samples}, Calculated Accuracy: {accuracy:.4f}, Avg Loss: {avg_epoch_loss:.4f}\")\n",
        "    return avg_epoch_loss, accuracy\n",
        "\n",
        "# --- Main Training Function for W&B ---\n",
        "def train_model():\n",
        "    with wandb.init() as run:\n",
        "        config = wandb.config\n",
        "\n",
        "        run_name = f\"{config.cell_type}_emb{config.embedding_dim}_hid{config.hidden_dim}\" \\\n",
        "                   f\"_encL{config.encoder_layers}_decL{config.decoder_layers}\" \\\n",
        "                   f\"_do{config.dropout_p:.2f}_lr{config.learning_rate:.2e}\" \\\n",
        "                   f\"_encBi{str(config.encoder_bidirectional)[0]}\" \\\n",
        "                   f\"_beam{config.get('beam_width_eval', 1)}\"\n",
        "\n",
        "        if hasattr(run, 'name'): run.name = run_name\n",
        "\n",
        "        DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        current_run_name_for_log = run.name if run.name else run.id\n",
        "        print(f\"Using device: {DEVICE}. Run: {current_run_name_for_log} (ID: {run.id})\")\n",
        "        print(f\"Config: {config}\")\n",
        "\n",
        "        BASE_DATA_DIR = \"/kaggle/input/dakshina-dl-a3/dakshina_dataset_v1.0/hi/\"\n",
        "        DATA_DIR = os.path.join(BASE_DATA_DIR, \"lexicons/\")\n",
        "\n",
        "        if not os.path.exists(DATA_DIR):\n",
        "            print(f\"ERROR: Lexicons directory not found: {DATA_DIR}\")\n",
        "            return 1\n",
        "\n",
        "        train_file = os.path.join(DATA_DIR, \"hi.translit.sampled.train.tsv\")\n",
        "        dev_file = os.path.join(DATA_DIR, \"hi.translit.sampled.dev.tsv\")\n",
        "\n",
        "        if not os.path.exists(train_file) or not os.path.exists(dev_file):\n",
        "            print(f\"ERROR: Train or Dev file not found. Train: {train_file}, Dev: {dev_file}\")\n",
        "            return 1\n",
        "\n",
        "        source_vocab = Vocabulary(\"latin\")\n",
        "        target_vocab = Vocabulary(\"devanagari\")\n",
        "\n",
        "        temp_train_dataset_for_vocab = TransliterationDataset(train_file, source_vocab, target_vocab, max_len=config.max_seq_len)\n",
        "        if not temp_train_dataset_for_vocab.pairs:\n",
        "            print(f\"ERROR: No data loaded for vocab building from {train_file}.\")\n",
        "            return 1\n",
        "        for src_str, tgt_str in temp_train_dataset_for_vocab.pairs:\n",
        "            source_vocab.add_sequence(src_str)\n",
        "            target_vocab.add_sequence(tgt_str)\n",
        "        source_vocab.build_vocab(min_freq=config.vocab_min_freq)\n",
        "        target_vocab.build_vocab(min_freq=config.vocab_min_freq)\n",
        "\n",
        "        print(f\"Source Vocab: {source_vocab.n_chars} chars. Target Vocab: {target_vocab.n_chars} chars.\")\n",
        "\n",
        "        train_dataset = TransliterationDataset(train_file, source_vocab, target_vocab, max_len=config.max_seq_len)\n",
        "        dev_dataset = TransliterationDataset(dev_file, source_vocab, target_vocab, max_len=config.max_seq_len)\n",
        "\n",
        "        if len(train_dataset) == 0 or len(dev_dataset) == 0:\n",
        "            print(f\"ERROR: Train/Dev dataset empty. Train: {len(train_dataset)}, Dev: {len(dev_dataset)}\")\n",
        "            return 1\n",
        "\n",
        "        num_loader_workers = 0\n",
        "        if DEVICE.type == 'cuda' and os.cpu_count() and os.cpu_count() > 1:\n",
        "            num_loader_workers = min(4, os.cpu_count() // 2)\n",
        "\n",
        "        train_dataloader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True,\n",
        "                                      collate_fn=lambda b: collate_fn(b, source_vocab.pad_idx, target_vocab.pad_idx),\n",
        "                                      num_workers=num_loader_workers, pin_memory=True if DEVICE.type == 'cuda' else False, drop_last=True)\n",
        "        dev_dataloader = DataLoader(dev_dataset, batch_size=config.batch_size, shuffle=False,\n",
        "                                     collate_fn=lambda b: collate_fn(b, source_vocab.pad_idx, target_vocab.pad_idx),\n",
        "                                     num_workers=num_loader_workers, pin_memory=True if DEVICE.type == 'cuda' else False, drop_last=False)\n",
        "\n",
        "        if len(train_dataloader) == 0 or len(dev_dataloader) == 0:\n",
        "            print(f\"ERROR: Train/Dev Dataloader empty. Train: {len(train_dataloader)}, Dev: {len(dev_dataloader)}\")\n",
        "            return 1\n",
        "\n",
        "        encoder = Encoder(source_vocab.n_chars, config.embedding_dim, config.hidden_dim,\n",
        "                          config.encoder_layers, config.cell_type, config.dropout_p,\n",
        "                          config.encoder_bidirectional, pad_idx=source_vocab.pad_idx).to(DEVICE)\n",
        "        decoder = Decoder(target_vocab.n_chars, config.embedding_dim,\n",
        "                          config.hidden_dim,\n",
        "                          config.decoder_layers, config.cell_type, config.dropout_p,\n",
        "                          pad_idx=target_vocab.pad_idx).to(DEVICE)\n",
        "        model = Seq2Seq(encoder, decoder, DEVICE, target_vocab.sos_idx).to(DEVICE)\n",
        "\n",
        "        print(f\"Model parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
        "        wandb.watch(model, log=\"all\", log_freq=100)\n",
        "\n",
        "        optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=config.get('lr_scheduler_patience', 3), factor=0.3, verbose=True)\n",
        "        criterion = nn.CrossEntropyLoss(ignore_index=target_vocab.pad_idx)\n",
        "\n",
        "        best_val_accuracy = -1.0\n",
        "        epochs_no_improve = 0\n",
        "        max_epochs_no_improve = config.get('max_epochs_no_improve', 7)\n",
        "\n",
        "        for epoch in range(config.epochs):\n",
        "            train_loss, train_accuracy = train_epoch(model, train_dataloader, optimizer, criterion, DEVICE,\n",
        "                                                     config.clip_value, config.teacher_forcing_ratio, target_vocab)\n",
        "\n",
        "            val_loss, val_accuracy = evaluate(model, dev_dataloader, criterion, DEVICE, target_vocab,\n",
        "                                              beam_width=config.get('beam_width_eval', 1),\n",
        "                                              target_eos_idx=target_vocab.eos_idx)\n",
        "\n",
        "            scheduler.step(val_accuracy)\n",
        "\n",
        "            print(f\"Epoch {epoch+1}/{config.epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_accuracy:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_accuracy:.4f}\")\n",
        "            log_dict = {\n",
        "                \"epoch\": epoch + 1,\n",
        "                \"train_loss\": train_loss if not np.isnan(train_loss) else 0.0,\n",
        "                \"train_accuracy\": train_accuracy if not np.isnan(train_accuracy) else 0.0,\n",
        "                \"val_loss\": val_loss if not np.isnan(val_loss) else 0.0,\n",
        "                \"val_accuracy\": val_accuracy if not np.isnan(val_accuracy) else 0.0,\n",
        "                \"learning_rate\": optimizer.param_groups[0]['lr']\n",
        "            }\n",
        "            wandb.log(log_dict)\n",
        "\n",
        "            current_val_acc = val_accuracy if not np.isnan(val_accuracy) else -1.0\n",
        "            if current_val_acc > best_val_accuracy:\n",
        "                best_val_accuracy = current_val_acc\n",
        "                epochs_no_improve = 0\n",
        "            else:\n",
        "                epochs_no_improve += 1\n",
        "\n",
        "            if config.early_stopping and epochs_no_improve >= max_epochs_no_improve:\n",
        "                print(f\"Early stopping triggered at epoch {epoch+1} after {epochs_no_improve} epochs with no improvement.\")\n",
        "                break\n",
        "\n",
        "        wandb.summary[\"best_val_accuracy\"] = best_val_accuracy if not np.isnan(best_val_accuracy) else 0.0\n",
        "        print(f\"Finished run. Best Validation Accuracy: {best_val_accuracy:.4f}\")\n",
        "        return 0\n",
        "\n",
        "# --- W&B Sweep Configuration ---\n",
        "sweep_config = {\n",
        "    'method': 'bayes',\n",
        "    'metric': {'name': 'val_accuracy', 'goal': 'maximize'},\n",
        "    'parameters': {\n",
        "        'embedding_dim': {'values': [128, 256, 300]},\n",
        "        'hidden_dim': {'values': [256, 512]},\n",
        "        'encoder_layers': {'values': [1, 2]},\n",
        "        'decoder_layers': {'values': [1, 2]},\n",
        "        'cell_type': {'values': ['GRU', 'LSTM']},\n",
        "        'dropout_p': {'values': [0.2, 0.3, 0.4, 0.5]},\n",
        "        'encoder_bidirectional': {'values': [True, False]},\n",
        "        'learning_rate': {'distribution': 'log_uniform_values', 'min': 1e-4, 'max': 3e-3},\n",
        "        'batch_size': {'values': [32, 64, 128]},\n",
        "        'epochs': {'value': 15},\n",
        "        'clip_value': {'value': 1.0},\n",
        "        'teacher_forcing_ratio': {'distribution': 'uniform', 'min': 0.4, 'max': 0.6},\n",
        "        'vocab_min_freq': {'value': 1},\n",
        "        'max_seq_len': {'value': 50},\n",
        "        'early_stopping': {'value': True},\n",
        "        'max_epochs_no_improve': {'values': [5, 7]},\n",
        "        'lr_scheduler_patience': {'values': [2, 3]},\n",
        "        'beam_width_eval': {'values': [1, 3]}\n",
        "    }\n",
        "}\n",
        "\n",
        "# --- Main Execution Block ---\n",
        "if __name__ == '__main__':\n",
        "    try:\n",
        "        if 'KAGGLE_KERNEL_RUN_TYPE' in os.environ:\n",
        "            print(\"Detected Kaggle environment. Ensure WANDB_API_KEY is set as a secret or environment variable.\")\n",
        "            try:\n",
        "                from kaggle_secrets import UserSecretsClient\n",
        "                user_secrets = UserSecretsClient()\n",
        "                wandb_api_key = user_secrets.get_secret(\"WANDB_API_KEY\")\n",
        "                wandb.login(key=wandb_api_key)\n",
        "                print(\"W&B login with Kaggle secret successful.\")\n",
        "            except Exception as e_secret:\n",
        "                print(f\"Could not login with Kaggle secret ({e_secret}). Attempting default login.\")\n",
        "                if \"WANDB_API_KEY\" in os.environ:\n",
        "                    wandb.login()\n",
        "                    print(\"W&B login using environment variable.\")\n",
        "                else:\n",
        "                    print(\"WANDB_API_KEY not found. Please set it up or login manually if prompted.\")\n",
        "                    wandb.login()\n",
        "        else:\n",
        "            wandb.login()\n",
        "        print(\"W&B login process attempted/completed.\")\n",
        "    except Exception as e:\n",
        "        print(f\"W&B login failed: {e}. Please ensure your W&B API key is correctly configured.\")\n",
        "        exit()\n",
        "\n",
        "    SWEEP_PROJECT_NAME = \"DL_A3\"\n",
        "\n",
        "    print(\"Initializing sweep...\")\n",
        "    try:\n",
        "        sweep_id = wandb.sweep(sweep_config, project=SWEEP_PROJECT_NAME)\n",
        "        print(f\"Sweep ID: {sweep_id}\")\n",
        "        print(f\"To run agents, execute: wandb agent YOUR_WANDB_USERNAME/{SWEEP_PROJECT_NAME}/{sweep_id}\")\n",
        "\n",
        "        print(\"Starting a W&B agent (adjust count as needed)...\")\n",
        "        wandb.agent(sweep_id, function=train_model, count=10)\n",
        "    except Exception as e:\n",
        "        print(f\"Could not initialize sweep or run agent. Error: {e}\")\n",
        "\n",
        "    print(\"Sweep agent finished or stopped. Check W&B dashboard for results.\")"
      ],
      "metadata": {
        "id": "8ccMJRPFmpqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q4 train and testing"
      ],
      "metadata": {
        "id": "r-8KVwOcnvkz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import numpy as np\n",
        "import wandb\n",
        "import os\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "import heapq\n",
        "\n",
        "# --- Constants for Special Tokens ---\n",
        "SOS_TOKEN = \"<sos>\"  # Start-of-sequence token\n",
        "EOS_TOKEN = \"<eos>\"  # End-of-sequence token\n",
        "PAD_TOKEN = \"<pad>\"  # Padding token\n",
        "UNK_TOKEN = \"<unk>\"  # Unknown token\n",
        "\n",
        "# --- Model Definitions (Non-Attention Version) ---\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    The Encoder processes the input sequence and produces a context vector (final hidden state).\n",
        "    It uses a recurrent neural network (RNN, GRU, or LSTM).\n",
        "    \"\"\"\n",
        "    def __init__(self, input_vocab_size, embedding_dim, hidden_dim, n_layers,\n",
        "                 cell_type='LSTM', dropout_p=0.1, bidirectional=False, pad_idx=0):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.input_vocab_size = input_vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.cell_type = cell_type.upper()\n",
        "        self.bidirectional = bidirectional\n",
        "        self.num_directions = 2 if bidirectional else 1\n",
        "\n",
        "        # Embedding layer converts input tokens (indices) into dense vectors\n",
        "        self.embedding = nn.Embedding(input_vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "        # Dropout layer for regularization\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "        # Apply dropout to RNN layers if n_layers > 1\n",
        "        rnn_dropout = dropout_p if n_layers > 1 else 0\n",
        "        if self.cell_type == 'RNN':\n",
        "            self.rnn = nn.RNN(embedding_dim, hidden_dim, n_layers,\n",
        "                              dropout=rnn_dropout, batch_first=True, bidirectional=self.bidirectional)\n",
        "        elif self.cell_type == 'GRU':\n",
        "            self.rnn = nn.GRU(embedding_dim, hidden_dim, n_layers,\n",
        "                              dropout=rnn_dropout, batch_first=True, bidirectional=self.bidirectional)\n",
        "        elif self.cell_type == 'LSTM':\n",
        "            self.rnn = nn.LSTM(embedding_dim, hidden_dim, n_layers,\n",
        "                               dropout=rnn_dropout, batch_first=True, bidirectional=self.bidirectional)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported cell type. Choose from 'RNN', 'GRU', 'LSTM'.\")\n",
        "\n",
        "    def forward(self, input_seq, input_lengths):\n",
        "        \"\"\"\n",
        "        Forward pass for the encoder.\n",
        "\n",
        "        Args:\n",
        "            input_seq (torch.Tensor): Padded input sequences of shape (batch_size, seq_len).\n",
        "            input_lengths (torch.Tensor): Lengths of the original sequences in the batch of shape (batch_size,).\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple (None, hidden_state).\n",
        "                   The first element is None because this encoder does not output full sequences for attention.\n",
        "                   The second element is the final hidden state of the encoder, which serves as the context.\n",
        "                   For LSTM, `hidden_state` is a tuple (h, c).\n",
        "        \"\"\"\n",
        "        embedded = self.embedding(input_seq)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        # Pack the padded sequences to handle variable-length inputs efficiently\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(\n",
        "            embedded, input_lengths.cpu(), batch_first=True, enforce_sorted=False\n",
        "        )\n",
        "        # Pass packed sequences through the RNN\n",
        "        # For a non-attention decoder, we primarily need the final hidden state.\n",
        "        _, hidden = self.rnn(packed_embedded)\n",
        "        return None, hidden # Return None for encoder_outputs as they are not used by the simple decoder\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    \"\"\"\n",
        "    The Decoder generates the output sequence one token at a time, conditioned on the\n",
        "    encoder's final hidden state and previously generated tokens.\n",
        "    This version does NOT use attention.\n",
        "    \"\"\"\n",
        "    def __init__(self, output_vocab_size, embedding_dim,\n",
        "                 decoder_hidden_dim, n_layers, cell_type='LSTM', dropout_p=0.1, pad_idx=0):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.output_vocab_size = output_vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.decoder_hidden_dim = decoder_hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.cell_type = cell_type.upper()\n",
        "\n",
        "        self.embedding = nn.Embedding(output_vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "        # Input to decoder RNN is just the embedding of the previous token\n",
        "        rnn_input_dim = embedding_dim\n",
        "\n",
        "        # Apply dropout to RNN layers if n_layers > 1\n",
        "        rnn_dropout = dropout_p if n_layers > 1 else 0\n",
        "        if self.cell_type == 'RNN':\n",
        "            self.rnn = nn.RNN(rnn_input_dim, decoder_hidden_dim, n_layers,\n",
        "                              dropout=rnn_dropout, batch_first=True)\n",
        "        elif self.cell_type == 'GRU':\n",
        "            self.rnn = nn.GRU(rnn_input_dim, decoder_hidden_dim, n_layers,\n",
        "                              dropout=rnn_dropout, batch_first=True)\n",
        "        elif self.cell_type == 'LSTM':\n",
        "            self.rnn = nn.LSTM(rnn_input_dim, decoder_hidden_dim, n_layers,\n",
        "                               dropout=rnn_dropout, batch_first=True)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported cell type. Choose from 'RNN', 'GRU', 'LSTM'.\")\n",
        "\n",
        "        # Output linear layer to project decoder's hidden state to vocabulary size\n",
        "        self.fc_out = nn.Linear(decoder_hidden_dim, output_vocab_size)\n",
        "\n",
        "    def forward(self, input_char, prev_decoder_hidden):\n",
        "        \"\"\"\n",
        "        Forward pass for the decoder.\n",
        "\n",
        "        Args:\n",
        "            input_char (torch.Tensor): A single token (or batch of single tokens) to be embedded,\n",
        "                                       shape (batch_size,).\n",
        "            prev_decoder_hidden (torch.Tensor or tuple): The previous hidden state (and cell state for LSTM)\n",
        "                                                         of the decoder RNN.\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple (prediction_logits, current_decoder_hidden).\n",
        "                   `prediction_logits` are the raw scores for each token in the vocabulary.\n",
        "                   `current_decoder_hidden` is the updated hidden state after processing the input.\n",
        "        \"\"\"\n",
        "        # Add a sequence dimension for RNN input (batch_size, 1, embedding_dim)\n",
        "        input_char = input_char.unsqueeze(1)\n",
        "        embedded = self.embedding(input_char)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        # Pass through the RNN layer\n",
        "        rnn_output, current_decoder_hidden = self.rnn(embedded, prev_decoder_hidden)\n",
        "\n",
        "        # Squeeze the sequence dimension for the linear layer\n",
        "        rnn_output_squeezed = rnn_output.squeeze(1)\n",
        "        # Project to vocabulary size to get logits\n",
        "        prediction_logits = self.fc_out(rnn_output_squeezed)\n",
        "        return prediction_logits, current_decoder_hidden\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    \"\"\"\n",
        "    The main Sequence-to-Sequence model that connects the Encoder and Decoder.\n",
        "    This architecture does NOT use an attention mechanism.\n",
        "    \"\"\"\n",
        "    def __init__(self, encoder, decoder, device, target_sos_idx):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "        self.target_sos_idx = target_sos_idx\n",
        "\n",
        "        # Calculate effective dimensions for hidden state adaptation\n",
        "        encoder_effective_output_dim_per_layer = self.encoder.hidden_dim * self.encoder.num_directions\n",
        "        decoder_rnn_expected_hidden_dim = self.decoder.decoder_hidden_dim\n",
        "\n",
        "        # Check if hidden state dimensions need to be adapted from encoder to decoder\n",
        "        self.needs_dim_adaptation = encoder_effective_output_dim_per_layer != decoder_rnn_expected_hidden_dim\n",
        "        self.fc_adapt_hidden = None\n",
        "        self.fc_adapt_cell = None\n",
        "\n",
        "        # Create linear layers for hidden state adaptation if necessary\n",
        "        if self.needs_dim_adaptation:\n",
        "            self.fc_adapt_hidden = nn.Linear(encoder_effective_output_dim_per_layer, decoder_rnn_expected_hidden_dim)\n",
        "            if self.encoder.cell_type == 'LSTM':\n",
        "                self.fc_adapt_cell = nn.Linear(encoder_effective_output_dim_per_layer, decoder_rnn_expected_hidden_dim)\n",
        "\n",
        "    def _adapt_encoder_hidden_for_decoder(self, encoder_final_hidden_state):\n",
        "        \"\"\"\n",
        "        Adapts the encoder's final hidden state(s) to match the decoder's expected hidden state dimensions and layer count.\n",
        "        Handles bidirectionality by concatenating forward and backward hidden states.\n",
        "        If decoder has more layers than encoder, the last encoder layer's state is repeated.\n",
        "        \"\"\"\n",
        "        is_lstm = self.encoder.cell_type == 'LSTM'\n",
        "        if is_lstm:\n",
        "            h_from_enc, c_from_enc = encoder_final_hidden_state\n",
        "        else:\n",
        "            h_from_enc = encoder_final_hidden_state\n",
        "            c_from_enc = None\n",
        "\n",
        "        batch_size = h_from_enc.size(1)\n",
        "\n",
        "        # Reshape encoder hidden state to (n_layers, num_directions, batch_size, hidden_dim)\n",
        "        h_processed = h_from_enc.view(self.encoder.n_layers, self.encoder.num_directions,\n",
        "                                     batch_size, self.encoder.hidden_dim)\n",
        "        if self.encoder.bidirectional:\n",
        "            # Concatenate forward and backward hidden states across the hidden dimension\n",
        "            h_processed = torch.cat([h_processed[:, 0, :, :], h_processed[:, 1, :, :]], dim=2)\n",
        "        else:\n",
        "            # Remove the single direction dimension\n",
        "            h_processed = h_processed.squeeze(1)\n",
        "\n",
        "        c_processed = None\n",
        "        if is_lstm and c_from_enc is not None:\n",
        "            c_processed = c_from_enc.view(self.encoder.n_layers, self.encoder.num_directions,\n",
        "                                         batch_size, self.encoder.hidden_dim)\n",
        "            if self.encoder.bidirectional:\n",
        "                c_processed = torch.cat([c_processed[:, 0, :, :], c_processed[:, 1, :, :]], dim=2)\n",
        "            else:\n",
        "                c_processed = c_processed.squeeze(1)\n",
        "\n",
        "        # Apply linear transformation if hidden dimensions mismatch\n",
        "        if self.needs_dim_adaptation:\n",
        "            h_processed = self.fc_adapt_hidden(h_processed)\n",
        "            if is_lstm and c_processed is not None and self.fc_adapt_cell:\n",
        "                c_processed = self.fc_adapt_cell(c_processed)\n",
        "\n",
        "        # Initialize decoder's hidden state(s)\n",
        "        final_h = torch.zeros(self.decoder.n_layers, batch_size, self.decoder.decoder_hidden_dim, device=self.device)\n",
        "        final_c = torch.zeros(self.decoder.n_layers, batch_size, self.decoder.decoder_hidden_dim, device=self.device) if is_lstm else None\n",
        "\n",
        "        # Copy or repeat encoder hidden states to match decoder's layer count\n",
        "        if self.encoder.n_layers == self.decoder.n_layers:\n",
        "            final_h = h_processed\n",
        "            if is_lstm: final_c = c_processed\n",
        "        elif self.encoder.n_layers > self.decoder.n_layers:\n",
        "            # Use the last N encoder layers where N is decoder_layers\n",
        "            final_h = h_processed[-self.decoder.n_layers:, :, :]\n",
        "            if is_lstm and c_processed is not None:\n",
        "                final_c = c_processed[-self.decoder.n_layers:, :, :]\n",
        "        else:\n",
        "            # Copy encoder states to the first M decoder layers, and repeat the last encoder layer state for the rest\n",
        "            final_h[:self.encoder.n_layers, :, :] = h_processed\n",
        "            if is_lstm and c_processed is not None:\n",
        "                final_c[:self.encoder.n_layers, :, :] = c_processed\n",
        "\n",
        "            if self.encoder.n_layers > 0:\n",
        "                last_h_layer_to_repeat = h_processed[self.encoder.n_layers-1, :, :]\n",
        "                for i in range(self.encoder.n_layers, self.decoder.n_layers):\n",
        "                    final_h[i, :, :] = last_h_layer_to_repeat\n",
        "                    if is_lstm and c_processed is not None:\n",
        "                        last_c_layer_to_repeat = c_processed[self.encoder.n_layers-1, :, :]\n",
        "                        final_c[i, :, :] = last_c_layer_to_repeat\n",
        "\n",
        "        return (final_h, final_c) if is_lstm else final_h\n",
        "\n",
        "    def forward(self, source_seq, source_lengths, target_seq, teacher_forcing_ratio=0.5):\n",
        "        \"\"\"\n",
        "        Forward pass for the Seq2Seq model during training.\n",
        "\n",
        "        Args:\n",
        "            source_seq (torch.Tensor): Padded source sequences.\n",
        "            source_lengths (torch.Tensor): Lengths of source sequences.\n",
        "            target_seq (torch.Tensor): Padded target sequences (including SOS token).\n",
        "            teacher_forcing_ratio (float): Probability of using actual target token as next input.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Logits for the predicted target sequence.\n",
        "        \"\"\"\n",
        "        batch_size = source_seq.shape[0]\n",
        "        target_len = target_seq.shape[1]\n",
        "        target_vocab_size = self.decoder.output_vocab_size\n",
        "\n",
        "        # Tensor to store decoder outputs\n",
        "        outputs_logits = torch.zeros(batch_size, target_len, target_vocab_size).to(self.device)\n",
        "\n",
        "        # Encode the source sequence to get the initial hidden state for the decoder\n",
        "        # Encoder returns None for encoder_outputs as they are not used by the simple decoder\n",
        "        _, encoder_final_hidden = self.encoder(source_seq, source_lengths)\n",
        "        # Adapt encoder hidden state to decoder's expected shape\n",
        "        decoder_hidden = self._adapt_encoder_hidden_for_decoder(encoder_final_hidden)\n",
        "\n",
        "        # First input to the decoder is the <sos> token\n",
        "        decoder_input = target_seq[:, 0]\n",
        "\n",
        "        # Iterate through the target sequence, predicting one token at a time\n",
        "        for t in range(target_len - 1): # Exclude the last token as we predict up to target_len-1\n",
        "            # Pass input and previous hidden state to the decoder\n",
        "            decoder_output_logits, decoder_hidden = \\\n",
        "                self.decoder(decoder_input, decoder_hidden)\n",
        "\n",
        "            # Store the current step's predictions\n",
        "            outputs_logits[:, t+1] = decoder_output_logits\n",
        "\n",
        "            # Decide whether to use teacher forcing\n",
        "            teacher_force_this_step = random.random() < teacher_forcing_ratio\n",
        "            # Get the top predicted token for the next input if not teacher forcing\n",
        "            top1_predicted_token = decoder_output_logits.argmax(1)\n",
        "\n",
        "            # Use actual target token (teacher forcing) or predicted token for the next step\n",
        "            decoder_input = target_seq[:, t+1] if teacher_force_this_step else top1_predicted_token\n",
        "\n",
        "        return outputs_logits\n",
        "\n",
        "    def predict_greedy(self, source_seq, source_lengths, max_output_len=50, target_eos_idx=None):\n",
        "        \"\"\"\n",
        "        Generates a sequence using greedy decoding.\n",
        "\n",
        "        Args:\n",
        "            source_seq (torch.Tensor): Input source sequence (can be a single example or a batch of 1).\n",
        "            source_lengths (torch.Tensor): Length of the source sequence.\n",
        "            max_output_len (int): Maximum length of the sequence to generate.\n",
        "            target_eos_idx (int, optional): Index of the EOS token in the target vocabulary.\n",
        "\n",
        "        Returns:\n",
        "            list: List of predicted token indices.\n",
        "        \"\"\"\n",
        "        self.eval() # Set model to evaluation mode\n",
        "        # Ensure input sequence is batched (even if batch size is 1)\n",
        "        if source_seq.dim() == 1:\n",
        "            source_seq = source_seq.unsqueeze(0)\n",
        "            source_lengths = torch.tensor([source_lengths if isinstance(source_lengths, int) else len(source_lengths)], device=self.device)\n",
        "\n",
        "        with torch.no_grad(): # Disable gradient calculations\n",
        "            # Encode the source sequence\n",
        "            _, encoder_final_hidden = self.encoder(source_seq, source_lengths)\n",
        "            # Adapt encoder hidden state for decoder initialization\n",
        "            decoder_hidden = self._adapt_encoder_hidden_for_decoder(encoder_final_hidden)\n",
        "\n",
        "            # First input to the decoder is the <sos> token\n",
        "            decoder_input = torch.tensor([self.target_sos_idx], device=self.device)\n",
        "            predicted_indices = []\n",
        "\n",
        "            for _ in range(max_output_len):\n",
        "                # Get decoder output and updated hidden state\n",
        "                decoder_output_logits, decoder_hidden = \\\n",
        "                    self.decoder(decoder_input, decoder_hidden)\n",
        "\n",
        "                # Greedily select the token with the highest probability\n",
        "                top1_predicted_token = decoder_output_logits.argmax(1)\n",
        "                predicted_idx = top1_predicted_token.item()\n",
        "\n",
        "                # Stop if EOS token is predicted\n",
        "                if target_eos_idx is not None and predicted_idx == target_eos_idx:\n",
        "                    break\n",
        "\n",
        "                predicted_indices.append(predicted_idx)\n",
        "                # Use the predicted token as the input for the next step\n",
        "                decoder_input = top1_predicted_token\n",
        "        return predicted_indices\n",
        "\n",
        "    def predict_beam_search(self, source_seq, source_lengths, max_output_len=50, target_eos_idx=None, beam_width=3):\n",
        "        \"\"\"\n",
        "        Generates a sequence using beam search decoding.\n",
        "\n",
        "        Args:\n",
        "            source_seq (torch.Tensor): Input source sequence (must be a single example).\n",
        "            source_lengths (torch.Tensor): Length of the source sequence.\n",
        "            max_output_len (int): Maximum length of the sequence to generate.\n",
        "            target_eos_idx (int, optional): Index of the EOS token in the target vocabulary.\n",
        "            beam_width (int): The number of top sequences to keep at each step.\n",
        "\n",
        "        Returns:\n",
        "            list: List of predicted token indices for the best sequence found.\n",
        "        \"\"\"\n",
        "        self.eval() # Set model to evaluation mode\n",
        "        # Ensure input sequence is batched (even if batch size is 1)\n",
        "        if source_seq.dim() == 1:\n",
        "            source_seq = source_seq.unsqueeze(0)\n",
        "            source_lengths = torch.tensor([source_lengths if isinstance(source_lengths, int) else len(source_lengths)], device=self.device)\n",
        "\n",
        "        if source_seq.shape[0] != 1:\n",
        "            raise ValueError(\"Beam search predict function currently supports batch_size=1.\")\n",
        "\n",
        "        with torch.no_grad(): # Disable gradient calculations\n",
        "            # Encode the source sequence\n",
        "            _, encoder_final_hidden = self.encoder(source_seq, source_lengths)\n",
        "            # Adapt encoder hidden state for decoder initialization\n",
        "            decoder_hidden_init = self._adapt_encoder_hidden_for_decoder(encoder_final_hidden)\n",
        "\n",
        "            # Beams are stored as (log_probability, sequence_of_indices, decoder_hidden_state)\n",
        "            # Use a min-heap to keep track of the top `beam_width` sequences (smallest log_prob at top)\n",
        "            beams = [(0.0, [self.target_sos_idx], decoder_hidden_init)]\n",
        "            completed_sequences = [] # Stores sequences that have predicted EOS\n",
        "\n",
        "            for _ in range(max_output_len):\n",
        "                new_beams = []\n",
        "                all_current_beams_ended = True # Flag to check if all beams have terminated\n",
        "\n",
        "                # Process each beam in the current set of beams\n",
        "                for log_prob_beam, seq_beam, hidden_beam in beams:\n",
        "                    # If this beam has already ended, move it to completed sequences and skip\n",
        "                    if not seq_beam or seq_beam[-1] == target_eos_idx:\n",
        "                        # Normalize log probability by length to counteract bias towards shorter sequences\n",
        "                        completed_sequences.append((log_prob_beam / len(seq_beam) if len(seq_beam) > 0 else -float('inf'), seq_beam))\n",
        "                        continue # Skip to the next beam\n",
        "\n",
        "                    all_current_beams_ended = False # At least one beam is still active\n",
        "\n",
        "                    # Get the last token from the current beam sequence as decoder input\n",
        "                    decoder_input = torch.tensor([seq_beam[-1]], device=self.device)\n",
        "                    # Pass through the decoder to get next token logits and hidden state\n",
        "                    decoder_output_logits, next_hidden_beam = \\\n",
        "                        self.decoder(decoder_input, hidden_beam)\n",
        "\n",
        "                    # Convert logits to log probabilities and get top K candidates\n",
        "                    log_probs_next_token = F.log_softmax(decoder_output_logits, dim=1)\n",
        "                    topk_log_probs, topk_indices = torch.topk(log_probs_next_token, beam_width, dim=1)\n",
        "\n",
        "                    # Expand each current beam into `beam_width` new beams\n",
        "                    for k in range(beam_width):\n",
        "                        next_token_idx = topk_indices[0, k].item()\n",
        "                        token_log_prob = topk_log_probs[0, k].item()\n",
        "\n",
        "                        new_seq = seq_beam + [next_token_idx]\n",
        "                        new_log_prob = log_prob_beam + token_log_prob # Accumulate log probability\n",
        "                        new_beams.append((new_log_prob, new_seq, next_hidden_beam))\n",
        "\n",
        "                # If no new beams were generated (e.g., all beams ended) or all current beams ended, stop\n",
        "                if not new_beams or all_current_beams_ended: break\n",
        "\n",
        "                # Sort all new candidate beams by log probability and select the top `beam_width`\n",
        "                new_beams.sort(key=lambda x: x[0], reverse=True)\n",
        "                beams = new_beams[:beam_width]\n",
        "\n",
        "            # After generation loop, add any remaining active beams to completed sequences\n",
        "            for log_prob_beam, seq_beam, _ in beams:\n",
        "                 if not seq_beam or seq_beam[-1] != target_eos_idx:\n",
        "                    completed_sequences.append((log_prob_beam / len(seq_beam) if len(seq_beam) > 0 else -float('inf'), seq_beam))\n",
        "\n",
        "            # If no sequences were completed (should not happen with good parameters), return default\n",
        "            if not completed_sequences:\n",
        "                return [target_eos_idx] if target_eos_idx is not None else []\n",
        "\n",
        "            # Sort all completed sequences by normalized log probability and return the best one\n",
        "            completed_sequences.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "            best_sequence_indices = completed_sequences[0][1]\n",
        "            # Remove the SOS token if it's at the beginning of the best sequence\n",
        "            return best_sequence_indices[1:] if best_sequence_indices and best_sequence_indices[0] == self.target_sos_idx else best_sequence_indices\n",
        "\n",
        "# --- Data Loading and Preprocessing ---\n",
        "\n",
        "class Vocabulary:\n",
        "    \"\"\"\n",
        "    Manages the mapping between characters and their numerical indices.\n",
        "    Includes special tokens for padding, start-of-sequence, end-of-sequence, and unknown characters.\n",
        "    \"\"\"\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.char2index = {PAD_TOKEN: 0, SOS_TOKEN: 1, EOS_TOKEN: 2, UNK_TOKEN: 3}\n",
        "        self.index2char = {0: PAD_TOKEN, 1: SOS_TOKEN, 2: EOS_TOKEN, 3: UNK_TOKEN}\n",
        "        self.char_counts = Counter()\n",
        "        self.n_chars = 4 # Initialize count with special tokens\n",
        "        self.pad_idx = self.char2index[PAD_TOKEN]\n",
        "        self.sos_idx = self.char2index[SOS_TOKEN]\n",
        "        self.eos_idx = self.char2index[EOS_TOKEN]\n",
        "\n",
        "    def add_sequence(self, sequence):\n",
        "        \"\"\"Adds characters from a sequence to the character counts.\"\"\"\n",
        "        for char in list(sequence): self.char_counts[char] += 1\n",
        "\n",
        "    def build_vocab(self, min_freq=1):\n",
        "        \"\"\"\n",
        "        Builds the vocabulary mapping based on character counts and a minimum frequency.\n",
        "        Characters appearing less than `min_freq` will be treated as UNK_TOKEN.\n",
        "        \"\"\"\n",
        "        sorted_chars = sorted(self.char_counts.keys(), key=lambda char: (-self.char_counts[char], char))\n",
        "        for char in sorted_chars:\n",
        "            if self.char_counts[char] >= min_freq and char not in self.char2index:\n",
        "                self.char2index[char] = self.n_chars\n",
        "                self.index2char[self.n_chars] = char\n",
        "                self.n_chars += 1\n",
        "\n",
        "    def sequence_to_indices(self, sequence, add_eos=False, add_sos=False):\n",
        "        \"\"\"Converts a character sequence into a list of numerical indices.\"\"\"\n",
        "        indices = [self.sos_idx] if add_sos else []\n",
        "        for char in list(sequence): indices.append(self.char2index.get(char, self.char2index[UNK_TOKEN]))\n",
        "        if add_eos: indices.append(self.eos_idx)\n",
        "        return indices\n",
        "\n",
        "    def indices_to_sequence(self, indices):\n",
        "        \"\"\"Converts a list of numerical indices back into a character sequence.\"\"\"\n",
        "        chars = []\n",
        "        for index_val in indices:\n",
        "            if index_val == self.eos_idx: break # Stop at EOS token\n",
        "            if index_val not in [self.sos_idx, self.pad_idx]: # Ignore SOS and PAD tokens in output\n",
        "                chars.append(self.index2char.get(index_val, UNK_TOKEN))\n",
        "        return \"\".join(chars)\n",
        "\n",
        "class TransliterationDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A PyTorch Dataset for loading and preparing transliteration pairs.\n",
        "    Reads data from a TSV file and converts text sequences to token indices.\n",
        "    \"\"\"\n",
        "    def __init__(self, file_path, source_vocab, target_vocab, max_len=50):\n",
        "        self.pairs = []\n",
        "        self.source_vocab = source_vocab\n",
        "        self.target_vocab = target_vocab\n",
        "\n",
        "        if not os.path.exists(file_path):\n",
        "            print(f\"ERROR: Data file not found: {file_path}\")\n",
        "            return\n",
        "\n",
        "        print(f\"Loading data from: {file_path}\")\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                parts = line.strip().split('\\t')\n",
        "                if len(parts) >= 2:\n",
        "                    target, source = parts[0], parts[1]\n",
        "                    # Skip empty sequences or sequences exceeding max_len\n",
        "                    if not source or not target or \\\n",
        "                       (max_len and (len(source) > max_len or len(target) > max_len)):\n",
        "                        continue\n",
        "                    self.pairs.append((source, target))\n",
        "        print(f\"Loaded {len(self.pairs)} pairs from {file_path}.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Returns a source-target pair as Tensors of indices.\"\"\"\n",
        "        source_str, target_str = self.pairs[idx]\n",
        "        source_indices = self.source_vocab.sequence_to_indices(source_str, add_eos=True)\n",
        "        target_indices = self.target_vocab.sequence_to_indices(target_str, add_sos=True, add_eos=True)\n",
        "        return torch.tensor(source_indices, dtype=torch.long), torch.tensor(target_indices, dtype=torch.long)\n",
        "\n",
        "def collate_fn(batch, pad_idx_source, pad_idx_target):\n",
        "    \"\"\"\n",
        "    Custom collate function for DataLoader to handle variable-length sequences.\n",
        "    Pads sequences within a batch to the maximum length of that batch.\n",
        "    \"\"\"\n",
        "    # Filter out any None items if __getitem__ could return them\n",
        "    batch = [item for item in batch if item is not None and len(item[0]) > 0 and len(item[1]) > 0]\n",
        "    if not batch: return None, None, None # Return None if batch is empty after filtering\n",
        "\n",
        "    source_seqs, target_seqs = zip(*batch)\n",
        "    source_lengths = torch.tensor([len(s) for s in source_seqs], dtype=torch.long)\n",
        "\n",
        "    # Pad sequences to the length of the longest sequence in the batch\n",
        "    padded_sources = pad_sequence(source_seqs, batch_first=True, padding_value=pad_idx_source)\n",
        "    padded_targets = pad_sequence(target_seqs, batch_first=True, padding_value=pad_idx_target)\n",
        "\n",
        "    return padded_sources, source_lengths, padded_targets\n",
        "\n",
        "# --- Training and Evaluation Functions ---\n",
        "\n",
        "def _train_one_epoch(model, dataloader, optimizer, criterion, device, clip_value, teacher_forcing_ratio, target_vocab):\n",
        "    \"\"\"\n",
        "    Trains the model for a single epoch.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The Seq2Seq model.\n",
        "        dataloader (DataLoader): DataLoader for the training set.\n",
        "        optimizer (optim.Optimizer): Optimizer for model parameters.\n",
        "        criterion (nn.Module): Loss function (e.g., CrossEntropyLoss).\n",
        "        device (torch.device): Device to run the model on (CPU or GPU).\n",
        "        clip_value (float): Gradient clipping value.\n",
        "        teacher_forcing_ratio (float): Probability of using teacher forcing.\n",
        "        target_vocab (Vocabulary): Vocabulary for the target language.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Average epoch loss and training accuracy.\n",
        "    \"\"\"\n",
        "    model.train() # Set model to training mode\n",
        "    epoch_loss = 0\n",
        "    total_correct_train = 0\n",
        "    total_samples_train = 0\n",
        "\n",
        "    if len(dataloader) == 0:\n",
        "        print(\"Warning: Training dataloader is empty.\")\n",
        "        return 0.0, 0.0\n",
        "\n",
        "    for batch_data in tqdm(dataloader, desc=\"Training\", leave=False):\n",
        "        # Skip invalid batches (e.g., from drop_last=True or filtering)\n",
        "        if batch_data[0] is None: continue\n",
        "        sources, source_lengths, targets = batch_data\n",
        "        if sources is None or sources.shape[0] == 0: continue # Skip if sources tensor is empty\n",
        "\n",
        "        sources, targets, source_lengths = sources.to(device), targets.to(device), source_lengths.to(device)\n",
        "        optimizer.zero_grad() # Clear gradients\n",
        "\n",
        "        # Forward pass: model predicts logits for the target sequence\n",
        "        outputs_logits = model(sources, source_lengths, targets, teacher_forcing_ratio)\n",
        "\n",
        "        # Reshape outputs and targets for CrossEntropyLoss\n",
        "        # We ignore the first token (SOS) in the target for loss calculation\n",
        "        output_dim = outputs_logits.shape[-1]\n",
        "        flat_outputs = outputs_logits[:, 1:].reshape(-1, output_dim)\n",
        "        flat_targets = targets[:, 1:].reshape(-1) # Also remove SOS from targets\n",
        "\n",
        "        loss = criterion(flat_outputs, flat_targets)\n",
        "\n",
        "        # Handle potential NaN/Inf loss (e.g., due to bad gradients or initial parameters)\n",
        "        if not (torch.isnan(loss) or torch.isinf(loss)):\n",
        "            loss.backward() # Backpropagation\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value) # Clip gradients to prevent exploding gradients\n",
        "            optimizer.step() # Update model parameters\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        # Calculate training accuracy (exact sequence match)\n",
        "        predictions_indices_train = outputs_logits.argmax(dim=2) # Get predicted token indices\n",
        "        for i in range(targets.shape[0]):\n",
        "            # Convert predicted and true indices to strings for comparison\n",
        "            pred_str_train = target_vocab.indices_to_sequence(predictions_indices_train[i, 1:].tolist())\n",
        "            true_str_train = target_vocab.indices_to_sequence(targets[i, 1:].tolist())\n",
        "            if pred_str_train == true_str_train:\n",
        "                total_correct_train += 1\n",
        "            total_samples_train += 1\n",
        "\n",
        "    avg_epoch_loss = epoch_loss / len(dataloader) if len(dataloader) > 0 else 0.0\n",
        "    train_accuracy = total_correct_train / total_samples_train if total_samples_train > 0 else 0.0\n",
        "    return avg_epoch_loss, train_accuracy\n",
        "\n",
        "def _evaluate_one_epoch(model, dataloader, criterion, device, target_vocab, beam_width=1, is_test_set=False):\n",
        "    \"\"\"\n",
        "    Evaluates the model's performance on a given dataset (validation or test).\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The Seq2Seq model.\n",
        "        dataloader (DataLoader): DataLoader for the evaluation set.\n",
        "        criterion (nn.Module): Loss function.\n",
        "        device (torch.device): Device to run the model on.\n",
        "        target_vocab (Vocabulary): Vocabulary for the target language.\n",
        "        beam_width (int): Beam width for decoding (1 for greedy, >1 for beam search).\n",
        "        is_test_set (bool): Flag to indicate if this is the final test evaluation (for logging and samples).\n",
        "\n",
        "    Returns:\n",
        "        tuple: Average epoch loss and evaluation accuracy.\n",
        "    \"\"\"\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    epoch_loss = 0\n",
        "    total_correct, total_samples = 0, 0\n",
        "\n",
        "    if len(dataloader) == 0:\n",
        "        print(\"WARNING: Evaluation dataloader is empty.\")\n",
        "        return 0.0, 0.0\n",
        "\n",
        "    desc_prefix = \"Testing\" if is_test_set else \"Validating\"\n",
        "\n",
        "    with torch.no_grad(): # Disable gradient calculations during evaluation\n",
        "        for batch_idx, batch_data in enumerate(tqdm(dataloader, desc=desc_prefix, leave=False)):\n",
        "            if batch_data[0] is None: continue\n",
        "            sources, source_lengths, targets = batch_data\n",
        "            if sources is None or sources.shape[0] == 0: continue\n",
        "\n",
        "            sources, targets, source_lengths = sources.to(device), targets.to(device), source_lengths.to(device)\n",
        "\n",
        "            # Forward pass for loss calculation (using teacher forcing=0.0)\n",
        "            outputs_for_loss = model(sources, source_lengths, targets, teacher_forcing_ratio=0.0)\n",
        "            output_dim = outputs_for_loss.shape[-1]\n",
        "            flat_outputs_for_loss = outputs_for_loss[:, 1:].reshape(-1, output_dim)\n",
        "            flat_targets_for_loss = targets[:, 1:].reshape(-1)\n",
        "\n",
        "            loss = criterion(flat_outputs_for_loss, flat_targets_for_loss)\n",
        "            epoch_loss += loss.item() if not (torch.isnan(loss) or torch.isinf(loss)) else 0\n",
        "\n",
        "            # Generate predictions for accuracy calculation for each item in batch\n",
        "            for i in range(sources.shape[0]):\n",
        "                src_single, src_len_single = sources[i:i+1], source_lengths[i:i+1]\n",
        "\n",
        "                # Choose decoding strategy (beam search or greedy)\n",
        "                if beam_width > 1 and hasattr(model, 'predict_beam_search'):\n",
        "                    predicted_indices = model.predict_beam_search(src_single, src_len_single,\n",
        "                                                                  max_output_len=targets.size(1) + 5, # Allow slightly longer output\n",
        "                                                                  target_eos_idx=target_vocab.eos_idx,\n",
        "                                                                  beam_width=beam_width)\n",
        "                else: # Default to greedy if beam_width is 1 or method not found\n",
        "                     predicted_indices = model.predict_greedy(src_single, src_len_single,\n",
        "                                                             max_output_len=targets.size(1) + 5,\n",
        "                                                             target_eos_idx=target_vocab.eos_idx)\n",
        "\n",
        "                # Convert predicted and true indices to strings for comparison\n",
        "                pred_str = target_vocab.indices_to_sequence(predicted_indices)\n",
        "                true_str = target_vocab.indices_to_sequence(targets[i, 1:].tolist()) # Exclude SOS from true target\n",
        "\n",
        "                # Check for exact match\n",
        "                if pred_str == true_str: total_correct += 1\n",
        "                total_samples += 1\n",
        "\n",
        "                # Print debug samples for the test set\n",
        "                if is_test_set and batch_idx == 0 and i < 3: # Print first 3 samples of the first batch\n",
        "                    print(f\"  Test Example {i} - Source: '{source_vocab.indices_to_sequence(src_single[0].tolist())}'\")\n",
        "                    print(f\"    Pred: '{pred_str}', True: '{true_str}', Match: {pred_str == true_str}\")\n",
        "\n",
        "    avg_epoch_loss = epoch_loss / len(dataloader) if len(dataloader) > 0 else 0.0\n",
        "    accuracy = total_correct / total_samples if total_samples > 0 else 0.0\n",
        "\n",
        "    print(f\"{desc_prefix} - Total Correct: {total_correct}, Total Samples: {total_samples}, Accuracy: {accuracy:.4f}, Avg Loss: {avg_epoch_loss:.4f}\")\n",
        "    return avg_epoch_loss, accuracy\n",
        "\n",
        "# --- Function to Train and Save the Best Model ---\n",
        "\n",
        "def train_and_save_best_model(config, model_save_path, device):\n",
        "    \"\"\"\n",
        "    Performs a dedicated training run using the best hyperparameters found from a sweep.\n",
        "    Saves the model checkpoint with the best validation accuracy.\n",
        "\n",
        "    Args:\n",
        "        config (dict): Dictionary of hyperparameters for this specific training run.\n",
        "        model_save_path (str): Path to save the best model's state_dict.\n",
        "        device (torch.device): Device to run the training on.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if training was successful and a model was saved, False otherwise.\n",
        "    \"\"\"\n",
        "    # Create a unique run name for this dedicated training\n",
        "    run_name_train_best = f\"TRAIN_BEST_{config['cell_type']}_emb{config['embedding_dim']}_hid{config['hidden_dim']}\"\n",
        "\n",
        "    # Initialize W&B run for this dedicated training\n",
        "    with wandb.init(project=\"DL_A3\", name=run_name_train_best, config=config, job_type=\"training_best_model\", reinit=True) as run:\n",
        "        cfg = wandb.config # Access hyperparameters via wandb.config\n",
        "        print(f\"Starting dedicated training for best model with config: {cfg}\")\n",
        "\n",
        "        # --- Data Loading and Vocabulary Building ---\n",
        "        BASE_DATA_DIR = \"/kaggle/input/dakshina-dl-a3/dakshina_dataset_v1.0/hi/\"\n",
        "        DATA_DIR = os.path.join(BASE_DATA_DIR, \"lexicons/\")\n",
        "        train_file = os.path.join(DATA_DIR, \"hi.translit.sampled.train.tsv\")\n",
        "        dev_file = os.path.join(DATA_DIR, \"hi.translit.sampled.dev.tsv\")\n",
        "\n",
        "        source_vocab = Vocabulary(\"latin\")\n",
        "        target_vocab = Vocabulary(\"devanagari\")\n",
        "\n",
        "        # Build vocabulary from the training data\n",
        "        temp_train_ds_vocab = TransliterationDataset(train_file, source_vocab, target_vocab, max_len=cfg.max_seq_len)\n",
        "        if not temp_train_ds_vocab.pairs:\n",
        "            print(f\"ERROR: No training data loaded for vocabulary building from {train_file}.\")\n",
        "            return False # Indicate failure\n",
        "        for src, tgt in temp_train_ds_vocab.pairs:\n",
        "            source_vocab.add_sequence(src)\n",
        "            target_vocab.add_sequence(tgt)\n",
        "        source_vocab.build_vocab(min_freq=cfg.vocab_min_freq)\n",
        "        target_vocab.build_vocab(min_freq=cfg.vocab_min_freq)\n",
        "        print(f\"Vocabs built. Source: {source_vocab.n_chars} unique chars, Target: {target_vocab.n_chars} unique chars.\")\n",
        "\n",
        "        # Create actual datasets for training and validation\n",
        "        train_dataset = TransliterationDataset(train_file, source_vocab, target_vocab, max_len=cfg.max_seq_len)\n",
        "        dev_dataset = TransliterationDataset(dev_file, source_vocab, target_vocab, max_len=cfg.max_seq_len)\n",
        "        if not train_dataset.pairs or not dev_dataset.pairs:\n",
        "            print(f\"ERROR: Train or Dev dataset is empty after filtering. Train: {len(train_dataset.pairs)}, Dev: {len(dev_dataset.pairs)}\")\n",
        "            return False\n",
        "\n",
        "        # --- DataLoader Setup ---\n",
        "        # Adjust number of workers based on CPU availability and device\n",
        "        num_w = 0 if device.type == 'cpu' else min(4, os.cpu_count() // 2 if os.cpu_count() else 0)\n",
        "        train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size_train, shuffle=True,\n",
        "                                  collate_fn=lambda b: collate_fn(b, source_vocab.pad_idx, target_vocab.pad_idx),\n",
        "                                  num_workers=num_w, pin_memory=True if device.type == 'cuda' else False, drop_last=True)\n",
        "        dev_loader = DataLoader(dev_dataset, batch_size=cfg.batch_size_train, shuffle=False,\n",
        "                                collate_fn=lambda b: collate_fn(b, source_vocab.pad_idx, target_vocab.pad_idx),\n",
        "                                num_workers=num_w, pin_memory=True if device.type == 'cuda' else False)\n",
        "        if not train_loader or not dev_loader:\n",
        "            print(f\"ERROR: Train or Dev DataLoader is empty. Train: {len(train_loader)}, Dev: {len(dev_loader)}\")\n",
        "            return False\n",
        "\n",
        "        # --- Model, Optimizer, Loss Function Setup ---\n",
        "        encoder = Encoder(source_vocab.n_chars, cfg.embedding_dim, cfg.hidden_dim,\n",
        "                          cfg.encoder_layers, cfg.cell_type, cfg.dropout_p,\n",
        "                          cfg.encoder_bidirectional, pad_idx=source_vocab.pad_idx).to(device)\n",
        "        decoder = Decoder(target_vocab.n_chars, cfg.embedding_dim, cfg.hidden_dim,\n",
        "                          cfg.decoder_layers, cfg.cell_type, cfg.dropout_p,\n",
        "                          pad_idx=target_vocab.pad_idx).to(device)\n",
        "        model = Seq2Seq(encoder, decoder, device, target_vocab.sos_idx).to(device)\n",
        "        print(f\"Best model parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
        "        wandb.watch(model, log=\"all\", log_freq=100) # Log model weights and gradients to W&B\n",
        "\n",
        "        optimizer = optim.Adam(model.parameters(), lr=cfg.learning_rate_train)\n",
        "        # Scheduler to reduce learning rate if validation accuracy plateaus\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=cfg.lr_scheduler_patience_train, factor=0.3, verbose=True)\n",
        "        criterion = nn.CrossEntropyLoss(ignore_index=target_vocab.pad_idx) # Ignore padding tokens in loss calculation\n",
        "\n",
        "        # --- Training Loop ---\n",
        "        best_val_acc_this_training = -1.0\n",
        "        epochs_no_improve = 0 # Counter for early stopping\n",
        "\n",
        "        for epoch in range(cfg.epochs_train):\n",
        "            train_loss, train_acc = _train_one_epoch(model, train_loader, optimizer, criterion, device,\n",
        "                                                 cfg.clip_value_train, cfg.teacher_forcing_train, target_vocab)\n",
        "            # Evaluate on validation set using greedy decoding during training for simplicity\n",
        "            val_loss, val_acc = _evaluate_one_epoch(model, dev_loader, criterion, device, target_vocab,\n",
        "                                                beam_width=1)\n",
        "\n",
        "            scheduler.step(val_acc) # Update learning rate based on validation accuracy\n",
        "\n",
        "            print(f\"Epoch {epoch+1}/{cfg.epochs_train} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "            # Log metrics to W&B\n",
        "            wandb.log({\"epoch_train_best\": epoch + 1, \"train_loss_best\": train_loss, \"train_acc_best\": train_acc,\n",
        "                       \"val_loss_best\": val_loss, \"val_acc_best\": val_acc, \"lr_best\": optimizer.param_groups[0]['lr']})\n",
        "\n",
        "            # Early stopping logic\n",
        "            if val_acc > best_val_acc_this_training:\n",
        "                best_val_acc_this_training = val_acc\n",
        "                epochs_no_improve = 0\n",
        "                torch.save(model.state_dict(), model_save_path) # Save best model\n",
        "                print(f\"Saved new best model to {model_save_path} (Val Acc: {best_val_acc_this_training:.4f})\")\n",
        "            else:\n",
        "                epochs_no_improve += 1\n",
        "\n",
        "            if epochs_no_improve >= cfg.max_epochs_no_improve_train:\n",
        "                print(f\"Early stopping for best model training at epoch {epoch+1}.\")\n",
        "                break\n",
        "\n",
        "        # Save the final model state if no improvement happened over the initial checkpoint (though this shouldn't be the best)\n",
        "        if not os.path.exists(model_save_path):\n",
        "            torch.save(model.state_dict(), model_save_path)\n",
        "            print(f\"Saved final model state (no improvement over initial) to {model_save_path}\")\n",
        "\n",
        "        wandb.summary[\"final_best_val_accuracy_during_training\"] = best_val_acc_this_training\n",
        "        print(f\"Finished training best model. Best Val Acc: {best_val_acc_this_training:.4f}\")\n",
        "    return True # Indicate successful training\n",
        "\n",
        "# --- Main Execution Block ---\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Determine the device (GPU if available, else CPU)\n",
        "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "    # --- W&B Login ---\n",
        "    try:\n",
        "        # Handle W&B login for Kaggle environments\n",
        "        if 'KAGGLE_KERNEL_RUN_TYPE' in os.environ:\n",
        "            print(\"Detected Kaggle environment. Ensuring WANDB_API_KEY is set.\")\n",
        "            if \"WANDB_API_KEY\" in os.environ:\n",
        "                wandb.login()\n",
        "            else:\n",
        "                # Attempt to get API key from Kaggle secrets\n",
        "                from kaggle_secrets import UserSecretsClient\n",
        "                user_secrets = UserSecretsClient()\n",
        "                wandb_api_key = user_secrets.get_secret(\"WANDB_API_KEY\")\n",
        "                wandb.login(key=wandb_api_key)\n",
        "            print(\"W&B login for Kaggle successful.\")\n",
        "        else:\n",
        "            wandb.login() # Regular W&B login for local environments\n",
        "        print(\"W&B login process attempted/completed.\")\n",
        "    except Exception as e:\n",
        "        print(f\"W&B login failed: {e}. Please ensure your W&B API key is correctly configured.\")\n",
        "        exit() # Exit if W&B login fails\n",
        "\n",
        "    # --- Best Hyperparameters (obtained from a previous sweep) ---\n",
        "    # These hyperparameters are chosen based on the provided sweep results (e.g., from the best run shown in comments).\n",
        "    BEST_HYPERPARAMETERS = {\n",
        "        'embedding_dim': 300,\n",
        "        'hidden_dim': 512,\n",
        "        'encoder_layers': 2,\n",
        "        'decoder_layers': 1,\n",
        "        'cell_type': 'LSTM',\n",
        "        'dropout_p': 0.5,\n",
        "        'encoder_bidirectional': True,\n",
        "        # Training specific parameters for this dedicated run:\n",
        "        'learning_rate_train': 0.000376, # Learning rate from the best sweep run\n",
        "        'batch_size_train': 128,         # Batch size from the best sweep run\n",
        "        'epochs_train': 20,              # Max epochs for this dedicated training\n",
        "        'clip_value_train': 1.0,         # Gradient clipping value\n",
        "        'teacher_forcing_train': 0.5,    # Teacher forcing ratio for this training\n",
        "        'max_epochs_no_improve_train': 7, # Early stopping patience for this training run\n",
        "        'lr_scheduler_patience_train': 3, # Patience for LR scheduler\n",
        "        # Parameters needed for dataset/vocabulary consistency:\n",
        "        'vocab_min_freq': 1,\n",
        "        'max_seq_len': 50,\n",
        "        # Parameters for final test evaluation:\n",
        "        'eval_batch_size': 128,          # Batch size for evaluation on test set\n",
        "        'beam_width_eval': 3             # Beam width for final test evaluation\n",
        "    }\n",
        "    MODEL_SAVE_PATH = \"/kaggle/working/best_model_for_testing.pt\" # Path to save the best model checkpoint\n",
        "\n",
        "    print(f\"Best hyperparameters selected for dedicated training and testing: {BEST_HYPERPARAMETERS}\")\n",
        "\n",
        "    # --- Phase 1: Train and Save the Best Model ---\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"--- Phase 1: Training and Saving Best Model Configuration ---\".center(80))\n",
        "    print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "    training_successful = train_and_save_best_model(BEST_HYPERPARAMETERS, MODEL_SAVE_PATH, DEVICE)\n",
        "\n",
        "    if not training_successful or not os.path.exists(MODEL_SAVE_PATH):\n",
        "        print(\"ERROR: Failed to train and save the best model. Exiting before test evaluation.\")\n",
        "        exit()\n",
        "    print(f\"\\nBest model trained and saved to {MODEL_SAVE_PATH}\")\n",
        "\n",
        "    # --- Phase 2: Load and Evaluate on Test Set ---\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"--- Phase 2: Loading and Evaluating Best Model on Test Set ---\".center(80))\n",
        "    print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "    # --- Data Setup for Test Evaluation ---\n",
        "    # Need to rebuild vocab using training data to ensure consistency before processing test data\n",
        "    BASE_DATA_DIR = \"/kaggle/input/dakshina-dl-a3/dakshina_dataset_v1.0/hi/\"\n",
        "    DATA_DIR = os.path.join(BASE_DATA_DIR, \"lexicons/\")\n",
        "    train_file_for_vocab = os.path.join(DATA_DIR, \"hi.translit.sampled.train.tsv\")\n",
        "    test_file = os.path.join(DATA_DIR, \"hi.translit.sampled.test.tsv\")\n",
        "\n",
        "    source_vocab_test = Vocabulary(\"latin_test\")\n",
        "    target_vocab_test = Vocabulary(\"devanagari_test\")\n",
        "\n",
        "    # Build vocabulary using the training dataset (crucial for consistent tokenization)\n",
        "    temp_train_ds_test_vocab = TransliterationDataset(train_file_for_vocab, source_vocab_test, target_vocab_test,\n",
        "                                                max_len=BEST_HYPERPARAMETERS['max_seq_len'])\n",
        "    if not temp_train_ds_test_vocab.pairs:\n",
        "        print(f\"ERROR: No data loaded for vocabulary building from {train_file_for_vocab}.\")\n",
        "        exit()\n",
        "    for src, tgt in temp_train_ds_test_vocab.pairs:\n",
        "        source_vocab_test.add_sequence(src)\n",
        "        target_vocab_test.add_sequence(tgt)\n",
        "    source_vocab_test.build_vocab(min_freq=BEST_HYPERPARAMETERS['vocab_min_freq'])\n",
        "    target_vocab_test.build_vocab(min_freq=BEST_HYPERPARAMETERS['vocab_min_freq'])\n",
        "    print(f\"Test Vocab built from training data. Source: {source_vocab_test.n_chars} chars. Target: {target_vocab_test.n_chars} chars.\")\n",
        "\n",
        "    # Create the test dataset\n",
        "    test_dataset = TransliterationDataset(test_file, source_vocab_test, target_vocab_test,\n",
        "                                          max_len=BEST_HYPERPARAMETERS['max_seq_len'])\n",
        "    if not test_dataset.pairs:\n",
        "        print(f\"ERROR: Test dataset is empty from {test_file}.\")\n",
        "        exit()\n",
        "\n",
        "    # Setup test DataLoader\n",
        "    num_w_test = 0 if DEVICE.type == 'cpu' else min(4, os.cpu_count() // 2 if os.cpu_count() else 0)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=BEST_HYPERPARAMETERS['eval_batch_size'], shuffle=False,\n",
        "                                 collate_fn=lambda b: collate_fn(b, source_vocab_test.pad_idx, target_vocab_test.pad_idx),\n",
        "                                 num_workers=num_w_test, pin_memory=True if DEVICE.type == 'cuda' else False)\n",
        "    if not test_dataloader:\n",
        "        print(f\"ERROR: Test DataLoader is empty.\")\n",
        "        exit()\n",
        "\n",
        "    # --- Initialize Model for Testing and Load Weights ---\n",
        "    # Instantiate the model architecture with the same hyperparameters as the trained model\n",
        "    encoder_test = Encoder(source_vocab_test.n_chars, BEST_HYPERPARAMETERS['embedding_dim'], BEST_HYPERPARAMETERS['hidden_dim'],\n",
        "                           BEST_HYPERPARAMETERS['encoder_layers'], BEST_HYPERPARAMETERS['cell_type'], BEST_HYPERPARAMETERS['dropout_p'],\n",
        "                           BEST_HYPERPARAMETERS['encoder_bidirectional'], pad_idx=source_vocab_test.pad_idx).to(DEVICE)\n",
        "    decoder_test = Decoder(target_vocab_test.n_chars, BEST_HYPERPARAMETERS['embedding_dim'], BEST_HYPERPARAMETERS['hidden_dim'],\n",
        "                           BEST_HYPERPARAMETERS['decoder_layers'], BEST_HYPERPARAMETERS['cell_type'], BEST_HYPERPARAMETERS['dropout_p'],\n",
        "                           pad_idx=target_vocab_test.pad_idx).to(DEVICE)\n",
        "    model_test = Seq2Seq(encoder_test, decoder_test, DEVICE, target_vocab_test.sos_idx).to(DEVICE)\n",
        "\n",
        "    print(f\"Loading weights into test model from: {MODEL_SAVE_PATH}\")\n",
        "    model_test.load_state_dict(torch.load(MODEL_SAVE_PATH, map_location=DEVICE)) # Load saved weights\n",
        "    print(\"Weights loaded successfully for test evaluation.\")\n",
        "\n",
        "    # --- Evaluate on Test Set ---\n",
        "    criterion_test = nn.CrossEntropyLoss(ignore_index=target_vocab_test.pad_idx) # Use the same loss criterion\n",
        "    test_loss, test_accuracy = _evaluate_one_epoch(model_test, test_dataloader, criterion_test, DEVICE,\n",
        "                                                   target_vocab_test,\n",
        "                                                   beam_width=BEST_HYPERPARAMETERS.get('beam_width_eval', 1), # Use the eval beam width\n",
        "                                                   is_test_set=True) # Flag for printing test samples\n",
        "\n",
        "    print(f\"\\n\" + \"=\"*80)\n",
        "    print(f\"--- FINAL TEST SET PERFORMANCE ---\".center(80))\n",
        "    print(f\"=\"*80)\n",
        "    print(f\"  Model Checkpoint: '{MODEL_SAVE_PATH}'\")\n",
        "    print(f\"  Hyperparameters used for training and testing: {BEST_HYPERPARAMETERS}\")\n",
        "    print(f\"  Test Loss (avg per batch): {test_loss:.4f}\")\n",
        "    print(f\"  Test Accuracy (Exact Match): {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
        "    print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "    # --- Log Final Test Results to W&B ---\n",
        "    try:\n",
        "        run_name_final_test = f\"FINAL_TEST_EVAL_{BEST_HYPERPARAMETERS['cell_type']}_beam{BEST_HYPERPARAMETERS['beam_width_eval']}\"\n",
        "        # Start a new W&B run to log only the final test results\n",
        "        with wandb.init(project=\"DL_A3\", name=run_name_final_test, config=BEST_HYPERPARAMETERS, job_type=\"final_evaluation\", reinit=True) as test_run:\n",
        "            test_run.summary[\"final_test_accuracy\"] = test_accuracy\n",
        "            test_run.summary[\"final_test_loss\"] = test_loss\n",
        "            test_run.summary[\"model_checkpoint_used\"] = MODEL_SAVE_PATH\n",
        "            print(\"Final test results logged to W&B.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not log final test results to W&B: {e}\")"
      ],
      "metadata": {
        "id": "_iM8i33xnuyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_ykkmlrcqitF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q5 Attention based model"
      ],
      "metadata": {
        "id": "iaoKVxUbqmlo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import numpy as np\n",
        "import wandb\n",
        "import os\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "import heapq\n",
        "\n",
        "# --- Constants for Special Tokens ---\n",
        "SOS_TOKEN = \"<sos>\"  # Start-of-sequence token\n",
        "EOS_TOKEN = \"<eos>\"  # End-of-sequence token\n",
        "PAD_TOKEN = \"<pad>\"  # Padding token\n",
        "UNK_TOKEN = \"<unk>\"  # Unknown token\n",
        "\n",
        "# --- Model Definitions (Non-Attention Version) ---\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    The Encoder processes the input sequence and produces a context vector (final hidden state).\n",
        "    It uses a recurrent neural network (RNN, GRU, or LSTM).\n",
        "    \"\"\"\n",
        "    def __init__(self, input_vocab_size, embedding_dim, hidden_dim, n_layers,\n",
        "                 cell_type='LSTM', dropout_p=0.1, bidirectional=False, pad_idx=0):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.input_vocab_size = input_vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.cell_type = cell_type.upper()\n",
        "        self.bidirectional = bidirectional\n",
        "        self.num_directions = 2 if bidirectional else 1\n",
        "\n",
        "        self.embedding = nn.Embedding(input_vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "        rnn_dropout = dropout_p if n_layers > 1 else 0\n",
        "        if self.cell_type == 'RNN':\n",
        "            self.rnn = nn.RNN(embedding_dim, hidden_dim, n_layers,\n",
        "                              dropout=rnn_dropout, batch_first=True, bidirectional=self.bidirectional)\n",
        "        elif self.cell_type == 'GRU':\n",
        "            self.rnn = nn.GRU(embedding_dim, hidden_dim, n_layers,\n",
        "                              dropout=rnn_dropout, batch_first=True, bidirectional=self.bidirectional)\n",
        "        elif self.cell_type == 'LSTM':\n",
        "            self.rnn = nn.LSTM(embedding_dim, hidden_dim, n_layers,\n",
        "                               dropout=rnn_dropout, batch_first=True, bidirectional=self.bidirectional)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported cell type. Choose from 'RNN', 'GRU', 'LSTM'.\")\n",
        "\n",
        "    def forward(self, input_seq, input_lengths):\n",
        "        \"\"\"\n",
        "        Forward pass for the encoder.\n",
        "\n",
        "        Args:\n",
        "            input_seq (torch.Tensor): Padded input sequences of shape (batch_size, seq_len).\n",
        "            input_lengths (torch.Tensor): Lengths of the original sequences in the batch of shape (batch_size,).\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple (None, hidden_state).\n",
        "                   The first element is None because this encoder does not output full sequences for attention.\n",
        "                   The second element is the final hidden state of the encoder, which serves as the context.\n",
        "                   For LSTM, `hidden_state` is a tuple (h, c).\n",
        "        \"\"\"\n",
        "        embedded = self.embedding(input_seq)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        # Pack the padded sequences to handle variable-length inputs efficiently\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(\n",
        "            embedded, input_lengths.cpu(), batch_first=True, enforce_sorted=False\n",
        "        )\n",
        "        # Pass packed sequences through the RNN\n",
        "        _, hidden = self.rnn(packed_embedded)\n",
        "        return None, hidden\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    \"\"\"\n",
        "    The Decoder generates the output sequence one token at a time, conditioned on the\n",
        "    encoder's final hidden state and previously generated tokens.\n",
        "    This version does NOT use attention.\n",
        "    \"\"\"\n",
        "    def __init__(self, output_vocab_size, embedding_dim,\n",
        "                 decoder_hidden_dim, n_layers, cell_type='LSTM', dropout_p=0.1, pad_idx=0):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.output_vocab_size = output_vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.decoder_hidden_dim = decoder_hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.cell_type = cell_type.upper()\n",
        "\n",
        "        self.embedding = nn.Embedding(output_vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "        rnn_input_dim = embedding_dim\n",
        "\n",
        "        rnn_dropout = dropout_p if n_layers > 1 else 0\n",
        "        if self.cell_type == 'RNN':\n",
        "            self.rnn = nn.RNN(rnn_input_dim, decoder_hidden_dim, n_layers,\n",
        "                              dropout=rnn_dropout, batch_first=True)\n",
        "        elif self.cell_type == 'GRU':\n",
        "            self.rnn = nn.GRU(rnn_input_dim, decoder_hidden_dim, n_layers,\n",
        "                              dropout=rnn_dropout, batch_first=True)\n",
        "        elif self.cell_type == 'LSTM':\n",
        "            self.rnn = nn.LSTM(rnn_input_dim, decoder_hidden_dim, n_layers,\n",
        "                               dropout=rnn_dropout, batch_first=True)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported cell type. Choose from 'RNN', 'GRU', 'LSTM'.\")\n",
        "\n",
        "        # Output linear layer to project decoder's hidden state to vocabulary size\n",
        "        self.fc_out = nn.Linear(decoder_hidden_dim, output_vocab_size)\n",
        "\n",
        "    def forward(self, input_char, prev_decoder_hidden):\n",
        "        \"\"\"\n",
        "        Forward pass for the decoder.\n",
        "\n",
        "        Args:\n",
        "            input_char (torch.Tensor): A single token (or batch of single tokens) to be embedded,\n",
        "                                       shape (batch_size,).\n",
        "            prev_decoder_hidden (torch.Tensor or tuple): The previous hidden state (and cell state for LSTM)\n",
        "                                                         of the decoder RNN.\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple (prediction_logits, current_decoder_hidden).\n",
        "                   `prediction_logits` are the raw scores for each token in the vocabulary.\n",
        "                   `current_decoder_hidden` is the updated hidden state after processing the input.\n",
        "        \"\"\"\n",
        "        # Add a sequence dimension for RNN input (batch_size, 1, embedding_dim)\n",
        "        input_char = input_char.unsqueeze(1)\n",
        "        embedded = self.embedding(input_char)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        # Pass through the RNN layer\n",
        "        rnn_output, current_decoder_hidden = self.rnn(embedded, prev_decoder_hidden)\n",
        "\n",
        "        # Squeeze the sequence dimension for the linear layer\n",
        "        rnn_output_squeezed = rnn_output.squeeze(1)\n",
        "        # Project to vocabulary size to get logits\n",
        "        prediction_logits = self.fc_out(rnn_output_squeezed)\n",
        "        return prediction_logits, current_decoder_hidden\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    \"\"\"\n",
        "    The main Sequence-to-Sequence model that connects the Encoder and Decoder.\n",
        "    This architecture does NOT use an attention mechanism.\n",
        "    \"\"\"\n",
        "    def __init__(self, encoder, decoder, device, target_sos_idx):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "        self.target_sos_idx = target_sos_idx\n",
        "\n",
        "        # Calculate effective dimensions for hidden state adaptation\n",
        "        encoder_effective_output_dim_per_layer = self.encoder.hidden_dim * self.encoder.num_directions\n",
        "        decoder_rnn_expected_hidden_dim = self.decoder.decoder_hidden_dim\n",
        "\n",
        "        # Check if hidden state dimensions need to be adapted from encoder to decoder\n",
        "        self.needs_dim_adaptation = encoder_effective_output_dim_per_layer != decoder_rnn_expected_hidden_dim\n",
        "        self.fc_adapt_hidden = None\n",
        "        self.fc_adapt_cell = None\n",
        "\n",
        "        # Create linear layers for hidden state adaptation if necessary\n",
        "        if self.needs_dim_adaptation:\n",
        "            self.fc_adapt_hidden = nn.Linear(encoder_effective_output_dim_per_layer, decoder_rnn_expected_hidden_dim)\n",
        "            if self.encoder.cell_type == 'LSTM':\n",
        "                self.fc_adapt_cell = nn.Linear(encoder_effective_output_dim_per_layer, decoder_rnn_expected_hidden_dim)\n",
        "\n",
        "    def _adapt_encoder_hidden_for_decoder(self, encoder_final_hidden_state):\n",
        "        \"\"\"\n",
        "        Adapts the encoder's final hidden state(s) to match the decoder's expected hidden state dimensions and layer count.\n",
        "        Handles bidirectionality by concatenating forward and backward hidden states.\n",
        "        If decoder has more layers than encoder, the last encoder layer's state is repeated.\n",
        "        \"\"\"\n",
        "        is_lstm = self.encoder.cell_type == 'LSTM'\n",
        "        if is_lstm:\n",
        "            h_from_enc, c_from_enc = encoder_final_hidden_state\n",
        "        else:\n",
        "            h_from_enc = encoder_final_hidden_state\n",
        "            c_from_enc = None\n",
        "\n",
        "        batch_size = h_from_enc.size(1)\n",
        "\n",
        "        # Reshape encoder hidden state\n",
        "        h_processed = h_from_enc.view(self.encoder.n_layers, self.encoder.num_directions,\n",
        "                                     batch_size, self.encoder.hidden_dim)\n",
        "        if self.encoder.bidirectional:\n",
        "            h_processed = torch.cat([h_processed[:, 0, :, :], h_processed[:, 1, :, :]], dim=2)\n",
        "        else:\n",
        "            h_processed = h_processed.squeeze(1)\n",
        "\n",
        "        c_processed = None\n",
        "        if is_lstm and c_from_enc is not None:\n",
        "            c_processed = c_from_enc.view(self.encoder.n_layers, self.encoder.num_directions,\n",
        "                                         batch_size, self.encoder.hidden_dim)\n",
        "            if self.encoder.bidirectional:\n",
        "                c_processed = torch.cat([c_processed[:, 0, :, :], c_processed[:, 1, :, :]], dim=2)\n",
        "            else:\n",
        "                c_processed = c_processed.squeeze(1)\n",
        "\n",
        "        # Apply linear transformation if hidden dimensions mismatch\n",
        "        if self.needs_dim_adaptation:\n",
        "            h_processed = self.fc_adapt_hidden(h_processed)\n",
        "            if is_lstm and c_processed is not None and self.fc_adapt_cell:\n",
        "                c_processed = self.fc_adapt_cell(c_processed)\n",
        "\n",
        "        # Initialize decoder's hidden state(s)\n",
        "        final_h = torch.zeros(self.decoder.n_layers, batch_size, self.decoder.decoder_hidden_dim, device=self.device)\n",
        "        final_c = torch.zeros(self.decoder.n_layers, batch_size, self.decoder.decoder_hidden_dim, device=self.device) if is_lstm else None\n",
        "\n",
        "        # Copy or repeat encoder hidden states to match decoder's layer count\n",
        "        if self.encoder.n_layers == self.decoder.n_layers:\n",
        "            final_h = h_processed\n",
        "            if is_lstm: final_c = c_processed\n",
        "        elif self.encoder.n_layers > self.decoder.n_layers:\n",
        "            final_h = h_processed[-self.decoder.n_layers:, :, :]\n",
        "            if is_lstm and c_processed is not None:\n",
        "                final_c = c_processed[-self.decoder.n_layers:, :, :]\n",
        "        else:\n",
        "            final_h[:self.encoder.n_layers, :, :] = h_processed\n",
        "            if is_lstm and c_processed is not None:\n",
        "                final_c[:self.encoder.n_layers, :, :] = c_processed\n",
        "\n",
        "            if self.encoder.n_layers > 0:\n",
        "                last_h_layer_to_repeat = h_processed[self.encoder.n_layers-1, :, :]\n",
        "                for i in range(self.encoder.n_layers, self.decoder.n_layers):\n",
        "                    final_h[i, :, :] = last_h_layer_to_repeat\n",
        "                    if is_lstm and c_processed is not None:\n",
        "                        last_c_layer_to_repeat = c_processed[self.encoder.n_layers-1, :, :]\n",
        "                        final_c[i, :, :] = last_c_layer_to_repeat\n",
        "\n",
        "        return (final_h, final_c) if is_lstm else final_h\n",
        "\n",
        "    def forward(self, source_seq, source_lengths, target_seq, teacher_forcing_ratio=0.5):\n",
        "        \"\"\"\n",
        "        Forward pass for the Seq2Seq model during training.\n",
        "\n",
        "        Args:\n",
        "            source_seq (torch.Tensor): Padded source sequences.\n",
        "            source_lengths (torch.Tensor): Lengths of source sequences.\n",
        "            target_seq (torch.Tensor): Padded target sequences (including SOS token).\n",
        "            teacher_forcing_ratio (float): Probability of using actual target token as next input.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Logits for the predicted target sequence.\n",
        "        \"\"\"\n",
        "        batch_size = source_seq.shape[0]\n",
        "        target_len = target_seq.shape[1]\n",
        "        target_vocab_size = self.decoder.output_vocab_size\n",
        "        outputs_logits = torch.zeros(batch_size, target_len, target_vocab_size).to(self.device)\n",
        "\n",
        "        # Encode the source sequence to get the initial hidden state for the decoder\n",
        "        _, encoder_final_hidden = self.encoder(source_seq, source_lengths)\n",
        "        # Adapt encoder hidden state to decoder's expected shape\n",
        "        decoder_hidden = self._adapt_encoder_hidden_for_decoder(encoder_final_hidden)\n",
        "\n",
        "        # First input to the decoder is the <sos> token\n",
        "        decoder_input = target_seq[:, 0]\n",
        "\n",
        "        # Iterate through the target sequence, predicting one token at a time\n",
        "        for t in range(target_len - 1):\n",
        "            decoder_output_logits, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
        "            outputs_logits[:, t+1] = decoder_output_logits\n",
        "\n",
        "            # Decide whether to use teacher forcing\n",
        "            teacher_force_this_step = random.random() < teacher_forcing_ratio\n",
        "            top1_predicted_token = decoder_output_logits.argmax(1)\n",
        "\n",
        "            # Use actual target token (teacher forcing) or predicted token for the next step\n",
        "            decoder_input = target_seq[:, t+1] if teacher_force_this_step else top1_predicted_token\n",
        "        return outputs_logits\n",
        "\n",
        "    def predict_greedy(self, source_seq, source_lengths, max_output_len=50, target_eos_idx=None):\n",
        "        \"\"\"\n",
        "        Generates a sequence using greedy decoding.\n",
        "\n",
        "        Args:\n",
        "            source_seq (torch.Tensor): Input source sequence (can be a single example or a batch of 1).\n",
        "            source_lengths (torch.Tensor): Length of the source sequence.\n",
        "            max_output_len (int): Maximum length of the sequence to generate.\n",
        "            target_eos_idx (int, optional): Index of the EOS token in the target vocabulary.\n",
        "\n",
        "        Returns:\n",
        "            list: List of predicted token indices.\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        if source_seq.dim() == 1:\n",
        "            source_seq = source_seq.unsqueeze(0)\n",
        "            source_lengths = torch.tensor([source_lengths if isinstance(source_lengths, int) else len(source_lengths)], device=self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            _, encoder_final_hidden = self.encoder(source_seq, source_lengths)\n",
        "            decoder_hidden = self._adapt_encoder_hidden_for_decoder(encoder_final_hidden)\n",
        "            decoder_input = torch.tensor([self.target_sos_idx], device=self.device)\n",
        "            predicted_indices = []\n",
        "            for _ in range(max_output_len):\n",
        "                decoder_output_logits, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
        "                top1_predicted_token = decoder_output_logits.argmax(1)\n",
        "                predicted_idx = top1_predicted_token.item()\n",
        "                if target_eos_idx is not None and predicted_idx == target_eos_idx:\n",
        "                    break\n",
        "                predicted_indices.append(predicted_idx)\n",
        "                decoder_input = top1_predicted_token\n",
        "        return predicted_indices\n",
        "\n",
        "    def predict_beam_search(self, source_seq, source_lengths, max_output_len=50, target_eos_idx=None, beam_width=3):\n",
        "        \"\"\"\n",
        "        Generates a sequence using beam search decoding.\n",
        "\n",
        "        Args:\n",
        "            source_seq (torch.Tensor): Input source sequence (must be a single example).\n",
        "            source_lengths (torch.Tensor): Length of the source sequence.\n",
        "            max_output_len (int): Maximum length of the sequence to generate.\n",
        "            target_eos_idx (int, optional): Index of the EOS token.\n",
        "            beam_width (int): The number of top sequences to keep at each step.\n",
        "\n",
        "        Returns:\n",
        "            list: List of predicted token indices for the best sequence found.\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        if source_seq.dim() == 1:\n",
        "            source_seq = source_seq.unsqueeze(0)\n",
        "            source_lengths = torch.tensor([source_lengths if isinstance(source_lengths, int) else len(source_lengths)], device=self.device)\n",
        "\n",
        "        if source_seq.shape[0] != 1:\n",
        "            raise ValueError(\"Beam search predict function currently supports batch_size=1.\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            _, encoder_final_hidden = self.encoder(source_seq, source_lengths)\n",
        "            decoder_hidden_init = self._adapt_encoder_hidden_for_decoder(encoder_final_hidden)\n",
        "\n",
        "            # Beams are stored as (cumulative_log_probability, sequence_of_indices, decoder_hidden_state)\n",
        "            beams = [(0.0, [self.target_sos_idx], decoder_hidden_init)]\n",
        "            completed_sequences = []\n",
        "\n",
        "            for _ in range(max_output_len):\n",
        "                new_beams = []\n",
        "                all_current_beams_ended = True\n",
        "                for log_prob_beam, seq_beam, hidden_beam in beams:\n",
        "                    # If this beam has already ended, move it to completed sequences and skip expansion\n",
        "                    if not seq_beam or seq_beam[-1] == target_eos_idx:\n",
        "                        # Normalize log probability by length to counteract bias towards shorter sequences\n",
        "                        completed_sequences.append((log_prob_beam / len(seq_beam) if len(seq_beam) > 0 else -float('inf'), seq_beam))\n",
        "                        continue\n",
        "                    all_current_beams_ended = False\n",
        "\n",
        "                    decoder_input = torch.tensor([seq_beam[-1]], device=self.device)\n",
        "                    decoder_output_logits, next_hidden_beam = self.decoder(decoder_input, hidden_beam)\n",
        "\n",
        "                    # Get top K next tokens (log probabilities)\n",
        "                    log_probs_next_token = F.log_softmax(decoder_output_logits, dim=1)\n",
        "                    topk_log_probs, topk_indices = torch.topk(log_probs_next_token, beam_width, dim=1)\n",
        "\n",
        "                    # Expand each current beam into `beam_width` new beams\n",
        "                    for k in range(beam_width):\n",
        "                        next_token_idx = topk_indices[0, k].item()\n",
        "                        token_log_prob = topk_log_probs[0, k].item()\n",
        "\n",
        "                        new_seq = seq_beam + [next_token_idx]\n",
        "                        new_log_prob = log_prob_beam + token_log_prob\n",
        "                        new_beams.append((new_log_prob, new_seq, next_hidden_beam))\n",
        "\n",
        "                if not new_beams or all_current_beams_ended:\n",
        "                    break # No new beams to explore or all current beams ended\n",
        "\n",
        "                # Sort all new candidate beams by their log probability and keep top `beam_width`\n",
        "                new_beams.sort(key=lambda x: x[0], reverse=True)\n",
        "                beams = new_beams[:beam_width]\n",
        "\n",
        "            # Add any remaining active beams to completed sequences\n",
        "            for log_prob_beam, seq_beam, _ in beams:\n",
        "                completed_sequences.append((log_prob_beam / len(seq_beam) if len(seq_beam) > 0 else -float('inf'), seq_beam))\n",
        "\n",
        "            if not completed_sequences:\n",
        "                # Fallback if somehow no sequences were completed or started\n",
        "                return [target_eos_idx] if target_eos_idx is not None else []\n",
        "            completed_sequences.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "            best_sequence_indices = completed_sequences[0][1]\n",
        "            # Remove SOS from the beginning if it was added\n",
        "            return best_sequence_indices[1:] if best_sequence_indices and best_sequence_indices[0] == self.target_sos_idx else best_sequence_indices\n",
        "\n",
        "# --- Data Loading and Preprocessing ---\n",
        "\n",
        "class Vocabulary:\n",
        "    \"\"\"\n",
        "    Manages the mapping between characters and their numerical indices.\n",
        "    Includes special tokens for padding, start-of-sequence, end-of-sequence, and unknown characters.\n",
        "    \"\"\"\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.char2index = {PAD_TOKEN: 0, SOS_TOKEN: 1, EOS_TOKEN: 2, UNK_TOKEN: 3}\n",
        "        self.index2char = {0: PAD_TOKEN, 1: SOS_TOKEN, 2: EOS_TOKEN, 3: UNK_TOKEN}\n",
        "        self.char_counts = Counter()\n",
        "        self.n_chars = 4\n",
        "        self.pad_idx = self.char2index[PAD_TOKEN]\n",
        "        self.sos_idx = self.char2index[SOS_TOKEN]\n",
        "        self.eos_idx = self.char2index[EOS_TOKEN]\n",
        "\n",
        "    def add_sequence(self, sequence):\n",
        "        \"\"\"Adds characters from a sequence to the character counts.\"\"\"\n",
        "        for char in list(sequence):\n",
        "            self.char_counts[char] += 1\n",
        "\n",
        "    def build_vocab(self, min_freq=1):\n",
        "        \"\"\"\n",
        "        Builds the vocabulary mapping based on character counts and a minimum frequency.\n",
        "        Characters appearing less than `min_freq` will be treated as UNK_TOKEN.\n",
        "        \"\"\"\n",
        "        sorted_chars = sorted(self.char_counts.keys(), key=lambda char: (-self.char_counts[char], char))\n",
        "        for char in sorted_chars:\n",
        "            if self.char_counts[char] >= min_freq and char not in self.char2index:\n",
        "                self.char2index[char] = self.n_chars\n",
        "                self.index2char[self.n_chars] = char\n",
        "                self.n_chars += 1\n",
        "\n",
        "    def sequence_to_indices(self, sequence, add_eos=False, add_sos=False):\n",
        "        \"\"\"Converts a character sequence into a list of numerical indices.\"\"\"\n",
        "        indices = []\n",
        "        if add_sos:\n",
        "            indices.append(self.sos_idx)\n",
        "        for char in list(sequence):\n",
        "            indices.append(self.char2index.get(char, self.char2index[UNK_TOKEN]))\n",
        "        if add_eos:\n",
        "            indices.append(self.eos_idx)\n",
        "        return indices\n",
        "\n",
        "    def indices_to_sequence(self, indices):\n",
        "        \"\"\"Converts a list of numerical indices back into a character sequence.\"\"\"\n",
        "        chars = []\n",
        "        for index_val in indices:\n",
        "            if index_val == self.eos_idx:\n",
        "                break # Stop at EOS token\n",
        "            if index_val != self.sos_idx and index_val != self.pad_idx:\n",
        "                 chars.append(self.index2char.get(index_val, UNK_TOKEN))\n",
        "        return \"\".join(chars)\n",
        "\n",
        "\n",
        "class TransliterationDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A PyTorch Dataset for loading and preparing transliteration pairs.\n",
        "    Reads data from a TSV file and converts text sequences to token indices.\n",
        "    \"\"\"\n",
        "    def __init__(self, file_path, source_vocab, target_vocab, max_len=50):\n",
        "        self.pairs = []\n",
        "        self.source_vocab = source_vocab\n",
        "        self.target_vocab = target_vocab\n",
        "\n",
        "        if not os.path.exists(file_path):\n",
        "            print(f\"ERROR: Data file not found during Dataset init: {file_path}\")\n",
        "            return\n",
        "\n",
        "        print(f\"Loading data from: {file_path}\")\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                for line_num, line in enumerate(f):\n",
        "                    parts = line.strip().split('\\t')\n",
        "                    if len(parts) >= 2:\n",
        "                        target_sequence, source_sequence = parts[0], parts[1]\n",
        "\n",
        "                        if max_len and (len(source_sequence) > max_len or len(target_sequence) > max_len):\n",
        "                            continue\n",
        "                        if not source_sequence or not target_sequence:\n",
        "                            continue\n",
        "                        self.pairs.append((source_sequence, target_sequence))\n",
        "            print(f\"Loaded {len(self.pairs)} pairs from {file_path}.\")\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR: Could not read or process file {file_path}. Error: {e}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Returns a source-target pair as Tensors of indices.\"\"\"\n",
        "        if idx >= len(self.pairs):\n",
        "            raise IndexError(\"Index out of bounds for dataset\")\n",
        "        source_str, target_str = self.pairs[idx]\n",
        "        source_indices = self.source_vocab.sequence_to_indices(source_str, add_eos=True)\n",
        "        target_indices = self.target_vocab.sequence_to_indices(target_str, add_sos=True, add_eos=True)\n",
        "        return torch.tensor(source_indices, dtype=torch.long), \\\n",
        "               torch.tensor(target_indices, dtype=torch.long)\n",
        "\n",
        "def collate_fn(batch, pad_idx_source, pad_idx_target):\n",
        "    \"\"\"\n",
        "    Custom collate function for DataLoader to handle variable-length sequences.\n",
        "    Pads sequences within a batch to the maximum length of that batch.\n",
        "    \"\"\"\n",
        "    # Filter out any None or empty items\n",
        "    batch = [item for item in batch if item is not None and item[0] is not None and item[1] is not None]\n",
        "    if not batch:\n",
        "        return None, None, None\n",
        "\n",
        "    source_seqs, target_seqs = zip(*batch)\n",
        "\n",
        "    # Filter out any empty sequences after zipping\n",
        "    valid_indices = [i for i, s in enumerate(source_seqs) if len(s) > 0]\n",
        "    if not valid_indices: return None, None, None\n",
        "\n",
        "    source_seqs = [source_seqs[i] for i in valid_indices]\n",
        "    target_seqs = [target_seqs[i] for i in valid_indices]\n",
        "    source_lengths = torch.tensor([len(s) for s in source_seqs], dtype=torch.long)\n",
        "\n",
        "    # Pad sequences to the length of the longest sequence in the batch\n",
        "    padded_sources = pad_sequence(source_seqs, batch_first=True, padding_value=pad_idx_source)\n",
        "    padded_targets = pad_sequence(target_seqs, batch_first=True, padding_value=pad_idx_target)\n",
        "    return padded_sources, source_lengths, padded_targets\n",
        "\n",
        "# --- Training and Evaluation Functions ---\n",
        "\n",
        "def _train_one_epoch(model, dataloader, optimizer, criterion, device, clip_value, teacher_forcing_ratio, target_vocab):\n",
        "    \"\"\"\n",
        "    Trains the model for a single epoch.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The Seq2Seq model.\n",
        "        dataloader (DataLoader): DataLoader for the training set.\n",
        "        optimizer (optim.Optimizer): Optimizer for model parameters.\n",
        "        criterion (nn.Module): Loss function (e.g., CrossEntropyLoss).\n",
        "        device (torch.device): Device to run the model on.\n",
        "        clip_value (float): Gradient clipping value.\n",
        "        teacher_forcing_ratio (float): Probability of using teacher forcing.\n",
        "        target_vocab (Vocabulary): Vocabulary for the target language.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Average epoch loss and training accuracy.\n",
        "    \"\"\"\n",
        "    model.train() # Set model to training mode\n",
        "    epoch_loss = 0\n",
        "    total_correct_train = 0\n",
        "    total_samples_train = 0\n",
        "\n",
        "    if len(dataloader) == 0:\n",
        "        print(\"Warning: Training dataloader is empty.\")\n",
        "        return 0.0, 0.0\n",
        "\n",
        "    for batch_data in tqdm(dataloader, desc=\"Training\", leave=False):\n",
        "        if batch_data[0] is None: continue\n",
        "        sources, source_lengths, targets = batch_data\n",
        "\n",
        "        if sources is None or source_lengths is None or targets is None or source_lengths.numel() == 0 or sources.shape[0] == 0:\n",
        "            print(\"Warning: Empty or invalid batch data after collate_fn in training. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        sources, targets, source_lengths = sources.to(device), targets.to(device), source_lengths.to(device)\n",
        "        optimizer.zero_grad() # Clear gradients\n",
        "\n",
        "        # Forward pass: model predicts logits for the target sequence\n",
        "        outputs_logits = model(sources, source_lengths, targets, teacher_forcing_ratio)\n",
        "\n",
        "        # Reshape outputs and targets for CrossEntropyLoss\n",
        "        # We ignore the first token (SOS) in the target for loss calculation\n",
        "        output_dim = outputs_logits.shape[-1]\n",
        "        flat_outputs = outputs_logits[:, 1:].reshape(-1, output_dim)\n",
        "        flat_targets = targets[:, 1:].reshape(-1)\n",
        "\n",
        "        loss = criterion(flat_outputs, flat_targets)\n",
        "\n",
        "        # Handle potential NaN/Inf loss (e.g., due to bad gradients)\n",
        "        if torch.isnan(loss) or torch.isinf(loss):\n",
        "            print(\"Warning: NaN or Inf loss detected in training. Skipping batch.\")\n",
        "            continue\n",
        "        loss.backward() # Backpropagation\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value) # Clip gradients\n",
        "        optimizer.step() # Update model parameters\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        # Calculate training accuracy (exact sequence match)\n",
        "        predictions_indices_train = outputs_logits.argmax(dim=2)\n",
        "        for i in range(targets.shape[0]):\n",
        "            pred_str_train = target_vocab.indices_to_sequence(predictions_indices_train[i, 1:].tolist())\n",
        "            true_str_train = target_vocab.indices_to_sequence(targets[i, 1:].tolist())\n",
        "            if pred_str_train == true_str_train:\n",
        "                total_correct_train += 1\n",
        "            total_samples_train += 1\n",
        "\n",
        "    avg_epoch_loss = epoch_loss / len(dataloader) if len(dataloader) > 0 else 0.0\n",
        "    train_accuracy = total_correct_train / total_samples_train if total_samples_train > 0 else 0.0\n",
        "    return avg_epoch_loss, train_accuracy\n",
        "\n",
        "\n",
        "def _evaluate_one_epoch(model, dataloader, criterion, device, source_vocab, target_vocab, beam_width=1, is_test_set=False):\n",
        "    \"\"\"\n",
        "    Evaluates the model's performance on a given dataset (validation or test).\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The Seq2Seq model.\n",
        "        dataloader (DataLoader): DataLoader for the evaluation set.\n",
        "        criterion (nn.Module): Loss function.\n",
        "        device (torch.device): Device to run the model on.\n",
        "        source_vocab (Vocabulary): Vocabulary for the source language (used for debug prints).\n",
        "        target_vocab (Vocabulary): Vocabulary for the target language.\n",
        "        beam_width (int): Beam width for decoding (1 for greedy, >1 for beam search).\n",
        "        is_test_set (bool): Flag to indicate if this is the final test evaluation (for logging and samples).\n",
        "\n",
        "    Returns:\n",
        "        tuple: Average epoch loss and evaluation accuracy.\n",
        "    \"\"\"\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    epoch_loss = 0\n",
        "    total_correct, total_samples = 0, 0\n",
        "    if len(dataloader) == 0:\n",
        "        print(\"WARNING: Evaluation dataloader is empty. Returning 0 loss and 0 accuracy.\")\n",
        "        return 0.0, 0.0\n",
        "    desc_prefix = \"Testing\" if is_test_set else \"Validating\"\n",
        "\n",
        "    with torch.no_grad(): # Disable gradient calculations during evaluation\n",
        "        for batch_idx, batch_data in enumerate(tqdm(dataloader, desc=desc_prefix, leave=False)):\n",
        "            if batch_data[0] is None: continue\n",
        "            sources, source_lengths, targets = batch_data\n",
        "\n",
        "            if sources is None or source_lengths is None or targets is None or source_lengths.numel() == 0 or sources.shape[0] == 0:\n",
        "                print(\"Warning: Empty or invalid batch data in eval after collate_fn. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            sources, targets, source_lengths = sources.to(device), targets.to(device), source_lengths.to(device)\n",
        "\n",
        "            # Forward pass for loss calculation (using teacher forcing=0.0)\n",
        "            outputs_for_loss = model(sources, source_lengths, targets, teacher_forcing_ratio=0.0)\n",
        "            output_dim = outputs_for_loss.shape[-1]\n",
        "            flat_outputs_for_loss = outputs_for_loss[:, 1:].reshape(-1, output_dim)\n",
        "            flat_targets_for_loss = targets[:, 1:].reshape(-1)\n",
        "            loss = criterion(flat_outputs_for_loss, flat_targets_for_loss)\n",
        "            epoch_loss += loss.item() if not (torch.isnan(loss) or torch.isinf(loss)) else 0\n",
        "\n",
        "            # Generate predictions for accuracy calculation for each item in batch\n",
        "            for i in range(sources.shape[0]):\n",
        "                src_single, src_len_single = sources[i:i+1], source_lengths[i:i+1]\n",
        "\n",
        "                # Choose decoding strategy (beam search or greedy)\n",
        "                if beam_width > 1 and hasattr(model, 'predict_beam_search'):\n",
        "                    predicted_indices = model.predict_beam_search(src_single, src_len_single,\n",
        "                                                                  max_output_len=targets.size(1) + 5,\n",
        "                                                                  target_eos_idx=target_vocab.eos_idx,\n",
        "                                                                  beam_width=beam_width)\n",
        "                elif hasattr(model, 'predict_greedy'):\n",
        "                     predicted_indices = model.predict_greedy(src_single, src_len_single,\n",
        "                                                             max_output_len=targets.size(1) + 5,\n",
        "                                                             target_eos_idx=target_vocab.eos_idx)\n",
        "                else: # Fallback to argmax on outputs_for_loss if predict methods are not available\n",
        "                    predicted_indices = outputs_for_loss[i:i+1].argmax(dim=2)[0, 1:].tolist()\n",
        "\n",
        "                # Convert predicted and true indices to strings for comparison\n",
        "                pred_str = target_vocab.indices_to_sequence(predicted_indices)\n",
        "                true_str = target_vocab.indices_to_sequence(targets[i, 1:].tolist())\n",
        "\n",
        "                # Check for exact match\n",
        "                if pred_str == true_str: total_correct += 1\n",
        "                total_samples += 1\n",
        "\n",
        "                # Print a single debug sample from the first batch\n",
        "                if is_test_set and batch_idx == 0 and i < 3:\n",
        "                    print(f\"  Test Example {i} - Source: '{source_vocab.indices_to_sequence(src_single[0].tolist())}'\")\n",
        "                    print(f\"    Pred: '{pred_str}', True: '{true_str}', Match: {pred_str == true_str}\")\n",
        "\n",
        "    avg_epoch_loss = epoch_loss / len(dataloader) if len(dataloader) > 0 else 0.0\n",
        "    accuracy = total_correct / total_samples if total_samples > 0 else 0.0\n",
        "    print(f\"{desc_prefix} - Total Correct: {total_correct}, Total Samples: {total_samples}, Accuracy: {accuracy:.4f}, Avg Loss: {avg_epoch_loss:.4f}\")\n",
        "    return avg_epoch_loss, accuracy\n",
        "\n",
        "# --- Function to Train and Save the Best Model ---\n",
        "\n",
        "def train_and_save_best_model(config_params, model_save_path, device):\n",
        "    \"\"\"\n",
        "    Performs a dedicated training run using the best hyperparameters found from a sweep.\n",
        "    Saves the model checkpoint with the best validation accuracy.\n",
        "\n",
        "    Args:\n",
        "        config_params (dict): Dictionary of hyperparameters for this specific training run.\n",
        "        model_save_path (str): Path to save the best model's state_dict.\n",
        "        device (torch.device): Device to run the training on.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if training was successful and a model was saved, False otherwise.\n",
        "    \"\"\"\n",
        "    run_name_train_best = f\"TRAIN_BEST_{config_params['cell_type']}_emb{config_params['embedding_dim']}_hid{config_params['hidden_dim']}\"\n",
        "\n",
        "    with wandb.init(project=\"DL_A3\", name=run_name_train_best, config=config_params, job_type=\"training_best_model_final\", reinit=True) as run:\n",
        "        cfg = wandb.config\n",
        "        print(f\"Starting dedicated training for best model with config: {cfg}\")\n",
        "\n",
        "        # --- Data Loading and Vocabulary Building ---\n",
        "        BASE_DATA_DIR = \"/kaggle/input/dakshina-dl-a3/dakshina_dataset_v1.0/hi/\"\n",
        "        DATA_DIR = os.path.join(BASE_DATA_DIR, \"lexicons/\")\n",
        "        train_file = os.path.join(DATA_DIR, \"hi.translit.sampled.train.tsv\")\n",
        "        dev_file = os.path.join(DATA_DIR, \"hi.translit.sampled.dev.tsv\")\n",
        "\n",
        "        source_vocab = Vocabulary(\"latin\")\n",
        "        target_vocab = Vocabulary(\"devanagari\")\n",
        "\n",
        "        # Build vocabulary from the training data\n",
        "        temp_train_ds_vocab = TransliterationDataset(train_file, source_vocab, target_vocab, max_len=cfg.max_seq_len)\n",
        "        if not temp_train_ds_vocab.pairs:\n",
        "            print(f\"ERROR: No training data loaded for vocabulary building from {train_file}.\")\n",
        "            return False\n",
        "        for src, tgt in temp_train_ds_vocab.pairs:\n",
        "            source_vocab.add_sequence(src)\n",
        "            target_vocab.add_sequence(tgt)\n",
        "        source_vocab.build_vocab(min_freq=cfg.vocab_min_freq)\n",
        "        target_vocab.build_vocab(min_freq=cfg.vocab_min_freq)\n",
        "        print(f\"Vocabs built. Source: {source_vocab.n_chars}, Target: {target_vocab.n_chars}\")\n",
        "\n",
        "        # Create actual datasets for training and validation\n",
        "        train_dataset = TransliterationDataset(train_file, source_vocab, target_vocab, max_len=cfg.max_seq_len)\n",
        "        dev_dataset = TransliterationDataset(dev_file, source_vocab, target_vocab, max_len=cfg.max_seq_len)\n",
        "        if not train_dataset.pairs or not dev_dataset.pairs:\n",
        "            print(f\"ERROR: Train or Dev dataset is empty after filtering. Train: {len(train_dataset.pairs)}, Dev: {len(dev_dataset.pairs)}\")\n",
        "            return False\n",
        "\n",
        "        # --- DataLoader Setup ---\n",
        "        num_w = 0 if device.type == 'cpu' else min(4, os.cpu_count() // 2 if os.cpu_count() else 0)\n",
        "        train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size_train, shuffle=True,\n",
        "                                  collate_fn=lambda b: collate_fn(b, source_vocab.pad_idx, target_vocab.pad_idx),\n",
        "                                  num_workers=num_w, pin_memory=True if device.type == 'cuda' else False, drop_last=True)\n",
        "        dev_loader = DataLoader(dev_dataset, batch_size=cfg.batch_size_train, shuffle=False,\n",
        "                                collate_fn=lambda b: collate_fn(b, source_vocab.pad_idx, target_vocab.pad_idx),\n",
        "                                num_workers=num_w, pin_memory=True if device.type == 'cuda' else False)\n",
        "        if not train_loader or not dev_loader:\n",
        "            print(f\"ERROR: Train or Dev DataLoader is empty. Train: {len(train_loader)}, Dev: {len(dev_loader)}\")\n",
        "            return False\n",
        "\n",
        "        # --- Model, Optimizer, Loss Function Setup ---\n",
        "        encoder = Encoder(source_vocab.n_chars, cfg.embedding_dim, cfg.hidden_dim,\n",
        "                          cfg.encoder_layers, cfg.cell_type, cfg.dropout_p,\n",
        "                          cfg.encoder_bidirectional, pad_idx=source_vocab.pad_idx).to(device)\n",
        "        decoder = Decoder(target_vocab.n_chars, cfg.embedding_dim, cfg.hidden_dim,\n",
        "                          cfg.decoder_layers, cfg.cell_type, cfg.dropout_p,\n",
        "                          pad_idx=target_vocab.pad_idx).to(device)\n",
        "        model = Seq2Seq(encoder, decoder, device, target_vocab.sos_idx).to(device)\n",
        "        print(f\"Best model parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
        "        wandb.watch(model, log=\"all\", log_freq=100)\n",
        "\n",
        "        optimizer = optim.Adam(model.parameters(), lr=cfg.learning_rate_train)\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=cfg.lr_scheduler_patience_train, factor=0.3, verbose=True)\n",
        "        criterion = nn.CrossEntropyLoss(ignore_index=target_vocab.pad_idx)\n",
        "\n",
        "        best_val_acc_this_training = -1.0\n",
        "        epochs_no_improve = 0\n",
        "\n",
        "        # --- Training Loop ---\n",
        "        for epoch in range(cfg.epochs_train):\n",
        "            train_loss, train_acc = _train_one_epoch(model, train_loader, optimizer, criterion, device,\n",
        "                                                 cfg.clip_value_train, cfg.teacher_forcing_train, target_vocab)\n",
        "\n",
        "            val_loss, val_acc = _evaluate_one_epoch(model, dev_loader, criterion, device,\n",
        "                                                    source_vocab, target_vocab,\n",
        "                                                    beam_width=1) # Use greedy for val during this training\n",
        "\n",
        "            scheduler.step(val_acc)\n",
        "            print(f\"Epoch {epoch+1}/{cfg.epochs_train} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "            wandb.log({\"epoch_train_best\": epoch + 1, \"train_loss_best\": train_loss, \"train_acc_best\": train_acc,\n",
        "                       \"val_loss_best\": val_loss, \"val_acc_best\": val_acc, \"lr_best\": optimizer.param_groups[0]['lr']})\n",
        "\n",
        "            # Early stopping logic\n",
        "            current_val_acc = val_acc if not np.isnan(val_acc) else -1.0\n",
        "            if current_val_acc > best_val_acc_this_training:\n",
        "                best_val_acc_this_training = current_val_acc\n",
        "                epochs_no_improve = 0\n",
        "                torch.save(model.state_dict(), model_save_path) # Save best model\n",
        "                print(f\"Saved new best model to {model_save_path} (Val Acc: {best_val_acc_this_training:.4f})\")\n",
        "            else:\n",
        "                epochs_no_improve += 1\n",
        "\n",
        "            if epochs_no_improve >= cfg.max_epochs_no_improve_train:\n",
        "                print(f\"Early stopping for best model training at epoch {epoch+1}.\")\n",
        "                break\n",
        "\n",
        "        # Save the final model state if no improvement happened over the initial checkpoint\n",
        "        if not os.path.exists(model_save_path):\n",
        "            torch.save(model.state_dict(), model_save_path)\n",
        "            print(f\"Saved final model state (no improvement over initial) to {model_save_path}\")\n",
        "\n",
        "        wandb.summary[\"final_best_val_accuracy_during_training\"] = best_val_acc_this_training\n",
        "        print(f\"Finished training best model. Best Val Acc: {best_val_acc_this_training:.4f}\")\n",
        "    return True\n",
        "\n",
        "# --- Main Execution Block for Training Best Model and Testing ---\n",
        "if __name__ == '__main__':\n",
        "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "    # --- W&B Login ---\n",
        "    try:\n",
        "        if 'KAGGLE_KERNEL_RUN_TYPE' in os.environ:\n",
        "            print(\"Detected Kaggle environment. Ensuring WANDB_API_KEY is set.\")\n",
        "            if \"WANDB_API_KEY\" in os.environ:\n",
        "                wandb.login()\n",
        "                print(\"W&B login using environment variable.\")\n",
        "            else:\n",
        "                from kaggle_secrets import UserSecretsClient\n",
        "                user_secrets = UserSecretsClient()\n",
        "                wandb_api_key = user_secrets.get_secret(\"WANDB_API_KEY\")\n",
        "                wandb.login(key=wandb_api_key)\n",
        "                print(\"W&B login for Kaggle successful.\")\n",
        "        else:\n",
        "            wandb.login()\n",
        "        print(\"W&B login process attempted/completed.\")\n",
        "    except Exception as e:\n",
        "        print(f\"W&B login failed: {e}. Ensure API key is configured.\")\n",
        "        exit()\n",
        "\n",
        "    # --- Best Hyperparameters (obtained from a previous sweep) ---\n",
        "    # These hyperparameters are chosen based on the provided sweep results.\n",
        "    BEST_HYPERPARAMETERS_FOR_TRAINING = {\n",
        "        'embedding_dim': 300,\n",
        "        'hidden_dim': 512,\n",
        "        'encoder_layers': 2,\n",
        "        'decoder_layers': 1,\n",
        "        'cell_type': 'LSTM',\n",
        "        'dropout_p': 0.5,\n",
        "        'encoder_bidirectional': True,\n",
        "        'learning_rate_train': 0.000376,\n",
        "        'batch_size_train': 128,\n",
        "        'epochs_train': 20,\n",
        "        'clip_value_train': 1.0,\n",
        "        'teacher_forcing_train': 0.5,\n",
        "        'max_epochs_no_improve_train': 7,\n",
        "        'lr_scheduler_patience_train': 3,\n",
        "        'vocab_min_freq': 1,\n",
        "        'max_seq_len': 50,\n",
        "        'eval_batch_size': 128,\n",
        "        'beam_width_eval': 3\n",
        "    }\n",
        "    MODEL_SAVE_PATH = \"/kaggle/working/best_model_for_testing.pt\"\n",
        "\n",
        "    print(f\"Best hyperparameters selected for training: {BEST_HYPERPARAMETERS_FOR_TRAINING}\")\n",
        "\n",
        "    # --- Phase 1: Train and Save the Best Model ---\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"--- Phase 1: Training and Saving Best Model Configuration ---\".center(80))\n",
        "    print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "    training_successful = train_and_save_best_model(BEST_HYPERPARAMETERS_FOR_TRAINING, MODEL_SAVE_PATH, DEVICE)\n",
        "\n",
        "    if not training_successful or not os.path.exists(MODEL_SAVE_PATH):\n",
        "        print(\"ERROR: Failed to train and save the best model. Exiting before test evaluation.\")\n",
        "        exit()\n",
        "    print(f\"Best model trained and saved to {MODEL_SAVE_PATH}\")\n",
        "\n",
        "    # --- Phase 2: Load and Evaluate on Test Set ---\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"--- Phase 2: Loading and Evaluating Best Model on Test Set ---\".center(80))\n",
        "    print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "    BASE_DATA_DIR = \"/kaggle/input/dakshina-dl-a3/dakshina_dataset_v1.0/hi/\"\n",
        "    DATA_DIR = os.path.join(BASE_DATA_DIR, \"lexicons/\")\n",
        "    train_file_for_vocab = os.path.join(DATA_DIR, \"hi.translit.sampled.train.tsv\")\n",
        "    test_file = os.path.join(DATA_DIR, \"hi.translit.sampled.test.tsv\")\n",
        "\n",
        "    source_vocab_test = Vocabulary(\"latin_test\")\n",
        "    target_vocab_test = Vocabulary(\"devanagari_test\")\n",
        "\n",
        "    # Build vocabulary using the training dataset (crucial for consistent tokenization)\n",
        "    temp_train_ds_test_vocab = TransliterationDataset(train_file_for_vocab, source_vocab_test, target_vocab_test,\n",
        "                                                max_len=BEST_HYPERPARAMETERS_FOR_TRAINING['max_seq_len'])\n",
        "    if not temp_train_ds_test_vocab.pairs:\n",
        "        print(f\"ERROR: No data loaded for vocabulary building from {train_file_for_vocab}.\")\n",
        "        exit()\n",
        "    for src, tgt in temp_train_ds_test_vocab.pairs:\n",
        "        source_vocab_test.add_sequence(src)\n",
        "        target_vocab_test.add_sequence(tgt)\n",
        "    source_vocab_test.build_vocab(min_freq=BEST_HYPERPARAMETERS_FOR_TRAINING['vocab_min_freq'])\n",
        "    target_vocab_test.build_vocab(min_freq=BEST_HYPERPARAMETERS_FOR_TRAINING['vocab_min_freq'])\n",
        "    print(f\"Test Vocab built from training data. Source: {source_vocab_test.n_chars} chars. Target: {target_vocab_test.n_chars} chars.\")\n",
        "\n",
        "    # Create the test dataset\n",
        "    test_dataset = TransliterationDataset(test_file, source_vocab_test, target_vocab_test,\n",
        "                                          max_len=BEST_HYPERPARAMETERS_FOR_TRAINING['max_seq_len'])\n",
        "    if not test_dataset.pairs:\n",
        "        print(f\"ERROR: Test dataset is empty from {test_file}.\")\n",
        "        exit()\n",
        "\n",
        "    # Setup test DataLoader\n",
        "    num_w_test = 0 if DEVICE.type == 'cpu' else min(4, os.cpu_count() // 2 if os.cpu_count() else 0)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=BEST_HYPERPARAMETERS_FOR_TRAINING['eval_batch_size'], shuffle=False,\n",
        "                                 collate_fn=lambda b: collate_fn(b, source_vocab_test.pad_idx, target_vocab_test.pad_idx),\n",
        "                                 num_workers=num_w_test, pin_memory=True if DEVICE.type == 'cuda' else False)\n",
        "    if not test_dataloader or len(test_dataloader) == 0:\n",
        "        print(f\"ERROR: Test Dataloader is empty. Dataset size: {len(test_dataset)}, Batch size: {BEST_HYPERPARAMETERS_FOR_TRAINING['eval_batch_size']}\")\n",
        "        exit()\n",
        "\n",
        "    # --- Initialize Model for Testing and Load Weights ---\n",
        "    encoder_test = Encoder(source_vocab_test.n_chars, BEST_HYPERPARAMETERS_FOR_TRAINING['embedding_dim'], BEST_HYPERPARAMETERS_FOR_TRAINING['hidden_dim'],\n",
        "                           BEST_HYPERPARAMETERS_FOR_TRAINING['encoder_layers'], BEST_HYPERPARAMETERS_FOR_TRAINING['cell_type'], BEST_HYPERPARAMETERS_FOR_TRAINING['dropout_p'],\n",
        "                           BEST_HYPERPARAMETERS_FOR_TRAINING['encoder_bidirectional'], pad_idx=source_vocab_test.pad_idx).to(DEVICE)\n",
        "    decoder_test = Decoder(target_vocab_test.n_chars, BEST_HYPERPARAMETERS_FOR_TRAINING['embedding_dim'], BEST_HYPERPARAMETERS_FOR_TRAINING['hidden_dim'],\n",
        "                           BEST_HYPERPARAMETERS_FOR_TRAINING['decoder_layers'], BEST_HYPERPARAMETERS_FOR_TRAINING['cell_type'], BEST_HYPERPARAMETERS_FOR_TRAINING['dropout_p'],\n",
        "                           pad_idx=target_vocab_test.pad_idx).to(DEVICE)\n",
        "    model_test = Seq2Seq(encoder_test, decoder_test, DEVICE, target_vocab_test.sos_idx).to(DEVICE)\n",
        "\n",
        "    print(f\"Loading weights into test model from: {MODEL_SAVE_PATH}\")\n",
        "    model_test.load_state_dict(torch.load(MODEL_SAVE_PATH, map_location=DEVICE))\n",
        "    print(\"Weights loaded successfully for test evaluation.\")\n",
        "\n",
        "    # --- Evaluate on Test Set ---\n",
        "    criterion_test = nn.CrossEntropyLoss(ignore_index=target_vocab_test.pad_idx)\n",
        "    test_loss, test_accuracy = _evaluate_one_epoch(model_test, test_dataloader, criterion_test, DEVICE,\n",
        "                                                   source_vocab_test, target_vocab_test,\n",
        "                                                   beam_width=BEST_HYPERPARAMETERS_FOR_TRAINING.get('beam_width_eval', 1),\n",
        "                                                   is_test_set=True)\n",
        "\n",
        "    print(f\"\\n--- FINAL TEST SET PERFORMANCE ---\")\n",
        "    print(f\"Model Checkpoint: '{MODEL_SAVE_PATH}'\")\n",
        "    # Print only relevant hyperparameters for the loaded model, not training-specific ones\n",
        "    eval_config_to_print = {k: v for k, v in BEST_HYPERPARAMETERS_FOR_TRAINING.items() if not k.endswith('_train')}\n",
        "    print(f\"Hyperparameters: {eval_config_to_print}\")\n",
        "    print(f\"  Test Loss (avg per batch): {test_loss:.4f}\")\n",
        "    print(f\"  Test Accuracy (Exact Match): {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
        "\n",
        "    # --- Log Test Results to W&B ---\n",
        "    try:\n",
        "        run_name_final_test = f\"TEST_BEST_MODEL_{BEST_HYPERPARAMETERS_FOR_TRAINING['cell_type']}\"\n",
        "        with wandb.init(project=\"DL_A3\", name=run_name_final_test, config=BEST_HYPERPARAMETERS_FOR_TRAINING, job_type=\"final_test_evaluation\", reinit=True) as test_run:\n",
        "            test_run.summary[\"final_test_accuracy\"] = test_accuracy\n",
        "            test_run.summary[\"final_test_loss\"] = test_loss\n",
        "            test_run.summary[\"model_checkpoint_used\"] = MODEL_SAVE_PATH\n",
        "            print(\"Final test results logged to W&B.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not log final test results to W&B: {e}\")"
      ],
      "metadata": {
        "id": "t8Fbh50UqpOy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}