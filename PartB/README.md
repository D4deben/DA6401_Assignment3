# PartB: Attention-based Seq2Seq Model

This section of the repository extends the fundamental Seq2Seq architecture by integrating an **Attention Mechanism**. This allows the model to achieve more sophisticated sequence-to-sequence mappings, particularly beneficial for handling longer sequences where the vanilla Seq2Seq model might struggle due to the fixed-size context vector bottleneck.

## Core Concepts

The attention-based Seq2Seq model differs from its vanilla counterpart by introducing a "soft alignment" mechanism:

1.  **Encoding**: The encoder processes the entire input sequence, similar to the vanilla model, but it outputs not only a final hidden state but **all intermediate hidden states** for each input token.
2.  **Attention Mechanism**: For each step of decoding, the attention mechanism calculates "attention weights" over all of the encoder's output states. These weights quantify how relevant each part of the input sequence is for generating the current output token.
3.  **Context Vector Creation**: A dynamic `context vector` is created as a weighted sum of the encoder's output states, using the attention weights. This means the context vector changes at every decoding step, focusing on different parts of the input as needed.
4.  **Decoding**: The decoder generates the output sequence using its previous hidden state, the embedded previous output token, AND the dynamically generated context vector. This allows the decoder to directly access and utilize relevant input information at each step, significantly improving performance for complex mappings.

## Repository Contents:

* **`DL_Assignment4PartB.ipynb`**: This Jupyter Notebook contains the complete code for the Attention-based Seq2Seq model.
    * **Model Architecture**:
        * **`Attention(nn.Module)`**: Implements the attention scoring function. It calculates attention scores (energies) using a linear layer and tanh activation, then applies softmax to get normalized attention weights. The output signifies how much each input character contributes to generating a specific output character.
        * **`Encoder(nn.Module)`**: Similar to the encoder in PartA, but its `forward` method is designed to return **all hidden states** from the last RNN layer (`encoder_outputs`), which are essential inputs for the `Attention` module.
        * **`DecoderWithAttention(nn.Module)`**: This is the core component that integrates the attention mechanism. At each decoding step, it takes the `attention_query_hidden` (typically the top layer's hidden state of the decoder), the `encoder_outputs`, and the embedded previous output character. It computes the `context_vector` via the `Attention` module and concatenates it with the current token embedding before feeding it to the decoder's RNN. It returns predictions, updated hidden state, and the `attention_weights` for visualization purposes.
        * **`Seq2SeqWithAttention(nn.Module)`**: The main wrapper that orchestrates the `Encoder` and `DecoderWithAttention`. It includes `_adapt_encoder_hidden_for_decoder` for proper initialization and `predict_greedy` and `predict_beam_search` methods for inference. It also features `predict_with_attention` which explicitly returns the attention matrices for visualization.
    * **Data Handling**: Reuses the `Vocabulary`, `TransliterationDataset`, and `collate_fn` from PartA, ensuring consistent data preparation across both model types.
    * **Training and Evaluation Functions**:
        * `_train_one_epoch`: Manages the training loop for one epoch, integrating the attention mechanism into the forward pass.
        * `_evaluate_one_epoch`: Evaluates model performance, collecting data for overall metrics and character-level analysis. It can leverage greedy or beam search decoding.
    * **Analysis and Visualization Specifics**:
        * **`plot_attention_heatmap` Function**: A utility function to generate visual heatmaps of attention weights. For a given input-output pair, it shows which input characters were most attended to for each generated output character. This function saves PNG files of these heatmaps.
    * **Hyperparameter Management**: Uses Weights & Biases (W&B) for experiment tracking, including `BEST_ATTN_HYPERPARAMETERS` for dedicated training. **Ensure these placeholders are replaced with the optimal values derived from your own hyperparameter tuning for the attention model.**
* **`prediction_attention/Q6.csv`**:
    * This CSV file contains the raw predictions generated by the trained Attention-based Seq2Seq model on the `hi.translit.sampled.test.tsv` dataset.
    * **Format**: Similar to PartA's output, each row includes `Input (Latin)`, `True Output (Devanagari)`, `Model Prediction (Devanagari)`, and `Correct?`. This file is essential for quantitative comparison with the vanilla model and other baselines.

## How to Run:

1.  **Data Setup**: Ensure the Dakshina Hindi transliteration dataset (`hi.translit.sampled.train.tsv`, `.dev.tsv`, `.test.tsv`) is located at the path specified in the notebook's `BASE_DATA_DIR` (default for Kaggle is `/kaggle/input/dakshina-dl-a3/dakshina_dataset_v1.0/hi/lexicons/`). If your data is elsewhere, modify this path in the notebook.
2.  **Environment Setup**: Install all required Python packages as listed in the main `README.md` and ensure you have logged into Weights & Biases via `wandb login`.
3.  **Execute Notebook**: Open `DL_Assignment4PartB.ipynb` in a Jupyter environment (e.g., Jupyter Lab, Google Colab) and run all cells sequentially.
    * The notebook is designed to:
        * Load data and build vocabularies.
        * Initialize and train the Attention-based Seq2Seq model using the `BEST_ATTN_HYPERPARAMETERS`.
        * Evaluate the trained model on the test set.
        * Save predictions to `PartB/prediction_attention/Q6.csv`.
        * Generate and save attention heatmaps for selected test examples in `PartB/attention_heatmaps_q5d/`.
        * Log various metrics, sample predictions, the full predictions file, and a character-level confusion matrix to your W&B project.

Upon successful execution, you will have:

* A trained attention-based Seq2Seq model.
* Detailed training and validation logs on your W&B dashboard.
* A CSV file of test predictions in `prediction_attention/`.
* PNG images of attention heatmaps in `attention_heatmaps_q5d/`, which visually represent how the model aligns input and output sequences.

---
